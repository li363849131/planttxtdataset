{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"lunwen9_flavia_vgg19.ipynb","provenance":[],"collapsed_sections":["4g1XCKxi5g1H","lZEFf6gy6UpN"],"toc_visible":true,"machine_shape":"hm","mount_file_id":"1ykb4VUGRd-vbnLklc9W98UiiGwiL2E_L","authorship_tag":"ABX9TyMLyUAUgmeTxfyFv/GAVvbA"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mBAX8LIflSYT"},"source":["# 新段落，如下代码用于测试flavia 数据集，精简版本,由于加入注意力之后的准确率非常低，为了找到原因，尝试将一个训练好的image分类模型给到注意力机制来看下结果。\n"]},{"cell_type":"code","metadata":{"id":"dCdzHPZrVrO3"},"source":["#@title matplotlib 中文字体设置\n","# 如下代码解决 matplotlib 上中文字体的问题，需要在运行这个之后， 选择 “代码执行程序\" -> \"重新启动代码执行程序\"\n","# 快捷键 ctrl M . \n","!cp /content/drive/MyDrive/lijiecode/font/matplotlibrc /usr/local/lib/python3.7/dist-packages/matplotlib/mpl-data/matplotlibrc\n","!cp /content/drive/MyDrive/lijiecode/font/SimHei.ttf /usr/local/lib/python3.7/dist-packages/matplotlib/mpl-data/fonts/ttf/\n","!rm /root/.cache/matplotlib -fr\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"id":"ifqlYxZzgdvv","executionInfo":{"status":"ok","timestamp":1627434296598,"user_tz":-480,"elapsed":56161,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"}},"outputId":"c93c64be-8158-4944-adb7-cc74864776d6"},"source":["# @title 各种头库,\n","\n","!cp /content/drive/MyDrive/data/embeddings_matrix.npy ./\n","!cp /content/drive/MyDrive/dataset/Flavia/flaviatxt.zip ./ \n","!unzip -o flaviatxt.zip -d flaviatxt\n","\n","# 首先要加载各种使用的库\n","!pip install thulac\n","!pip install annoy\n","!pip install mxnet\n","import thulac\n","import tensorflow as tf\n","# You ll generate plots of attention in order to see which parts of an image\n","# our model focuses on during captioning\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","# Scikit-learn includes many helpful utilities\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","import collections\n","import random\n","import re\n","import numpy as np\n","import os\n","import time\n","import json\n","from glob import glob\n","from PIL import Image\n","import pickle\n","import codecs\n","import gc\n","from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\n","from tensorflow.keras.layers import Conv1D, LSTM, Conv2D, MaxPooling2D\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n","from tensorflow.keras.models import Sequential,Model\n","import numpy as np\n","import tensorflow.keras as keras\n","import shutil\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","import jieba\n","import jieba.analyse\n","import jieba.posseg as pseg\n","import json\n","from collections import OrderedDict\n","from gensim.models import KeyedVectors\n","# from annoy import AnnoyIndex\n","from keras.layers import Embedding\n","import gensim\n","from gensim.test.utils import common_texts, get_tmpfile\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","from gensim.test.utils import datapath\n","import time\n","# import mxnet\n","import os\n","# print(\"list mxnet's vector:\", mxnet.contrib.text.embedding.get_pretrained_file_names())\n","# below line can get the pretrained file's url, like\n","# https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/embeddings/fasttext/wiki.ch.zip\n","# if 0:\n","#   mxnet.contrib.text.embedding.FastText(pretrained_file_name=\"wiki.ch.vec\")\n","\n","# @title 复制腾讯词嵌入，wiki.zh.vec。\n","# 后者应该不用了，因为它没有腾讯词嵌入的东西多。\n","import os\n","# if os.path.isfile(\"Tencent_AILab_ChineseEmbedding.tar.gz\") == False:\n","#   !cp /content/drive/MyDrive/data/Tencent_AILab_ChineseEmbedding.tar.gz ./\n","#   !tar -zxvf Tencent_AILab_ChineseEmbedding.tar.gz\n","#\n","# if os.path.isfile(\"/content/wiki.zh.vec\") == False:\n","#   !cp /content/drive/MyDrive/data/wiki.zh.vec .\n","\n","from pylab import mpl\n","\n","mpl.rcParams['font.sans-serif'] = ['SimHei']  # 汉字字体,优先使用楷体，如果找不到楷体，则使用黑体\n","mpl.rcParams['font.size'] = 12  # 字体大小\n","mpl.rcParams['axes.unicode_minus'] = False  # 正常显示负号\n","import pickle as p\n","\n","\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Archive:  flaviatxt.zip\n","  inflating: flaviatxt/1.txt         \n","  inflating: flaviatxt/10.txt        \n","  inflating: flaviatxt/11.txt        \n","  inflating: flaviatxt/12.txt        \n","  inflating: flaviatxt/14.txt        \n","  inflating: flaviatxt/15.txt        \n","  inflating: flaviatxt/16.txt        \n","  inflating: flaviatxt/17.txt        \n","  inflating: flaviatxt/18.txt        \n","  inflating: flaviatxt/19.txt        \n","  inflating: flaviatxt/2.txt         \n","  inflating: flaviatxt/20.txt        \n","  inflating: flaviatxt/21.txt        \n","  inflating: flaviatxt/22.txt        \n","  inflating: flaviatxt/23.txt        \n","  inflating: flaviatxt/24.txt        \n","  inflating: flaviatxt/25.txt        \n","  inflating: flaviatxt/26.txt        \n","  inflating: flaviatxt/27.txt        \n","  inflating: flaviatxt/28.txt        \n","  inflating: flaviatxt/29.txt        \n","  inflating: flaviatxt/3.txt         \n","  inflating: flaviatxt/30.txt        \n","  inflating: flaviatxt/31.txt        \n","  inflating: flaviatxt/32.txt        \n","  inflating: flaviatxt/33.txt        \n","  inflating: flaviatxt/4.txt         \n","  inflating: flaviatxt/5.txt         \n","  inflating: flaviatxt/6.txt         \n","  inflating: flaviatxt/7.txt         \n","  inflating: flaviatxt/8.txt         \n","  inflating: flaviatxt/9.txt         \n","Collecting thulac\n","  Downloading thulac-0.2.1.tar.gz (52.9 MB)\n","\u001b[K     |████████████████████████████████| 52.9 MB 26 kB/s \n","\u001b[?25hBuilding wheels for collected packages: thulac\n","  Building wheel for thulac (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for thulac: filename=thulac-0.2.1-py3-none-any.whl size=53141672 sha256=12229db4ec79b0834b9cdec53e84fb19cbbcab14d6932d72d4721c030bb1fe83\n","  Stored in directory: /root/.cache/pip/wheels/97/37/f3/be4ae10faf0fbf35cc192469b737ead6f8f99404bcd82fb2e0\n","Successfully built thulac\n","Installing collected packages: thulac\n","Successfully installed thulac-0.2.1\n","Collecting annoy\n","  Downloading annoy-1.17.0.tar.gz (646 kB)\n","\u001b[K     |████████████████████████████████| 646 kB 10.1 MB/s \n","\u001b[?25hBuilding wheels for collected packages: annoy\n","  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391654 sha256=be8f50b9a6dfe419e8aa8a6d0510412b30013f43b2ec8cf97029a905d98053ab\n","  Stored in directory: /root/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n","Successfully built annoy\n","Installing collected packages: annoy\n","Successfully installed annoy-1.17.0\n","Collecting mxnet\n","  Downloading mxnet-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (46.9 MB)\n","\u001b[K     |████████████████████████████████| 46.9 MB 68 kB/s \n","\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n","Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.19.5)\n","Collecting graphviz<0.9.0,>=0.8.1\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","Successfully installed graphviz-0.8.4 mxnet-1.8.0.post0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eZB-gabqgsoS","executionInfo":{"status":"ok","timestamp":1627434339650,"user_tz":-480,"elapsed":43068,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"}},"outputId":"0955de15-cf93-4906-8a7f-a4f10e274087"},"source":["#@title 准备flavia图片和txt的数据\n","mpl.rcParams['font.sans-serif'] = ['SimHei']  # 汉字字体,优先使用楷体，如果找不到楷体，则使用黑体\n","mpl.rcParams['font.size'] = 12  # 字体大小\n","mpl.rcParams['axes.unicode_minus'] = False  # 正常显示负号\n","\n","# pc\n","pathofflaviatxt = r\".\\flaviatxt\"\n","leaves_folder = r\".\\Leaves\"\n","imgrootpath = r\".\\Leaves2\"\n","\n","#colab\n","pathofflaviatxt = \"/content/flaviatxt\"\n","leaves_folder = r\"Leaves\"\n","imgrootpath = \"/content/Leaves2\"\n","# pathofflaviatxt = r\"./flaviatxt\"\n","leaves_folder = r\"./Leaves\"\n","imgrootpath = r\"./Leaves2\"\n","\n","#请记得最重要的不同是分隔符的不同。\n","\n","\n","# if os.path.isdir(\"model_text\"):\n","#     model_text = tf.keras.models.load_model(\"model_text\")\n","# else:\n","#     raise (\"Not found model 'model_text'\")\n","# model_text.trainable = False\n","\n","def getfilenum(filefolder):  # 这个好像没有人使用。\n","  i = 0\n","  for root, dirs, files in os.walk(filefolder, topdown=False):\n","      for file in files:\n","          i += 1\n","  return (i)\n","\n","\n","def prepare_img_todataset(folder1, filelabel, leaf_class):\n","    # global leaf_class\n","    oldi = 0\n","    labels = filelabel\n","    filelist = os.listdir(folder1)\n","    labels = list(set(labels))\n","    if os.path.isdir(folder1 + \"2\"):\n","        shutil.rmtree(folder1 + \"2\")\n","    os.mkdir(folder1 + \"2\")\n","    for x in labels:\n","        if os.path.isdir(os.path.join(folder1 + \"2\", str(x))):\n","            shutil.rmtree(os.path.join(folder1 + \"2\", str(x)))\n","        os.mkdir(os.path.join(folder1 + \"2\", \"%02d\" % x))\n","    i = 0\n","    subfolders = []\n","    for x in filelist:\n","        i += 1\n","        if int(100 * i / len(filelist)) != oldi:\n","            print(\"%02d\" % (int(100 * i / len(filelist)),) + \"%\")\n","            oldi = int(100 * i / len(filelist))\n","        subfolder = \"\"\n","        for l, s, e in leaf_class:\n","            if int(x.split(\".\")[0]) in range(s, e + 1):\n","                subfolder = \"%02d\" % l\n","                subfolders.append(subfolder)\n","                break\n","        # os.system(\"cp %s %s\"%(os.path.join(folder1,x), os.path.join(os.path.join(folder1+\"2\",subfolder),x)) )\n","        shutil.copy(os.path.join(folder1, x), os.path.join(os.path.join(folder1 + \"2\", subfolder), x))\n","\n","\n","def prepareflaviaPicdata():\n","    # if we have the data, so don't copy it.\n","    if os.path.isdir(leaves_folder+\"2\") == False:\n","      !cp /content/drive/MyDrive/dataset/Flavia/Leaves.tar ./\n","      !tar -xvf Leaves.tar\n","      !rm Leaves/Leaves -fr\n","      !rm Leaves2 -fr\n","\n","    # 定义了类别，leaf_class为最终类别的结果，是一个两个数为一个单元的list\n","    flavia_class = ['1001-1059', '1060-1122', '1552-1616', '1123-1194', '1195-1267', '1268-1323', '1324-1385',\n","                    '1386-1437', '1497-1551', '1438-1496', '2001-2050', '2051-2113', '2114-2165', '2166-2230',\n","                    '2231-2290', '2291-2346', '2347-2423', '2424-2485', '2486-2546', '2547-2612', '2616-2675',\n","                    '3001-3055', '3056-3110', '3111-3175', '3176-3229', '3230-3281', '3282-3334', '3335-3389',\n","                    '3390-3446', '3447-3510', '3511-3563', '3566-3621']\n","    labels = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,\n","              31, 32, 33]\n","    # labels = labelnames\n","\n","    # prepare pair [class, filenames]\n","    leaf_class = []\n","    for i, x in zip(labels, flavia_class):\n","        leaf_class.append([i, int(x.split(\"-\")[0]), int(x.split(\"-\")[1])])\n","    # print(leaf_class)\n","\n","    # 将图片复制到到不同的文件夹，每个文件夹就是一个类别\n","    # 整体放倒 leaves2 里面了\n","    oldi = 0  # this is for tell me the progress\n","\n","    # 如果没有leveas2，那么运行一下。\n","    # prepare_img_todataset(leaves_folder, labels)\n","    if os.path.isdir(leaves_folder + \"2\") == False:\n","        prepare_img_todataset(leaves_folder, labels, leaf_class)\n","        !cp /content/Leaves2/08/1386.jpg /content/Leaves2/08/13861.jpg\n","        !cp /content/Leaves2/08/1387.jpg /content/Leaves2/08/13862.jpg\n","        !cp /content/Leaves2/08/1388.jpg /content/Leaves2/08/13863.jpg\n","        !cp /content/Leaves2/08/1389.jpg /content/Leaves2/08/13864.jpg\n","        !cp /content/Leaves2/08/1396.jpg /content/Leaves2/08/13865.jpg\n","        !cp /content/Leaves2/08/1397.jpg /content/Leaves2/08/13866.jpg\n","        !cp /content/Leaves2/08/1398.jpg /content/Leaves2/08/13867.jpg\n","        !cp /content/Leaves2/08/1399.jpg /content/Leaves2/08/13868.jpg\n","        !cp /content/Leaves2/08/1400.jpg /content/Leaves2/08/13869.jpg\n","        !cp /content/Leaves2/08/1401.jpg /content/Leaves2/08/138610.jpg\n","        !cp /content/Leaves2/08/1402.jpg /content/Leaves2/08/138611.jpg\n","        !cp /content/Leaves2/08/1403.jpg /content/Leaves2/08/138612.jpg\n","        !cp /content/Leaves2/08/1404.jpg /content/Leaves2/08/138613.jpg\n","    \n","    !mv /content/Leaves2/33 /content/Leaves2/00\n","    !mv /content/Leaves2/32 /content/Leaves2/13\n","\n","\n","\n","def prepareflaviaTxtdata():\n","    !mv /content/flaviatxt/33.txt /content/flaviatxt/0.txt\n","    !mv /content/flaviatxt/32.txt /content/flaviatxt/13.txt\n","    # @title 准备flavia文本数据\n","    # sometimes, memory is full, so we need to free it.\n","    gc.collect()\n","\n","    # !cp /content/drive/MyDrive/dataset/Flavia/flaviatxt.zip ./\n","    # !mkdir flaviatxt\n","    # !unzip -o -d flaviatxt flaviatxt.zip\n","\n","\n","prepareflaviaPicdata()\n","prepareflaviaTxtdata()\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Leaves/\n","Leaves/3529.jpg\n","Leaves/3563.jpg\n","Leaves/3411.jpg\n","Leaves/2339.jpg\n","Leaves/3571.jpg\n","Leaves/2515.jpg\n","Leaves/1061.jpg\n","Leaves/1389.jpg\n","Leaves/2123.jpg\n","Leaves/2423.jpg\n","Leaves/2154.jpg\n","Leaves/2176.jpg\n","Leaves/1360.jpg\n","Leaves/1070.jpg\n","Leaves/1441.jpg\n","Leaves/2234.jpg\n","Leaves/3421.jpg\n","Leaves/3337.jpg\n","Leaves/2500.jpg\n","Leaves/1595.jpg\n","Leaves/2557.jpg\n","Leaves/2470.jpg\n","Leaves/3353.jpg\n","Leaves/1122.jpg\n","Leaves/1260.jpg\n","Leaves/2675.jpg\n","Leaves/2190.jpg\n","Leaves/1049.jpg\n","Leaves/1028.jpg\n","Leaves/3167.jpg\n","Leaves/3345.jpg\n","Leaves/3151.jpg\n","Leaves/1436.jpg\n","Leaves/1439.jpg\n","Leaves/3315.jpg\n","Leaves/2411.jpg\n","Leaves/2172.jpg\n","Leaves/3091.jpg\n","Leaves/3184.jpg\n","Leaves/3501.jpg\n","Leaves/1309.jpg\n","Leaves/3146.jpg\n","Leaves/1481.jpg\n","Leaves/1486.jpg\n","Leaves/3616.jpg\n","Leaves/1544.jpg\n","Leaves/1151.jpg\n","Leaves/1321.jpg\n","Leaves/2247.jpg\n","Leaves/1279.jpg\n","Leaves/2106.jpg\n","Leaves/1231.jpg\n","Leaves/1400.jpg\n","Leaves/1549.jpg\n","Leaves/2340.jpg\n","Leaves/2619.jpg\n","Leaves/3142.jpg\n","Leaves/2452.jpg\n","Leaves/2406.jpg\n","Leaves/1608.jpg\n","Leaves/1531.jpg\n","Leaves/1406.jpg\n","Leaves/1518.jpg\n","Leaves/2533.jpg\n","Leaves/1302.jpg\n","Leaves/3543.jpg\n","Leaves/3454.jpg\n","Leaves/3611.jpg\n","Leaves/2188.jpg\n","Leaves/2259.jpg\n","Leaves/3056.jpg\n","Leaves/3403.jpg\n","Leaves/2273.jpg\n","Leaves/2132.jpg\n","Leaves/3412.jpg\n","Leaves/3375.jpg\n","Leaves/2618.jpg\n","Leaves/2049.jpg\n","Leaves/2447.jpg\n","Leaves/2227.jpg\n","Leaves/2643.jpg\n","Leaves/2008.jpg\n","Leaves/1345.jpg\n","Leaves/2669.jpg\n","Leaves/3027.jpg\n","Leaves/1504.jpg\n","Leaves/1519.jpg\n","Leaves/2320.jpg\n","Leaves/3434.jpg\n","Leaves/3389.jpg\n","Leaves/3156.jpg\n","Leaves/1408.jpg\n","Leaves/2193.jpg\n","Leaves/3445.jpg\n","Leaves/2214.jpg\n","Leaves/3200.jpg\n","Leaves/3530.jpg\n","Leaves/3245.jpg\n","Leaves/2654.jpg\n","Leaves/1214.jpg\n","Leaves/1113.jpg\n","Leaves/1007.jpg\n","Leaves/2617.jpg\n","Leaves/1589.jpg\n","Leaves/1201.jpg\n","Leaves/3004.jpg\n","Leaves/3354.jpg\n","Leaves/1606.jpg\n","Leaves/2114.jpg\n","Leaves/1091.jpg\n","Leaves/2198.jpg\n","Leaves/2203.jpg\n","Leaves/2440.jpg\n","Leaves/3324.jpg\n","Leaves/1357.jpg\n","Leaves/3103.jpg\n","Leaves/1052.jpg\n","Leaves/2479.jpg\n","Leaves/2572.jpg\n","Leaves/1066.jpg\n","Leaves/2638.jpg\n","Leaves/1444.jpg\n","Leaves/3376.jpg\n","Leaves/2389.jpg\n","Leaves/2568.jpg\n","Leaves/1601.jpg\n","Leaves/2027.jpg\n","Leaves/3070.jpg\n","Leaves/1269.jpg\n","Leaves/2255.jpg\n","Leaves/2001.jpg\n","Leaves/1225.jpg\n","Leaves/2541.jpg\n","Leaves/1536.jpg\n","Leaves/1508.jpg\n","Leaves/3128.jpg\n","Leaves/2237.jpg\n","Leaves/2603.jpg\n","Leaves/1303.jpg\n","Leaves/2162.jpg\n","Leaves/3359.jpg\n","Leaves/3489.jpg\n","Leaves/2474.jpg\n","Leaves/2503.jpg\n","Leaves/1538.jpg\n","Leaves/1199.jpg\n","Leaves/3283.jpg\n","Leaves/2107.jpg\n","Leaves/1131.jpg\n","Leaves/3096.jpg\n","Leaves/3326.jpg\n","Leaves/3374.jpg\n","Leaves/2555.jpg\n","Leaves/2630.jpg\n","Leaves/1588.jpg\n","Leaves/1140.jpg\n","Leaves/1179.jpg\n","Leaves/1257.jpg\n","Leaves/1298.jpg\n","Leaves/2242.jpg\n","Leaves/3039.jpg\n","Leaves/3484.jpg\n","Leaves/3405.jpg\n","Leaves/2472.jpg\n","Leaves/3169.jpg\n","Leaves/3274.jpg\n","Leaves/3061.jpg\n","Leaves/1551.jpg\n","Leaves/1425.jpg\n","Leaves/3621.jpg\n","Leaves/3204.jpg\n","Leaves/1571.jpg\n","Leaves/3124.jpg\n","Leaves/3075.jpg\n","Leaves/3163.jpg\n","Leaves/1382.jpg\n","Leaves/1056.jpg\n","Leaves/2055.jpg\n","Leaves/3393.jpg\n","Leaves/1397.jpg\n","Leaves/2051.jpg\n","Leaves/1447.jpg\n","Leaves/2156.jpg\n","Leaves/1499.jpg\n","Leaves/1158.jpg\n","Leaves/1474.jpg\n","Leaves/1575.jpg\n","Leaves/1072.jpg\n","Leaves/3223.jpg\n","Leaves/3220.jpg\n","Leaves/1244.jpg\n","Leaves/1341.jpg\n","Leaves/1064.jpg\n","Leaves/1376.jpg\n","Leaves/1316.jpg\n","Leaves/3230.jpg\n","Leaves/1527.jpg\n","Leaves/1077.jpg\n","Leaves/2549.jpg\n","Leaves/2125.jpg\n","Leaves/1385.jpg\n","Leaves/3221.jpg\n","Leaves/3232.jpg\n","Leaves/2251.jpg\n","Leaves/3132.jpg\n","Leaves/2596.jpg\n","Leaves/1271.jpg\n","Leaves/1155.jpg\n","Leaves/1003.jpg\n","Leaves/3224.jpg\n","Leaves/1308.jpg\n","Leaves/2205.jpg\n","Leaves/2292.jpg\n","Leaves/1018.jpg\n","Leaves/2384.jpg\n","Leaves/3473.jpg\n","Leaves/2396.jpg\n","Leaves/3093.jpg\n","Leaves/2602.jpg\n","Leaves/1522.jpg\n","Leaves/1386.jpg\n","Leaves/3073.jpg\n","Leaves/3572.jpg\n","Leaves/3358.jpg\n","Leaves/2349.jpg\n","Leaves/1426.jpg\n","Leaves/3318.jpg\n","Leaves/3317.jpg\n","Leaves/2041.jpg\n","Leaves/1094.jpg\n","Leaves/1255.jpg\n","Leaves/3544.jpg\n","Leaves/3208.jpg\n","Leaves/1340.jpg\n","Leaves/1291.jpg\n","Leaves/3534.jpg\n","Leaves/2457.jpg\n","Leaves/1030.jpg\n","Leaves/2436.jpg\n","Leaves/2220.jpg\n","Leaves/1087.jpg\n","Leaves/2365.jpg\n","Leaves/2483.jpg\n","Leaves/3183.jpg\n","Leaves/1005.jpg\n","Leaves/1275.jpg\n","Leaves/3095.jpg\n","Leaves/3293.jpg\n","Leaves/3092.jpg\n","Leaves/1020.jpg\n","Leaves/3378.jpg\n","Leaves/2099.jpg\n","Leaves/1211.jpg\n","Leaves/2032.jpg\n","Leaves/1461.jpg\n","Leaves/2082.jpg\n","Leaves/1431.jpg\n","Leaves/2471.jpg\n","Leaves/1242.jpg\n","Leaves/2381.jpg\n","Leaves/2341.jpg\n","Leaves/2302.jpg\n","Leaves/2244.jpg\n","Leaves/1582.jpg\n","Leaves/1429.jpg\n","Leaves/2245.jpg\n","Leaves/1240.jpg\n","Leaves/2433.jpg\n","Leaves/1156.jpg\n","Leaves/3060.jpg\n","Leaves/2584.jpg\n","Leaves/1609.jpg\n","Leaves/3396.jpg\n","Leaves/2554.jpg\n","Leaves/1042.jpg\n","Leaves/2104.jpg\n","Leaves/2258.jpg\n","Leaves/2298.jpg\n","Leaves/1570.jpg\n","Leaves/2633.jpg\n","Leaves/1226.jpg\n","Leaves/3119.jpg\n","Leaves/3280.jpg\n","Leaves/3137.jpg\n","Leaves/1092.jpg\n","Leaves/2200.jpg\n","Leaves/2392.jpg\n","Leaves/3098.jpg\n","Leaves/3250.jpg\n","Leaves/1503.jpg\n","Leaves/2511.jpg\n","Leaves/3341.jpg\n","Leaves/1277.jpg\n","Leaves/3236.jpg\n","Leaves/1209.jpg\n","Leaves/2417.jpg\n","Leaves/3545.jpg\n","Leaves/2263.jpg\n","Leaves/1530.jpg\n","Leaves/1614.jpg\n","Leaves/2328.jpg\n","Leaves/3265.jpg\n","Leaves/3066.jpg\n","Leaves/3162.jpg\n","Leaves/1086.jpg\n","Leaves/1012.jpg\n","Leaves/1555.jpg\n","Leaves/3425.jpg\n","Leaves/2419.jpg\n","Leaves/2006.jpg\n","Leaves/2437.jpg\n","Leaves/2531.jpg\n","Leaves/1543.jpg\n","Leaves/1218.jpg\n","Leaves/1411.jpg\n","Leaves/1031.jpg\n","Leaves/2357.jpg\n","Leaves/3198.jpg\n","Leaves/2297.jpg\n","Leaves/1243.jpg\n","Leaves/3243.jpg\n","Leaves/3319.jpg\n","Leaves/3164.jpg\n","Leaves/1207.jpg\n","Leaves/2502.jpg\n","Leaves/3217.jpg\n","Leaves/3492.jpg\n","Leaves/1335.jpg\n","Leaves/1525.jpg\n","Leaves/1390.jpg\n","Leaves/2670.jpg\n","Leaves/3502.jpg\n","Leaves/2091.jpg\n","Leaves/2076.jpg\n","Leaves/2336.jpg\n","Leaves/3356.jpg\n","Leaves/1133.jpg\n","Leaves/1424.jpg\n","Leaves/3035.jpg\n","Leaves/2477.jpg\n","Leaves/1513.jpg\n","Leaves/3189.jpg\n","Leaves/3525.jpg\n","Leaves/1312.jpg\n","Leaves/2405.jpg\n","Leaves/1197.jpg\n","Leaves/1516.jpg\n","Leaves/3328.jpg\n","Leaves/3043.jpg\n","Leaves/1117.jpg\n","Leaves/3541.jpg\n","Leaves/2352.jpg\n","Leaves/1256.jpg\n","Leaves/3423.jpg\n","Leaves/2408.jpg\n","Leaves/2105.jpg\n","Leaves/2274.jpg\n","Leaves/2456.jpg\n","Leaves/2407.jpg\n","Leaves/3383.jpg\n","Leaves/1078.jpg\n","Leaves/2520.jpg\n","Leaves/2629.jpg\n","Leaves/1446.jpg\n","Leaves/1511.jpg\n","Leaves/1480.jpg\n","Leaves/3322.jpg\n","Leaves/1027.jpg\n","Leaves/2516.jpg\n","Leaves/1228.jpg\n","Leaves/1174.jpg\n","Leaves/2110.jpg\n","Leaves/1135.jpg\n","Leaves/2594.jpg\n","Leaves/2111.jpg\n","Leaves/2473.jpg\n","Leaves/2151.jpg\n","Leaves/3536.jpg\n","Leaves/1359.jpg\n","Leaves/3429.jpg\n","Leaves/3363.jpg\n","Leaves/2300.jpg\n","Leaves/2079.jpg\n","Leaves/3284.jpg\n","Leaves/3598.jpg\n","Leaves/2489.jpg\n","Leaves/3090.jpg\n","Leaves/1542.jpg\n","Leaves/1249.jpg\n","Leaves/3399.jpg\n","Leaves/1159.jpg\n","Leaves/2305.jpg\n","Leaves/1247.jpg\n","Leaves/2646.jpg\n","Leaves/2161.jpg\n","Leaves/3040.jpg\n","Leaves/3123.jpg\n","Leaves/1354.jpg\n","Leaves/3516.jpg\n","Leaves/3339.jpg\n","Leaves/2581.jpg\n","Leaves/2314.jpg\n","Leaves/3045.jpg\n","Leaves/2652.jpg\n","Leaves/3468.jpg\n","Leaves/2246.jpg\n","Leaves/1485.jpg\n","Leaves/1221.jpg\n","Leaves/1432.jpg\n","Leaves/2257.jpg\n","Leaves/2222.jpg\n","Leaves/3540.jpg\n","Leaves/1167.jpg\n","Leaves/2454.jpg\n","Leaves/2657.jpg\n","Leaves/1367.jpg\n","Leaves/3101.jpg\n","Leaves/2301.jpg\n","Leaves/1405.jpg\n","Leaves/2347.jpg\n","Leaves/2317.jpg\n","Leaves/3115.jpg\n","Leaves/2080.jpg\n","Leaves/3436.jpg\n","Leaves/1014.jpg\n","Leaves/1387.jpg\n","Leaves/2386.jpg\n","Leaves/1610.jpg\n","Leaves/2011.jpg\n","Leaves/3179.jpg\n","Leaves/1427.jpg\n","Leaves/2576.jpg\n","Leaves/2414.jpg\n","Leaves/3038.jpg\n","Leaves/2639.jpg\n","Leaves/3068.jpg\n","Leaves/1307.jpg\n","Leaves/3312.jpg\n","Leaves/3286.jpg\n","Leaves/1548.jpg\n","Leaves/1148.jpg\n","Leaves/2319.jpg\n","Leaves/2185.jpg\n","Leaves/2248.jpg\n","Leaves/3472.jpg\n","Leaves/2169.jpg\n","Leaves/1454.jpg\n","Leaves/2121.jpg\n","Leaves/1090.jpg\n","Leaves/1067.jpg\n","Leaves/2084.jpg\n","Leaves/3402.jpg\n","Leaves/3438.jpg\n","Leaves/3084.jpg\n","Leaves/1137.jpg\n","Leaves/3554.jpg\n","Leaves/2194.jpg\n","Leaves/1071.jpg\n","Leaves/1186.jpg\n","Leaves/3573.jpg\n","Leaves/2098.jpg\n","Leaves/1535.jpg\n","Leaves/2265.jpg\n","Leaves/2133.jpg\n","Leaves/3238.jpg\n","Leaves/3490.jpg\n","Leaves/3059.jpg\n","Leaves/1139.jpg\n","Leaves/1528.jpg\n","Leaves/2097.jpg\n","Leaves/3607.jpg\n","Leaves/1149.jpg\n","Leaves/3133.jpg\n","Leaves/2513.jpg\n","Leaves/3409.jpg\n","Leaves/3302.jpg\n","Leaves/3384.jpg\n","Leaves/2239.jpg\n","Leaves/2018.jpg\n","Leaves/2413.jpg\n","Leaves/1459.jpg\n","Leaves/3385.jpg\n","Leaves/3173.jpg\n","Leaves/3050.jpg\n","Leaves/3538.jpg\n","Leaves/1060.jpg\n","Leaves/3185.jpg\n","Leaves/1053.jpg\n","Leaves/2539.jpg\n","Leaves/1038.jpg\n","Leaves/2482.jpg\n","Leaves/2348.jpg\n","Leaves/2182.jpg\n","Leaves/3494.jpg\n","Leaves/2215.jpg\n","Leaves/1289.jpg\n","Leaves/2269.jpg\n","Leaves/1401.jpg\n","Leaves/3456.jpg\n","Leaves/2641.jpg\n","Leaves/3136.jpg\n","Leaves/3380.jpg\n","Leaves/3157.jpg\n","Leaves/2553.jpg\n","Leaves/1089.jpg\n","Leaves/1136.jpg\n","Leaves/3019.jpg\n","Leaves/3168.jpg\n","Leaves/3088.jpg\n","Leaves/1266.jpg\n","Leaves/2088.jpg\n","Leaves/1164.jpg\n","Leaves/3531.jpg\n","Leaves/3537.jpg\n","Leaves/2204.jpg\n","Leaves/2604.jpg\n","Leaves/1276.jpg\n","Leaves/3343.jpg\n","Leaves/2377.jpg\n","Leaves/3524.jpg\n","Leaves/2379.jpg\n","Leaves/1381.jpg\n","Leaves/1584.jpg\n","Leaves/2109.jpg\n","Leaves/1224.jpg\n","Leaves/3515.jpg\n","Leaves/3606.jpg\n","Leaves/3294.jpg\n","Leaves/1356.jpg\n","Leaves/3013.jpg\n","Leaves/1611.jpg\n","Leaves/2063.jpg\n","Leaves/2563.jpg\n","Leaves/2040.jpg\n","Leaves/2191.jpg\n","Leaves/3379.jpg\n","Leaves/1305.jpg\n","Leaves/3049.jpg\n","Leaves/2660.jpg\n","Leaves/3578.jpg\n","Leaves/2118.jpg\n","Leaves/3237.jpg\n","Leaves/3617.jpg\n","Leaves/1261.jpg\n","Leaves/2590.jpg\n","Leaves/3177.jpg\n","Leaves/1248.jpg\n","Leaves/3205.jpg\n","Leaves/2124.jpg\n","Leaves/1506.jpg\n","Leaves/3057.jpg\n","Leaves/1580.jpg\n","Leaves/2611.jpg\n","Leaves/3408.jpg\n","Leaves/2538.jpg\n","Leaves/2211.jpg\n","Leaves/2416.jpg\n","Leaves/3166.jpg\n","Leaves/3507.jpg\n","Leaves/3470.jpg\n","Leaves/2635.jpg\n","Leaves/2221.jpg\n","Leaves/1198.jpg\n","Leaves/2565.jpg\n","Leaves/2353.jpg\n","Leaves/3477.jpg\n","Leaves/2412.jpg\n","Leaves/1081.jpg\n","Leaves/1313.jpg\n","Leaves/3297.jpg\n","Leaves/1097.jpg\n","Leaves/3290.jpg\n","Leaves/2035.jpg\n","Leaves/1033.jpg\n","Leaves/1468.jpg\n","Leaves/2095.jpg\n","Leaves/2283.jpg\n","Leaves/1181.jpg\n","Leaves/1206.jpg\n","Leaves/3054.jpg\n","Leaves/3014.jpg\n","Leaves/3031.jpg\n","Leaves/2398.jpg\n","Leaves/2015.jpg\n","Leaves/1015.jpg\n","Leaves/1019.jpg\n","Leaves/1040.jpg\n","Leaves/3586.jpg\n","Leaves/1422.jpg\n","Leaves/1100.jpg\n","Leaves/1393.jpg\n","Leaves/2595.jpg\n","Leaves/3334.jpg\n","Leaves/3523.jpg\n","Leaves/2030.jpg\n","Leaves/1600.jpg\n","Leaves/1191.jpg\n","Leaves/3235.jpg\n","Leaves/2588.jpg\n","Leaves/3110.jpg\n","Leaves/2024.jpg\n","Leaves/2509.jpg\n","Leaves/2484.jpg\n","Leaves/3170.jpg\n","Leaves/1490.jpg\n","Leaves/2143.jpg\n","Leaves/1232.jpg\n","Leaves/1384.jpg\n","Leaves/2150.jpg\n","Leaves/1435.jpg\n","Leaves/2048.jpg\n","Leaves/2290.jpg\n","Leaves/1602.jpg\n","Leaves/1034.jpg\n","Leaves/1118.jpg\n","Leaves/3034.jpg\n","Leaves/3592.jpg\n","Leaves/1222.jpg\n","Leaves/3532.jpg\n","Leaves/2653.jpg\n","Leaves/1105.jpg\n","Leaves/1445.jpg\n","Leaves/2021.jpg\n","Leaves/1334.jpg\n","Leaves/2672.jpg\n","Leaves/3360.jpg\n","Leaves/2343.jpg\n","Leaves/2113.jpg\n","Leaves/2521.jpg\n","Leaves/3211.jpg\n","Leaves/2426.jpg\n","Leaves/3246.jpg\n","Leaves/3414.jpg\n","Leaves/3148.jpg\n","Leaves/3596.jpg\n","Leaves/3278.jpg\n","Leaves/1347.jpg\n","Leaves/2620.jpg\n","Leaves/3264.jpg\n","Leaves/2358.jpg\n","Leaves/2007.jpg\n","Leaves/3364.jpg\n","Leaves/2393.jpg\n","Leaves/1442.jpg\n","Leaves/1142.jpg\n","Leaves/2649.jpg\n","Leaves/3474.jpg\n","Leaves/3215.jpg\n","Leaves/2573.jpg\n","Leaves/2428.jpg\n","Leaves/1088.jpg\n","Leaves/1128.jpg\n","Leaves/1460.jpg\n","Leaves/2401.jpg\n","Leaves/1416.jpg\n","Leaves/3392.jpg\n","Leaves/3267.jpg\n","Leaves/2546.jpg\n","Leaves/3420.jpg\n","Leaves/3512.jpg\n","Leaves/1310.jpg\n","Leaves/3372.jpg\n","Leaves/1324.jpg\n","Leaves/3518.jpg\n","Leaves/2226.jpg\n","Leaves/3568.jpg\n","Leaves/1473.jpg\n","Leaves/3496.jpg\n","Leaves/1489.jpg\n","Leaves/2196.jpg\n","Leaves/1285.jpg\n","Leaves/3037.jpg\n","Leaves/2560.jpg\n","Leaves/1062.jpg\n","Leaves/2427.jpg\n","Leaves/2044.jpg\n","Leaves/3085.jpg\n","Leaves/2644.jpg\n","Leaves/3533.jpg\n","Leaves/3350.jpg\n","Leaves/1246.jpg\n","Leaves/2029.jpg\n","Leaves/1371.jpg\n","Leaves/2512.jpg\n","Leaves/1567.jpg\n","Leaves/1025.jpg\n","Leaves/1428.jpg\n","Leaves/1121.jpg\n","Leaves/3175.jpg\n","Leaves/2053.jpg\n","Leaves/3116.jpg\n","Leaves/1502.jpg\n","Leaves/2582.jpg\n","Leaves/1044.jpg\n","Leaves/2310.jpg\n","Leaves/2187.jpg\n","Leaves/3078.jpg\n","Leaves/3613.jpg\n","Leaves/2199.jpg\n","Leaves/1559.jpg\n","Leaves/3397.jpg\n","Leaves/3107.jpg\n","Leaves/3587.jpg\n","Leaves/1050.jpg\n","Leaves/1361.jpg\n","Leaves/1593.jpg\n","Leaves/2499.jpg\n","Leaves/1250.jpg\n","Leaves/2313.jpg\n","Leaves/3522.jpg\n","Leaves/2047.jpg\n","Leaves/3332.jpg\n","Leaves/3325.jpg\n","Leaves/2627.jpg\n","Leaves/2586.jpg\n","Leaves/1200.jpg\n","Leaves/3055.jpg\n","Leaves/3410.jpg\n","Leaves/2009.jpg\n","Leaves/3113.jpg\n","Leaves/2493.jpg\n","Leaves/2526.jpg\n","Leaves/2277.jpg\n","Leaves/2561.jpg\n","Leaves/3558.jpg\n","Leaves/3333.jpg\n","Leaves/3355.jpg\n","Leaves/1414.jpg\n","Leaves/1111.jpg\n","Leaves/3079.jpg\n","Leaves/2656.jpg\n","Leaves/3335.jpg\n","Leaves/3381.jpg\n","Leaves/1022.jpg\n","Leaves/2230.jpg\n","Leaves/2664.jpg\n","Leaves/3313.jpg\n","Leaves/1147.jpg\n","Leaves/1160.jpg\n","Leaves/1448.jpg\n","Leaves/2387.jpg\n","Leaves/3063.jpg\n","Leaves/1552.jpg\n","Leaves/1239.jpg\n","Leaves/2037.jpg\n","Leaves/2334.jpg\n","Leaves/3310.jpg\n","Leaves/3288.jpg\n","Leaves/3130.jpg\n","Leaves/3006.jpg\n","Leaves/2597.jpg\n","Leaves/2333.jpg\n","Leaves/1526.jpg\n","Leaves/3147.jpg\n","Leaves/2544.jpg\n","Leaves/2148.jpg\n","Leaves/2404.jpg\n","Leaves/1346.jpg\n","Leaves/2056.jpg\n","Leaves/2431.jpg\n","Leaves/3314.jpg\n","Leaves/1217.jpg\n","Leaves/2064.jpg\n","Leaves/3214.jpg\n","Leaves/1450.jpg\n","Leaves/1353.jpg\n","Leaves/1229.jpg\n","Leaves/2126.jpg\n","Leaves/3201.jpg\n","Leaves/1453.jpg\n","Leaves/3187.jpg\n","Leaves/1556.jpg\n","Leaves/1187.jpg\n","Leaves/3422.jpg\n","Leaves/3222.jpg\n","Leaves/2667.jpg\n","Leaves/1395.jpg\n","Leaves/3083.jpg\n","Leaves/3097.jpg\n","Leaves/1320.jpg\n","Leaves/2175.jpg\n","Leaves/2655.jpg\n","Leaves/2020.jpg\n","Leaves/3196.jpg\n","Leaves/2042.jpg\n","Leaves/2031.jpg\n","Leaves/2287.jpg\n","Leaves/3118.jpg\n","Leaves/1323.jpg\n","Leaves/1024.jpg\n","Leaves/3508.jpg\n","Leaves/2281.jpg\n","Leaves/3011.jpg\n","Leaves/3135.jpg\n","Leaves/3369.jpg\n","Leaves/2507.jpg\n","Leaves/3225.jpg\n","Leaves/3020.jpg\n","Leaves/1293.jpg\n","Leaves/1278.jpg\n","Leaves/3330.jpg\n","Leaves/1282.jpg\n","Leaves/2116.jpg\n","Leaves/2490.jpg\n","Leaves/3289.jpg\n","Leaves/1377.jpg\n","Leaves/2235.jpg\n","Leaves/3268.jpg\n","Leaves/2458.jpg\n","Leaves/2362.jpg\n","Leaves/3127.jpg\n","Leaves/3159.jpg\n","Leaves/2232.jpg\n","Leaves/3600.jpg\n","Leaves/3469.jpg\n","Leaves/2066.jpg\n","Leaves/1166.jpg\n","Leaves/1562.jpg\n","Leaves/1176.jpg\n","Leaves/3192.jpg\n","Leaves/1413.jpg\n","Leaves/3017.jpg\n","Leaves/1372.jpg\n","Leaves/2567.jpg\n","Leaves/1344.jpg\n","Leaves/1170.jpg\n","Leaves/1574.jpg\n","Leaves/3338.jpg\n","Leaves/3589.jpg\n","Leaves/2532.jpg\n","Leaves/3253.jpg\n","Leaves/1545.jpg\n","Leaves/1336.jpg\n","Leaves/2019.jpg\n","Leaves/1017.jpg\n","Leaves/1196.jpg\n","Leaves/1287.jpg\n","Leaves/2460.jpg\n","Leaves/2497.jpg\n","Leaves/3346.jpg\n","Leaves/1327.jpg\n","Leaves/2249.jpg\n","Leaves/3048.jpg\n","Leaves/2060.jpg\n","Leaves/1565.jpg\n","Leaves/3463.jpg\n","Leaves/3551.jpg\n","Leaves/3435.jpg\n","Leaves/3269.jpg\n","Leaves/2410.jpg\n","Leaves/3581.jpg\n","Leaves/2100.jpg\n","Leaves/2267.jpg\n","Leaves/3462.jpg\n","Leaves/1104.jpg\n","Leaves/3451.jpg\n","Leaves/3465.jpg\n","Leaves/1515.jpg\n","Leaves/1500.jpg\n","Leaves/3044.jpg\n","Leaves/3377.jpg\n","Leaves/2564.jpg\n","Leaves/2219.jpg\n","Leaves/3476.jpg\n","Leaves/2335.jpg\n","Leaves/3583.jpg\n","Leaves/3279.jpg\n","Leaves/1505.jpg\n","Leaves/3086.jpg\n","Leaves/2367.jpg\n","Leaves/3591.jpg\n","Leaves/2450.jpg\n","Leaves/1177.jpg\n","Leaves/1145.jpg\n","Leaves/3012.jpg\n","Leaves/1304.jpg\n","Leaves/1497.jpg\n","Leaves/1124.jpg\n","Leaves/1388.jpg\n","Leaves/2523.jpg\n","Leaves/2243.jpg\n","Leaves/3182.jpg\n","Leaves/3149.jpg\n","Leaves/2285.jpg\n","Leaves/2181.jpg\n","Leaves/2272.jpg\n","Leaves/2093.jpg\n","Leaves/3154.jpg\n","Leaves/3361.jpg\n","Leaves/3234.jpg\n","Leaves/2330.jpg\n","Leaves/1272.jpg\n","Leaves/2318.jpg\n","Leaves/3504.jpg\n","Leaves/2469.jpg\n","Leaves/2385.jpg\n","Leaves/2505.jpg\n","Leaves/1223.jpg\n","Leaves/2374.jpg\n","Leaves/2651.jpg\n","Leaves/2438.jpg\n","Leaves/1219.jpg\n","Leaves/3464.jpg\n","Leaves/1258.jpg\n","Leaves/2376.jpg\n","Leaves/2380.jpg\n","Leaves/2293.jpg\n","Leaves/1173.jpg\n","Leaves/3081.jpg\n","Leaves/2225.jpg\n","Leaves/1355.jpg\n","Leaves/1168.jpg\n","Leaves/1183.jpg\n","Leaves/2344.jpg\n","Leaves/2578.jpg\n","Leaves/2517.jpg\n","Leaves/1041.jpg\n","Leaves/3082.jpg\n","Leaves/3281.jpg\n","Leaves/2022.jpg\n","Leaves/2501.jpg\n","Leaves/3442.jpg\n","Leaves/3099.jpg\n","Leaves/1299.jpg\n","Leaves/1569.jpg\n","Leaves/1524.jpg\n","Leaves/2178.jpg\n","Leaves/2455.jpg\n","Leaves/3194.jpg\n","Leaves/2443.jpg\n","Leaves/1410.jpg\n","Leaves/2422.jpg\n","Leaves/1523.jpg\n","Leaves/1016.jpg\n","Leaves/1592.jpg\n","Leaves/3351.jpg\n","Leaves/3500.jpg\n","Leaves/2209.jpg\n","Leaves/2264.jpg\n","Leaves/3026.jpg\n","Leaves/2626.jpg\n","Leaves/3348.jpg\n","Leaves/3271.jpg\n","Leaves/3506.jpg\n","Leaves/3491.jpg\n","Leaves/2491.jpg\n","Leaves/1326.jpg\n","Leaves/3553.jpg\n","Leaves/1540.jpg\n","Leaves/1263.jpg\n","Leaves/1458.jpg\n","Leaves/3590.jpg\n","Leaves/2282.jpg\n","Leaves/1301.jpg\n","Leaves/2163.jpg\n","Leaves/3244.jpg\n","Leaves/1598.jpg\n","Leaves/1322.jpg\n","Leaves/2607.jpg\n","Leaves/3241.jpg\n","Leaves/2551.jpg\n","Leaves/1032.jpg\n","Leaves/3074.jpg\n","Leaves/1558.jpg\n","Leaves/1107.jpg\n","Leaves/1517.jpg\n","Leaves/1564.jpg\n","Leaves/1233.jpg\n","Leaves/2388.jpg\n","Leaves/2238.jpg\n","Leaves/1603.jpg\n","Leaves/2519.jpg\n","Leaves/2180.jpg\n","Leaves/1560.jpg\n","Leaves/2086.jpg\n","Leaves/3352.jpg\n","Leaves/2153.jpg\n","Leaves/2149.jpg\n","Leaves/2228.jpg\n","Leaves/2514.jpg\n","Leaves/2498.jpg\n","Leaves/1472.jpg\n","Leaves/3298.jpg\n","Leaves/2119.jpg\n","Leaves/2355.jpg\n","Leaves/2451.jpg\n","Leaves/3546.jpg\n","Leaves/1365.jpg\n","Leaves/2609.jpg\n","Leaves/2112.jpg\n","Leaves/2322.jpg\n","Leaves/2485.jpg\n","Leaves/1026.jpg\n","Leaves/2368.jpg\n","Leaves/3165.jpg\n","Leaves/3161.jpg\n","Leaves/2043.jpg\n","Leaves/2140.jpg\n","Leaves/1083.jpg\n","Leaves/3260.jpg\n","Leaves/2363.jpg\n","Leaves/3171.jpg\n","Leaves/1292.jpg\n","Leaves/1563.jpg\n","Leaves/2540.jpg\n","Leaves/1230.jpg\n","Leaves/2177.jpg\n","Leaves/2152.jpg\n","Leaves/1315.jpg\n","Leaves/3608.jpg\n","Leaves/1178.jpg\n","Leaves/3213.jpg\n","Leaves/1106.jpg\n","Leaves/2373.jpg\n","Leaves/2476.jpg\n","Leaves/1074.jpg\n","Leaves/2146.jpg\n","Leaves/2537.jpg\n","Leaves/3561.jpg\n","Leaves/1404.jpg\n","Leaves/2059.jpg\n","Leaves/2074.jpg\n","Leaves/1075.jpg\n","Leaves/3417.jpg\n","Leaves/1006.jpg\n","Leaves/1205.jpg\n","Leaves/3309.jpg\n","Leaves/3601.jpg\n","Leaves/3362.jpg\n","Leaves/2418.jpg\n","Leaves/1470.jpg\n","Leaves/2054.jpg\n","Leaves/1420.jpg\n","Leaves/1059.jpg\n","Leaves/1171.jpg\n","Leaves/3258.jpg\n","Leaves/2135.jpg\n","Leaves/1116.jpg\n","Leaves/2395.jpg\n","Leaves/1379.jpg\n","Leaves/2069.jpg\n","Leaves/2271.jpg\n","Leaves/3172.jpg\n","Leaves/1296.jpg\n","Leaves/1554.jpg\n","Leaves/1235.jpg\n","Leaves/2467.jpg\n","Leaves/2496.jpg\n","Leaves/3304.jpg\n","Leaves/1456.jpg\n","Leaves/2131.jpg\n","Leaves/2504.jpg\n","Leaves/1220.jpg\n","Leaves/2430.jpg\n","Leaves/2645.jpg\n","Leaves/2089.jpg\n","Leaves/2129.jpg\n","Leaves/1581.jpg\n","Leaves/2487.jpg\n","Leaves/3615.jpg\n","Leaves/3527.jpg\n","Leaves/1130.jpg\n","Leaves/1348.jpg\n","Leaves/1373.jpg\n","Leaves/3018.jpg\n","Leaves/2420.jpg\n","Leaves/3141.jpg\n","Leaves/3419.jpg\n","Leaves/2640.jpg\n","Leaves/2002.jpg\n","Leaves/1338.jpg\n","Leaves/2050.jpg\n","Leaves/1579.jpg\n","Leaves/2270.jpg\n","Leaves/2168.jpg\n","Leaves/1317.jpg\n","Leaves/2663.jpg\n","Leaves/2192.jpg\n","Leaves/2535.jpg\n","Leaves/1591.jpg\n","Leaves/3058.jpg\n","Leaves/2462.jpg\n","Leaves/2004.jpg\n","Leaves/3229.jpg\n","Leaves/1479.jpg\n","Leaves/2016.jpg\n","Leaves/3444.jpg\n","Leaves/1541.jpg\n","Leaves/2316.jpg\n","Leaves/3387.jpg\n","Leaves/3371.jpg\n","Leaves/3134.jpg\n","Leaves/2038.jpg\n","Leaves/3453.jpg\n","Leaves/2624.jpg\n","Leaves/1475.jpg\n","Leaves/2668.jpg\n","Leaves/1409.jpg\n","Leaves/2304.jpg\n","Leaves/1368.jpg\n","Leaves/2659.jpg\n","Leaves/3307.jpg\n","Leaves/2354.jpg\n","Leaves/1213.jpg\n","Leaves/1281.jpg\n","Leaves/1165.jpg\n","Leaves/2311.jpg\n","Leaves/3603.jpg\n","Leaves/2028.jpg\n","Leaves/2171.jpg\n","Leaves/2332.jpg\n","Leaves/3030.jpg\n","Leaves/3520.jpg\n","Leaves/3576.jpg\n","Leaves/3145.jpg\n","Leaves/1501.jpg\n","Leaves/2299.jpg\n","Leaves/1037.jpg\n","Leaves/3447.jpg\n","Leaves/2183.jpg\n","Leaves/2446.jpg\n","Leaves/1318.jpg\n","Leaves/2612.jpg\n","Leaves/1099.jpg\n","Leaves/2329.jpg\n","Leaves/3016.jpg\n","Leaves/1058.jpg\n","Leaves/1215.jpg\n","Leaves/1351.jpg\n","Leaves/2197.jpg\n","Leaves/2145.jpg\n","Leaves/2014.jpg\n","Leaves/3047.jpg\n","Leaves/1068.jpg\n","Leaves/2331.jpg\n","Leaves/2184.jpg\n","Leaves/3493.jpg\n","Leaves/1616.jpg\n","Leaves/2481.jpg\n","Leaves/3292.jpg\n","Leaves/1483.jpg\n","Leaves/2071.jpg\n","Leaves/1350.jpg\n","Leaves/2252.jpg\n","Leaves/1055.jpg\n","Leaves/2621.jpg\n","Leaves/3514.jpg\n","Leaves/3509.jpg\n","Leaves/1352.jpg\n","Leaves/1418.jpg\n","Leaves/3131.jpg\n","Leaves/3295.jpg\n","Leaves/3180.jpg\n","Leaves/3158.jpg\n","Leaves/2445.jpg\n","Leaves/1375.jpg\n","Leaves/3252.jpg\n","Leaves/3618.jpg\n","Leaves/3370.jpg\n","Leaves/2632.jpg\n","Leaves/2122.jpg\n","Leaves/2570.jpg\n","Leaves/3400.jpg\n","Leaves/3071.jpg\n","Leaves/3439.jpg\n","Leaves/3460.jpg\n","Leaves/2522.jpg\n","Leaves/3291.jpg\n","Leaves/2268.jpg\n","Leaves/3305.jpg\n","Leaves/3336.jpg\n","Leaves/2606.jpg\n","Leaves/3001.jpg\n","Leaves/3028.jpg\n","Leaves/2566.jpg\n","Leaves/2117.jpg\n","Leaves/1157.jpg\n","Leaves/3199.jpg\n","Leaves/3579.jpg\n","Leaves/2391.jpg\n","Leaves/1102.jpg\n","Leaves/2240.jpg\n","Leaves/2308.jpg\n","Leaves/1467.jpg\n","Leaves/2369.jpg\n","Leaves/3487.jpg\n","Leaves/2364.jpg\n","Leaves/1311.jpg\n","Leaves/2312.jpg\n","Leaves/2548.jpg\n","Leaves/1449.jpg\n","Leaves/3144.jpg\n","Leaves/2075.jpg\n","Leaves/2202.jpg\n","Leaves/2372.jpg\n","Leaves/3212.jpg\n","Leaves/3277.jpg\n","Leaves/2256.jpg\n","Leaves/1141.jpg\n","Leaves/1259.jpg\n","Leaves/3296.jpg\n","Leaves/1599.jpg\n","Leaves/3580.jpg\n","Leaves/3614.jpg\n","Leaves/2067.jpg\n","Leaves/1493.jpg\n","Leaves/1054.jpg\n","Leaves/1114.jpg\n","Leaves/2092.jpg\n","Leaves/2241.jpg\n","Leaves/1004.jpg\n","Leaves/2128.jpg\n","Leaves/2370.jpg\n","Leaves/1241.jpg\n","Leaves/3373.jpg\n","Leaves/2309.jpg\n","Leaves/3458.jpg\n","Leaves/2421.jpg\n","Leaves/3105.jpg\n","Leaves/3206.jpg\n","Leaves/1465.jpg\n","Leaves/3228.jpg\n","Leaves/2465.jpg\n","Leaves/3239.jpg\n","Leaves/2486.jpg\n","Leaves/2065.jpg\n","Leaves/3340.jpg\n","Leaves/3009.jpg\n","Leaves/2101.jpg\n","Leaves/3480.jpg\n","Leaves/1234.jpg\n","Leaves/3548.jpg\n","Leaves/3593.jpg\n","Leaves/3366.jpg\n","Leaves/1204.jpg\n","Leaves/1161.jpg\n","Leaves/2429.jpg\n","Leaves/2081.jpg\n","Leaves/1477.jpg\n","Leaves/2236.jpg\n","Leaves/2605.jpg\n","Leaves/2466.jpg\n","Leaves/1319.jpg\n","Leaves/2315.jpg\n","Leaves/1510.jpg\n","Leaves/2650.jpg\n","Leaves/3357.jpg\n","Leaves/1125.jpg\n","Leaves/3588.jpg\n","Leaves/3497.jpg\n","Leaves/2036.jpg\n","Leaves/1185.jpg\n","Leaves/1294.jpg\n","Leaves/3452.jpg\n","Leaves/3104.jpg\n","Leaves/2569.jpg\n","Leaves/1407.jpg\n","Leaves/1466.jpg\n","Leaves/2642.jpg\n","Leaves/3481.jpg\n","Leaves/3427.jpg\n","Leaves/3120.jpg\n","Leaves/2424.jpg\n","Leaves/1046.jpg\n","Leaves/2321.jpg\n","Leaves/2383.jpg\n","Leaves/2350.jpg\n","Leaves/3087.jpg\n","Leaves/1469.jpg\n","Leaves/2070.jpg\n","Leaves/1521.jpg\n","Leaves/1488.jpg\n","Leaves/1539.jpg\n","Leaves/3505.jpg\n","Leaves/1103.jpg\n","Leaves/2025.jpg\n","Leaves/3287.jpg\n","Leaves/2231.jpg\n","Leaves/3557.jpg\n","Leaves/2400.jpg\n","Leaves/3270.jpg\n","Leaves/3466.jpg\n","Leaves/1146.jpg\n","Leaves/2409.jpg\n","Leaves/3174.jpg\n","Leaves/1396.jpg\n","Leaves/1001.jpg\n","Leaves/3150.jpg\n","Leaves/3069.jpg\n","Leaves/3344.jpg\n","Leaves/2399.jpg\n","Leaves/2528.jpg\n","Leaves/2625.jpg\n","Leaves/1421.jpg\n","Leaves/1265.jpg\n","Leaves/3365.jpg\n","Leaves/1482.jpg\n","Leaves/3216.jpg\n","Leaves/1193.jpg\n","Leaves/2275.jpg\n","Leaves/2366.jpg\n","Leaves/1533.jpg\n","Leaves/2136.jpg\n","Leaves/2459.jpg\n","Leaves/3033.jpg\n","Leaves/1568.jpg\n","Leaves/3062.jpg\n","Leaves/3562.jpg\n","Leaves/2648.jpg\n","Leaves/2266.jpg\n","Leaves/2260.jpg\n","Leaves/1009.jpg\n","Leaves/2206.jpg\n","Leaves/3528.jpg\n","Leaves/3566.jpg\n","Leaves/2012.jpg\n","Leaves/2432.jpg\n","Leaves/1085.jpg\n","Leaves/2488.jpg\n","Leaves/1546.jpg\n","Leaves/1328.jpg\n","Leaves/1529.jpg\n","Leaves/1590.jpg\n","Leaves/1254.jpg\n","Leaves/1572.jpg\n","Leaves/2078.jpg\n","Leaves/1325.jpg\n","Leaves/3388.jpg\n","Leaves/3178.jpg\n","Leaves/3041.jpg\n","Leaves/2543.jpg\n","Leaves/1065.jpg\n","Leaves/3485.jpg\n","Leaves/2577.jpg\n","Leaves/3597.jpg\n","Leaves/1236.jpg\n","Leaves/2623.jpg\n","Leaves/3391.jpg\n","Leaves/2083.jpg\n","Leaves/2616.jpg\n","Leaves/3426.jpg\n","Leaves/2137.jpg\n","Leaves/2338.jpg\n","Leaves/3349.jpg\n","Leaves/2210.jpg\n","Leaves/1002.jpg\n","Leaves/1227.jpg\n","Leaves/2286.jpg\n","Leaves/2475.jpg\n","Leaves/3311.jpg\n","Leaves/2674.jpg\n","Leaves/2351.jpg\n","Leaves/2461.jpg\n","Leaves/2010.jpg\n","Leaves/1286.jpg\n","Leaves/3051.jpg\n","Leaves/3240.jpg\n","Leaves/2571.jpg\n","Leaves/2195.jpg\n","Leaves/1415.jpg\n","Leaves/2250.jpg\n","Leaves/2327.jpg\n","Leaves/1268.jpg\n","Leaves/2023.jpg\n","Leaves/2463.jpg\n","Leaves/3186.jpg\n","Leaves/2575.jpg\n","Leaves/3139.jpg\n","Leaves/3443.jpg\n","Leaves/3005.jpg\n","Leaves/2068.jpg\n","Leaves/1295.jpg\n","Leaves/3517.jpg\n","Leaves/1108.jpg\n","Leaves/2155.jpg\n","Leaves/3457.jpg\n","Leaves/1557.jpg\n","Leaves/1010.jpg\n","Leaves/1514.jpg\n","Leaves/1337.jpg\n","Leaves/3526.jpg\n","Leaves/3320.jpg\n","Leaves/1430.jpg\n","Leaves/1013.jpg\n","Leaves/2550.jpg\n","Leaves/2120.jpg\n","Leaves/2276.jpg\n","Leaves/1162.jpg\n","Leaves/1069.jpg\n","Leaves/3152.jpg\n","Leaves/1043.jpg\n","Leaves/2212.jpg\n","Leaves/3112.jpg\n","Leaves/1330.jpg\n","Leaves/2637.jpg\n","Leaves/1189.jpg\n","Leaves/1398.jpg\n","Leaves/3431.jpg\n","Leaves/1339.jpg\n","Leaves/3275.jpg\n","Leaves/2094.jpg\n","Leaves/1478.jpg\n","Leaves/2647.jpg\n","Leaves/1314.jpg\n","Leaves/3015.jpg\n","Leaves/1123.jpg\n","Leaves/1175.jpg\n","Leaves/3432.jpg\n","Leaves/3450.jpg\n","Leaves/2371.jpg\n","Leaves/1188.jpg\n","Leaves/3542.jpg\n","Leaves/2636.jpg\n","Leaves/2013.jpg\n","Leaves/1280.jpg\n","Leaves/3003.jpg\n","Leaves/1096.jpg\n","Leaves/2213.jpg\n","Leaves/1216.jpg\n","Leaves/1484.jpg\n","Leaves/2529.jpg\n","Leaves/3272.jpg\n","Leaves/1057.jpg\n","Leaves/1134.jpg\n","Leaves/2115.jpg\n","Leaves/1547.jpg\n","Leaves/2585.jpg\n","Leaves/1613.jpg\n","Leaves/2142.jpg\n","Leaves/2360.jpg\n","Leaves/2296.jpg\n","Leaves/1035.jpg\n","Leaves/1402.jpg\n","Leaves/3331.jpg\n","Leaves/1329.jpg\n","Leaves/3022.jpg\n","Leaves/1553.jpg\n","Leaves/1093.jpg\n","Leaves/3117.jpg\n","Leaves/3401.jpg\n","Leaves/3259.jpg\n","Leaves/1288.jpg\n","Leaves/3549.jpg\n","Leaves/1048.jpg\n","Leaves/3007.jpg\n","Leaves/3282.jpg\n","Leaves/3100.jpg\n","Leaves/3308.jpg\n","Leaves/2591.jpg\n","Leaves/1300.jpg\n","Leaves/2294.jpg\n","Leaves/2233.jpg\n","Leaves/2216.jpg\n","Leaves/1290.jpg\n","Leaves/2072.jpg\n","Leaves/1363.jpg\n","Leaves/1364.jpg\n","Leaves/3418.jpg\n","Leaves/3437.jpg\n","Leaves/3567.jpg\n","Leaves/3499.jpg\n","Leaves/1607.jpg\n","Leaves/3467.jpg\n","Leaves/2164.jpg\n","Leaves/3475.jpg\n","Leaves/3254.jpg\n","Leaves/2542.jpg\n","Leaves/1496.jpg\n","Leaves/3207.jpg\n","Leaves/3181.jpg\n","Leaves/1008.jpg\n","Leaves/3394.jpg\n","Leaves/1419.jpg\n","Leaves/2453.jpg\n","Leaves/2005.jpg\n","Leaves/2073.jpg\n","Leaves/1150.jpg\n","Leaves/1383.jpg\n","Leaves/1331.jpg\n","Leaves/3619.jpg\n","Leaves/1494.jpg\n","Leaves/1423.jpg\n","Leaves/2556.jpg\n","Leaves/3368.jpg\n","Leaves/3535.jpg\n","Leaves/3539.jpg\n","Leaves/3111.jpg\n","Leaves/1615.jpg\n","Leaves/1143.jpg\n","Leaves/2087.jpg\n","Leaves/1434.jpg\n","Leaves/2665.jpg\n","Leaves/3433.jpg\n","Leaves/3190.jpg\n","Leaves/3262.jpg\n","Leaves/3306.jpg\n","Leaves/3126.jpg\n","Leaves/1342.jpg\n","Leaves/3276.jpg\n","Leaves/2415.jpg\n","Leaves/3242.jpg\n","Leaves/2295.jpg\n","Leaves/1585.jpg\n","Leaves/1029.jpg\n","Leaves/2593.jpg\n","Leaves/3513.jpg\n","Leaves/1129.jpg\n","Leaves/3620.jpg\n","Leaves/3109.jpg\n","Leaves/2530.jpg\n","Leaves/2628.jpg\n","Leaves/2325.jpg\n","Leaves/1495.jpg\n","Leaves/2160.jpg\n","Leaves/1333.jpg\n","Leaves/2307.jpg\n","Leaves/2189.jpg\n","Leaves/2492.jpg\n","Leaves/1594.jpg\n","Leaves/2289.jpg\n","Leaves/2634.jpg\n","Leaves/1109.jpg\n","Leaves/1433.jpg\n","Leaves/1283.jpg\n","Leaves/1471.jpg\n","Leaves/2480.jpg\n","Leaves/3461.jpg\n","Leaves/3584.jpg\n","Leaves/3251.jpg\n","Leaves/3129.jpg\n","Leaves/3559.jpg\n","Leaves/1154.jpg\n","Leaves/1462.jpg\n","Leaves/2147.jpg\n","Leaves/3398.jpg\n","Leaves/2631.jpg\n","Leaves/2342.jpg\n","Leaves/1464.jpg\n","Leaves/1605.jpg\n","Leaves/2345.jpg\n","Leaves/3424.jpg\n","Leaves/3209.jpg\n","Leaves/2518.jpg\n","Leaves/3233.jpg\n","Leaves/2157.jpg\n","Leaves/1380.jpg\n","Leaves/3521.jpg\n","Leaves/3449.jpg\n","Leaves/1120.jpg\n","Leaves/3263.jpg\n","Leaves/3008.jpg\n","Leaves/3143.jpg\n","Leaves/2608.jpg\n","Leaves/3195.jpg\n","Leaves/3226.jpg\n","Leaves/3595.jpg\n","Leaves/3367.jpg\n","Leaves/2346.jpg\n","Leaves/1253.jpg\n","Leaves/3440.jpg\n","Leaves/1238.jpg\n","Leaves/3555.jpg\n","Leaves/1509.jpg\n","Leaves/2217.jpg\n","Leaves/3585.jpg\n","Leaves/3053.jpg\n","Leaves/2402.jpg\n","Leaves/1532.jpg\n","Leaves/1237.jpg\n","Leaves/3257.jpg\n","Leaves/2600.jpg\n","Leaves/1597.jpg\n","Leaves/1274.jpg\n","Leaves/3202.jpg\n","Leaves/2262.jpg\n","Leaves/2058.jpg\n","Leaves/3575.jpg\n","Leaves/3599.jpg\n","Leaves/3046.jpg\n","Leaves/1438.jpg\n","Leaves/1192.jpg\n","Leaves/1047.jpg\n","Leaves/3382.jpg\n","Leaves/3602.jpg\n","Leaves/1119.jpg\n","Leaves/1512.jpg\n","Leaves/3285.jpg\n","Leaves/3495.jpg\n","Leaves/3102.jpg\n","Leaves/3138.jpg\n","Leaves/3574.jpg\n","Leaves/2208.jpg\n","Leaves/3519.jpg\n","Leaves/2580.jpg\n","Leaves/2661.jpg\n","Leaves/3560.jpg\n","Leaves/1520.jpg\n","Leaves/3067.jpg\n","Leaves/2442.jpg\n","Leaves/2583.jpg\n","Leaves/2448.jpg\n","Leaves/1208.jpg\n","Leaves/3488.jpg\n","Leaves/1210.jpg\n","Leaves/3430.jpg\n","Leaves/2253.jpg\n","Leaves/2510.jpg\n","Leaves/1392.jpg\n","Leaves/1403.jpg\n","Leaves/1378.jpg\n","Leaves/3612.jpg\n","Leaves/3122.jpg\n","Leaves/1358.jpg\n","Leaves/1577.jpg\n","Leaves/2323.jpg\n","Leaves/1063.jpg\n","Leaves/2622.jpg\n","Leaves/1366.jpg\n","Leaves/3155.jpg\n","Leaves/3323.jpg\n","Leaves/1576.jpg\n","Leaves/1596.jpg\n","Leaves/3125.jpg\n","Leaves/3218.jpg\n","Leaves/3273.jpg\n","Leaves/2134.jpg\n","Leaves/2306.jpg\n","Leaves/1144.jpg\n","Leaves/1251.jpg\n","Leaves/3483.jpg\n","Leaves/1153.jpg\n","Leaves/2464.jpg\n","Leaves/1391.jpg\n","Leaves/2096.jpg\n","Leaves/2173.jpg\n","Leaves/2361.jpg\n","Leaves/3386.jpg\n","Leaves/2425.jpg\n","Leaves/3197.jpg\n","Leaves/3032.jpg\n","Leaves/1369.jpg\n","Leaves/1267.jpg\n","Leaves/3077.jpg\n","Leaves/1202.jpg\n","Leaves/2158.jpg\n","Leaves/1182.jpg\n","Leaves/2378.jpg\n","Leaves/2562.jpg\n","Leaves/1101.jpg\n","Leaves/1362.jpg\n","Leaves/3550.jpg\n","Leaves/1399.jpg\n","Leaves/3577.jpg\n","Leaves/1110.jpg\n","Leaves/2592.jpg\n","Leaves/1084.jpg\n","Leaves/1394.jpg\n","Leaves/3511.jpg\n","Leaves/1172.jpg\n","Leaves/2495.jpg\n","Leaves/1573.jpg\n","Leaves/3415.jpg\n","Leaves/3482.jpg\n","Leaves/1262.jpg\n","Leaves/3029.jpg\n","Leaves/2478.jpg\n","Leaves/3316.jpg\n","Leaves/2449.jpg\n","Leaves/1343.jpg\n","Leaves/1537.jpg\n","Leaves/3160.jpg\n","Leaves/2026.jpg\n","Leaves/1023.jpg\n","Leaves/2218.jpg\n","Leaves/2127.jpg\n","Leaves/1491.jpg\n","Leaves/1203.jpg\n","Leaves/2508.jpg\n","Leaves/3446.jpg\n","Leaves/3448.jpg\n","Leaves/2359.jpg\n","Leaves/2207.jpg\n","Leaves/1132.jpg\n","Leaves/2326.jpg\n","Leaves/1045.jpg\n","Leaves/1212.jpg\n","Leaves/1163.jpg\n","Leaves/2141.jpg\n","Leaves/2525.jpg\n","Leaves/2167.jpg\n","Leaves/3347.jpg\n","Leaves/3395.jpg\n","Leaves/1126.jpg\n","Leaves/1039.jpg\n","Leaves/2303.jpg\n","Leaves/2434.jpg\n","Leaves/1587.jpg\n","Leaves/3441.jpg\n","Leaves/1443.jpg\n","Leaves/2224.jpg\n","Leaves/1463.jpg\n","Leaves/2435.jpg\n","Leaves/3052.jpg\n","Leaves/2144.jpg\n","Leaves/2138.jpg\n","Leaves/2165.jpg\n","Leaves/2003.jpg\n","Leaves/1583.jpg\n","Leaves/1297.jpg\n","Leaves/3188.jpg\n","Leaves/2524.jpg\n","Leaves/1076.jpg\n","Leaves/3255.jpg\n","Leaves/2444.jpg\n","Leaves/3300.jpg\n","Leaves/2587.jpg\n","Leaves/2574.jpg\n","Leaves/1245.jpg\n","Leaves/2558.jpg\n","Leaves/2441.jpg\n","Leaves/3406.jpg\n","Leaves/2278.jpg\n","Leaves/3080.jpg\n","Leaves/2545.jpg\n","Leaves/3455.jpg\n","Leaves/3471.jpg\n","Leaves/3094.jpg\n","Leaves/2174.jpg\n","Leaves/1080.jpg\n","Leaves/1169.jpg\n","Leaves/3153.jpg\n","Leaves/3021.jpg\n","Leaves/3610.jpg\n","Leaves/2261.jpg\n","Leaves/3413.jpg\n","Leaves/3108.jpg\n","Leaves/3140.jpg\n","Leaves/1273.jpg\n","Leaves/3065.jpg\n","Leaves/2601.jpg\n","Leaves/3498.jpg\n","Leaves/2598.jpg\n","Leaves/2179.jpg\n","Leaves/2494.jpg\n","Leaves/2170.jpg\n","Leaves/1451.jpg\n","Leaves/1412.jpg\n","Leaves/2397.jpg\n","Leaves/3605.jpg\n","Leaves/1534.jpg\n","Leaves/2061.jpg\n","Leaves/2139.jpg\n","Leaves/3256.jpg\n","Leaves/2394.jpg\n","Leaves/1115.jpg\n","Leaves/2671.jpg\n","Leaves/1306.jpg\n","Leaves/3036.jpg\n","Leaves/1550.jpg\n","Leaves/3407.jpg\n","Leaves/1578.jpg\n","Leaves/1604.jpg\n","Leaves/3219.jpg\n","Leaves/3002.jpg\n","Leaves/1152.jpg\n","Leaves/1195.jpg\n","Leaves/1021.jpg\n","Leaves/3227.jpg\n","Leaves/2223.jpg\n","Leaves/1487.jpg\n","Leaves/3266.jpg\n","Leaves/2390.jpg\n","Leaves/3570.jpg\n","Leaves/3024.jpg\n","Leaves/2284.jpg\n","Leaves/2288.jpg\n","Leaves/2468.jpg\n","Leaves/2324.jpg\n","Leaves/1612.jpg\n","Leaves/1051.jpg\n","Leaves/2547.jpg\n","Leaves/3025.jpg\n","Leaves/1138.jpg\n","Leaves/1455.jpg\n","Leaves/1437.jpg\n","Leaves/3503.jpg\n","Leaves/2045.jpg\n","Leaves/1457.jpg\n","Leaves/2673.jpg\n","Leaves/2579.jpg\n","Leaves/3106.jpg\n","Leaves/3072.jpg\n","Leaves/3552.jpg\n","Leaves/3089.jpg\n","Leaves/2103.jpg\n","Leaves/2599.jpg\n","Leaves/2046.jpg\n","Leaves/3023.jpg\n","Leaves/3042.jpg\n","Leaves/1079.jpg\n","Leaves/3547.jpg\n","Leaves/3416.jpg\n","Leaves/2130.jpg\n","Leaves/1284.jpg\n","Leaves/2552.jpg\n","Leaves/1036.jpg\n","Leaves/3390.jpg\n","Leaves/3510.jpg\n","Leaves/3231.jpg\n","Leaves/3459.jpg\n","Leaves/2201.jpg\n","Leaves/2527.jpg\n","Leaves/2254.jpg\n","Leaves/2534.jpg\n","Leaves/3486.jpg\n","Leaves/1586.jpg\n","Leaves/1098.jpg\n","Leaves/2559.jpg\n","Leaves/1270.jpg\n","Leaves/1492.jpg\n","Leaves/3301.jpg\n","Leaves/3064.jpg\n","Leaves/3404.jpg\n","Leaves/2052.jpg\n","Leaves/2403.jpg\n","Leaves/2017.jpg\n","Leaves/1561.jpg\n","Leaves/2034.jpg\n","Leaves/2039.jpg\n","Leaves/1073.jpg\n","Leaves/2229.jpg\n","Leaves/3176.jpg\n","Leaves/2090.jpg\n","Leaves/2658.jpg\n","Leaves/3569.jpg\n","Leaves/2033.jpg\n","Leaves/3329.jpg\n","Leaves/2666.jpg\n","Leaves/1184.jpg\n","Leaves/2337.jpg\n","Leaves/3479.jpg\n","Leaves/2536.jpg\n","Leaves/1349.jpg\n","Leaves/2610.jpg\n","Leaves/3582.jpg\n","Leaves/1452.jpg\n","Leaves/2057.jpg\n","Leaves/1264.jpg\n","Leaves/1370.jpg\n","Leaves/3121.jpg\n","Leaves/3249.jpg\n","Leaves/1180.jpg\n","Leaves/1440.jpg\n","Leaves/1252.jpg\n","Leaves/2102.jpg\n","Leaves/1374.jpg\n","Leaves/1095.jpg\n","Leaves/3114.jpg\n","Leaves/3261.jpg\n","Leaves/3203.jpg\n","Leaves/2159.jpg\n","Leaves/2662.jpg\n","Leaves/2506.jpg\n","Leaves/3478.jpg\n","Leaves/3191.jpg\n","Leaves/3428.jpg\n","Leaves/1507.jpg\n","Leaves/1194.jpg\n","Leaves/3247.jpg\n","Leaves/2085.jpg\n","Leaves/1498.jpg\n","Leaves/1082.jpg\n","Leaves/1566.jpg\n","Leaves/1476.jpg\n","Leaves/2291.jpg\n","Leaves/2439.jpg\n","Leaves/3303.jpg\n","Leaves/2279.jpg\n","Leaves/1417.jpg\n","Leaves/1332.jpg\n","Leaves/3594.jpg\n","Leaves/3076.jpg\n","Leaves/3604.jpg\n","Leaves/2382.jpg\n","Leaves/1011.jpg\n","Leaves/2077.jpg\n","Leaves/3299.jpg\n","Leaves/2375.jpg\n","Leaves/3609.jpg\n","Leaves/3342.jpg\n","Leaves/3556.jpg\n","Leaves/3248.jpg\n","Leaves/3193.jpg\n","Leaves/3321.jpg\n","Leaves/2356.jpg\n","Leaves/3327.jpg\n","Leaves/2280.jpg\n","Leaves/1127.jpg\n","Leaves/3210.jpg\n","Leaves/3010.jpg\n","Leaves/2062.jpg\n","Leaves/1190.jpg\n","Leaves/2166.jpg\n","Leaves/2186.jpg\n","Leaves/1112.jpg\n","Leaves/2589.jpg\n","Leaves/2108.jpg\n","01%\n","02%\n","03%\n","04%\n","05%\n","06%\n","07%\n","08%\n","09%\n","10%\n","11%\n","12%\n","13%\n","14%\n","15%\n","16%\n","17%\n","18%\n","19%\n","20%\n","21%\n","22%\n","23%\n","24%\n","25%\n","26%\n","27%\n","28%\n","29%\n","30%\n","31%\n","32%\n","33%\n","34%\n","35%\n","36%\n","37%\n","38%\n","39%\n","40%\n","41%\n","42%\n","43%\n","44%\n","45%\n","46%\n","47%\n","48%\n","49%\n","50%\n","51%\n","52%\n","53%\n","54%\n","55%\n","56%\n","57%\n","58%\n","59%\n","60%\n","61%\n","62%\n","63%\n","64%\n","65%\n","66%\n","67%\n","68%\n","69%\n","70%\n","71%\n","72%\n","73%\n","74%\n","75%\n","76%\n","77%\n","78%\n","79%\n","80%\n","81%\n","82%\n","83%\n","84%\n","85%\n","86%\n","87%\n","88%\n","89%\n","90%\n","91%\n","92%\n","93%\n","94%\n","95%\n","96%\n","97%\n","98%\n","99%\n","100%\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"cellView":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"7_svE9axlW0I","executionInfo":{"status":"ok","timestamp":1627436911548,"user_tz":-480,"elapsed":2571913,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"}},"outputId":"4f952d1b-e1d7-4fcd-ef70-0905e1726721"},"source":["#@title 训练文本模型，用于在colab上面训练model_text,拆分0\n","\n","!rm /content/model_text -fr\n","!cp /content/drive/MyDrive/data/tc_wordlist.bin ./\n","# !cp /content/drive/MyDrive/data/model_0629_1335/model_text ./ -fr\n","# !cp /content/drive/MyDrive/data/model_text_p96_07142135/ ./model_text/ -fr\n","\n","# !cp /content/drive/MyDrive/datafinal/wenbenModel/Flavia/model_text/ ./model_text -fr\n","\n","!cp /content/drive/MyDrive/data/tokenizer.bin  ./\n","\n","random.seed()\n","\n","pathofflaviatxt = r\"./flaviatxt\"\n","leaves_folder = r\"Leaves\"\n","imgrootpath = r\"./Leaves2\"\n","top_k = 10000\n","EMBEDDING_DIM = 200  # 词向量维度\n","\n","pathofflaviatxt = \"/content/flaviatxt\"\n","leaves_folder = r\"Leaves\"\n","imgrootpath = \"/content/Leaves2\"\n","\n","# @title 获取文本featrue\n","alltxtdata = \"\"\n","keyfeature = \"叶\"\n","\n","def collectalltxt(pathofflaviatxt):\n","    alltxtdata = \"\"\n","    for x in os.listdir(pathofflaviatxt):\n","        # print (x)\n","        alltxtdata += codecs.open(os.path.join(pathofflaviatxt, x), encoding=\"gbk\").read()\n","    return alltxtdata\n","alltxtdata = collectalltxt(pathofflaviatxt)\n","\n","# @title 文本分类\n","# 这个脚本用来读取腾讯词嵌入中所有的880w的词语，放到一个list中tc_wordlist\n","if os.path.isfile(\"tc_wordlist.bin\") == False:\n","    tc_w2v = codecs.open(\"Tencent_AILab_ChineseEmbedding.txt\", encoding=\"utf-8\")\n","    tc_wordlist = []\n","    x = tc_w2v.readline()  # skip first line\n","    x = tc_w2v.readline()\n","    i = 1\n","    while x != \"\":\n","        tc_wordlist.append(x.split(\" \")[0])\n","        x = tc_w2v.readline()\n","        if i % 100000 == 0:\n","            print(i)\n","        i += 1\n","    print(\"have collect tencent word embedding's words to list tc_wordlist\")\n","    print(\"len(tc_wordlist):\", len(tc_wordlist))\n","    p.dump(tc_wordlist, open(\"tc_wordlist.bin\", \"wb\"))\n","tc_wordlist = p.load(open(\"tc_wordlist.bin\", \"rb\"))\n","\n","if os.path.isfile(\"tokenizer.bin\") == False:\n","\n","    # 首先定义一个tokenizer，使用jieba及alltxtdata训练一次，\n","    # 然后发现有些词语在腾讯词嵌入里面居然没有，那么就重新把那些没有的词从jieba里面删掉。不包含英文\n","    # 然后再做一次使用jieba及alltxtdata训练，可以得到一个tokenizer里面的词都可以在腾讯词嵌入中找到。\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n","                                                      oov_token=\"<unk>\",\n","                                                      filters='')\n","    tokenizer.fit_on_texts(jieba.lcut(alltxtdata, HMM=False) + list(re.sub(r'\\r|\\n', ' ', alltxtdata)))\n","    print(\"有多少个不包含的：\", len(set(tokenizer.word_index.keys()) - set(tc_wordlist)))\n","    print(\"那些不包含的：\", (set(tokenizer.word_index.keys()) - set(tc_wordlist)))\n","    for x in (set(tokenizer.word_index.keys()) - set(tc_wordlist)):\n","        if re.sub(r\"[a-zA-Z]\", \"\", x) != \"\":\n","            jieba.del_word(x)\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n","                                                      oov_token=\"<unk>\",\n","                                                      filters='')\n","    tokenizer.fit_on_texts(jieba.lcut(alltxtdata, HMM=False) + list(re.sub(r'\\r|\\n', ' ', alltxtdata)))\n","    tokenizer.word_index['<pad>'] = 0\n","    tokenizer.index_word[0] = '<pad>'\n","    # print(len(tokenizer.word_counts))\n","    tokenizer.word_index['<start>'] = len(tokenizer.word_counts)\n","    tokenizer.index_word[len(tokenizer.word_counts)] = '<start>'\n","    tokenizer.word_index['<end>'] = len(tokenizer.word_counts) + 1\n","    tokenizer.index_word[len(tokenizer.word_counts) + 1] = '<end>'\n","    p.dump(tokenizer, open(\"tokenizer.bin\", \"wb\"))\n","tokenizer = p.load(open(\"tokenizer.bin\", \"rb\"))\n","\n","if os.path.isfile(\"embeddings_matrix.npy\") == False:\n","    # 将所有Tokenizer中用到的那些词的向量添加到embeddings_matrix中。\n","    # len(tokenizer.word_index.keys())\n","    # tokenizer.num_words\n","    vector_size = 200\n","    embeddings_matrix = np.zeros((tokenizer.num_words+1, vector_size))\n","    tok_words = tokenizer.word_index.keys()\n","    # print(tok_words)\n","    with codecs.open(\"/content/Tencent_AILab_ChineseEmbedding.txt\", encoding=\"utf-8\") as f:\n","        i = 0\n","        line_1 = f.readline()\n","        while line_1 != \"\":\n","            tc_word = line_1.split(\" \")[0]\n","            if tc_word in tok_words:\n","                # print(np.array([float(x) for x in line_1.split(\" \")[1:]]))\n","                embeddings_matrix[tokenizer.word_index[tc_word]] = np.array([float(x) for x in line_1.split(\" \")[1:]])\n","            line_1 = f.readline()\n","            i += 1\n","            if i % 100000 == 0:\n","                print(i)\n","    print(\"embeddings_matrix:\", embeddings_matrix)\n","\n","    # 定义Embedding层\n","    embedding_layer = Embedding(len(embeddings_matrix), EMBEDDING_DIM,\n","                                weights=[embeddings_matrix],  # 重点：预训练的词向量系数\n","                                input_length=100,  # 每句话的 最大长度（必须padding）\n","                                trainable=False  # 是否在 训练的过程中 更新词向量\n","                                )\n","    np.save(\"embeddings_matrix.npy\", embeddings_matrix)\n","embeddings_matrix = np.load(\"embeddings_matrix.npy\")\n","# 定义Embedding层\n","embedding_layer = Embedding(len(embeddings_matrix), EMBEDDING_DIM,\n","                            weights=[embeddings_matrix],  # 重点：预训练的词向量系数\n","                            input_length=100,  # 每句话的 最大长度（必须padding）\n","                            trainable=False  # 是否在 训练的过程中 更新词向量\n","                            )\n","\n","# model 定义\n","def create_model():\n","    if 0:\n","        model_text = keras.Sequential()\n","        # embd = keras.layers.Embedding(10000, 200)\n","        # embd = embedding_layer\n","        model_text.add(embd)\n","        gap1d = keras.layers.GlobalAveragePooling1D()\n","        model_text.add(gap1d)\n","        # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))\n","        fc1 = keras.layers.Dense(512, activation='relu')\n","        model_text.add(fc1)\n","        fc2 = keras.layers.Dense(256, activation='relu')\n","        model_text.add(fc2)\n","        fc3 = keras.layers.Dense(32, activation='softmax')\n","        model_text.add(fc3)\n","        model_text.summary()\n","\n","    model_text = keras.Sequential()\n","    embd = keras.layers.Embedding(10000+1, 200)\n","    # embd = embedding_layer\n","    model_text.add(embd)\n","    # gap1d = keras.layers.GlobalAveragePooling1D()\n","    # model_text.add(gap1d)\n","    model_text.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(400,  return_sequences=True)))\n","    model_text.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(200)))\n","    # model_text.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(400,  return_sequences=True)))\n","    # model_text.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(200)))\n","    # model_text.add(tf.keras.layers.GRU(64, return_sequences=True))\n","    # model_text.add(tf.keras.layers.GRU(32))\n","    # tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32))\n","    fc1 = keras.layers.Dense(512, activation='relu')\n","    model_text.add(fc1)\n","    fc3 = keras.layers.Dense(32, activation='softmax')\n","    model_text.add(fc3)\n","    model_text.summary()\n","    model_text.compile(optimizer='adam',\n","                       loss=tf.keras.losses.categorical_crossentropy,\n","                       metrics=['accuracy'])\n","    base_learning_rate = 0.00001\n","    model_text.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n","                       # loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","                       loss=tf.keras.losses.categorical_crossentropy,\n","                       metrics=['accuracy'])\n","    return model_text\n","model_text = create_model()\n","\n","# @title create flavia txt dataset\n","# vocab_size = 10000\n","# base_learning_rate = 0.0001\n","# model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n","#   loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","#   metrics=['accuracy'])\n","def getTokenizerAndKeyword(top_k, alltxtdata, keyfeature):\n","    #                           filters='!\"#$%&*+,:;=?@[\\]^_`{|}~ ')\n","    # tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n","    #                                                   oov_token=\"<unk>\",\n","    #                                                   filters='')\n","    # # tokenizer.fit_on_texts(alltxtdata)\n","    # tokenizer.fit_on_texts(jieba.lcut(alltxtdata) + list(alltxtdata))\n","    # tokenizer.word_index['<pad>'] = 0\n","    # tokenizer.index_word[0] = '<pad>'\n","    # # print(len(tokenizer.word_counts))\n","    # tokenizer.word_index['<start>'] = len(tokenizer.word_counts)\n","    # tokenizer.index_word[len(tokenizer.word_counts)] = '<start>'\n","    # tokenizer.word_index['<end>'] = len(tokenizer.word_counts) + 1\n","    # tokenizer.index_word[len(tokenizer.word_counts) + 1] = '<end>'\n","\n","    # 从所有的文本feature中，找到包含叶的句子，然后抽取关键词。\n","    allfiletrue = re.sub(r\"\\r|\\n\", r\"\", alltxtdata)\n","    allfeature = list(\n","        x.lstrip().rstrip().replace(\" \", \"\") if keyfeature in x else \"\" for x in re.split(r\"[。;；,]\", allfiletrue))\n","    allfeature = \",\".join([x for x in allfeature if x != \"\"])\n","    feature_keywords = jieba.analyse.extract_tags(allfeature, 50, allowPOS=(\"na\"))\n","    feature_keywords_set = set(feature_keywords)\n","    # print(jieba.lcut(feature))\n","    print(\"feature_keywords:\", feature_keywords)\n","    return tokenizer, feature_keywords, feature_keywords_set\n","tokenizer, feature_keywords, feature_keywords_set = getTokenizerAndKeyword(top_k, alltxtdata, keyfeature)\n","\n","def gettxtdata_wb(file1):\n","    global feature_keywords_set\n","    filetxt = codecs.open(os.path.join(pathofflaviatxt, file1), encoding=\"gbk\").read()\n","    filetxt = re.sub(r\"\\r|\\n\", r\"\", filetxt)\n","    feature1 = []\n","    str1 = []\n","    feature = list(\n","        x.lstrip().rstrip().replace(\" \", \"\") if \"叶\" in x else \"\" for x in re.split(r\"[。;；,：:a-zA-Z]\", filetxt))\n","    for x in feature:\n","        if x != \"\":\n","            # for x1 in x.split(\"，\"):\n","            if list(feature_keywords_set & set(jieba.lcut(x))) != []:\n","                str1.extend(x.split(\"，\"))\n","    return str1\n","\n","def getrandstr(text1, num1):\n","    # for _ in range(num1):\n","    str2 = \"\"\n","    rn = np.random.randint(low=0, high=len(text1), size=num1)\n","    # print (rn)\n","    for _i in rn:\n","        tempstr = text1[_i]\n","        # print(\"tempstr:\",tempstr)\n","        if str2 != \"\":\n","            str2 += \",\"\n","        str2 += tempstr\n","    return str2\n","\n","wb_d = {}\n","def create_wbData(liang=3):\n","    global wb_d\n","    wb_data = []\n","    wb_label = []\n","    wb_d = {}\n","    for wbf in os.listdir(pathofflaviatxt):\n","        wb_d[wbf] = gettxtdata_wb(wbf)\n","    print(wb_d)\n","    print(wb_d.keys())\n","    for _ in range(liang):\n","        for x in wb_d.keys():\n","            desc_txt = wb_d[x]\n","            feature1 = []\n","            feature1.append([y[0] for y in tokenizer.texts_to_sequences(\n","                [f\"<start>\"] + jieba.lcut(getrandstr(desc_txt, np.random.randint(10,16)), HMM=False) + [f\"<end>\"])])\n","            feature1 = tf.keras.preprocessing.sequence.pad_sequences(feature1, padding='post', maxlen=150)[0]\n","            wb_data.extend([feature1])\n","\n","            # wb_data.extend([getrandstr(desc_txt, 8)])\n","            wb_label.extend([int(x.split(\".\")[0])])\n","            # break\n","    return np.array(wb_data), np.array(wb_label)\n","\n","BATCH_SIZE = 64\n","train_d, train_l = create_wbData(1600)\n","train_l = tf.one_hot(train_l, 32)\n","test_d, test_l = create_wbData(200)\n","test_l = tf.one_hot(test_l, 32)\n","\n","# print(len(d),len(l))\n","print(train_d.shape, train_l.shape)\n","print(test_d.shape, test_l.shape)\n","\n","# train_data = keras.preprocessing.sequence.pad_sequences(d, padding='post', maxlen=256)\n","\n","\n","# if os.path.isdir(\"model_text\") == True:\n","if os.path.isdir(\"model_text\") == False:\n","    def decay(epoch):\n","        return 1e-5\n","        # if epoch < 3:\n","        #     print(\"learning rate: 1e-3\")\n","        #     return 1e-3\n","        # # elif epoch >= 5 and epoch < 10:\n","        # #     print(\"learning rate: 1e-5\")\n","        # #     return 1e-5\n","        # # elif epoch >= 15 and epoch < 20:\n","        # #     print(\"learning rate: 1e-6\")\n","        # #     return 1e-6\n","        # else:\n","        #     print(\"learning rate: 2e-7\")\n","        #     return 2e-7\n","    # @title 开始训练\n","    # 如果学习率太大，特别容易崩，一般来说0.0001就会崩；但是0.00001不会崩。\n","    # model_text = tf.keras.models.load_model(\"model_text\")\n","    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)\n","    history = model_text.fit(train_d,\n","                             train_l,\n","                             epochs=40,\n","                             batch_size=32,\n","                             validation_data=(test_d, test_l),\n","                             callbacks=[tf.keras.callbacks.LearningRateScheduler(decay), early_stop],\n","                             verbose=1)\n","    import matplotlib.pyplot as plt\n","    # history_dict = history\n","    history_dict = history.history\n","    acc = history_dict['accuracy']\n","    val_acc = history_dict['val_accuracy']\n","    loss = history_dict['loss']\n","    val_loss = history_dict['val_loss']\n","\n","    epochs = range(1, len(acc) + 1)\n","\n","    # “bo”代表 \"蓝点\"\n","    plt.plot(epochs, loss, 'bo', label='训练损失')\n","    # b代表“蓝色实线”\n","    plt.plot(epochs, val_loss, 'b', label='验证损失')\n","    plt.title('训练和验证损失')\n","    plt.xlabel('迭代次数')\n","    plt.ylabel('损失')\n","    plt.legend()\n","\n","    plt.show()\n","\n","    plt.plot(epochs, acc, 'bo', label='训练准确率')\n","    # b代表“蓝色实线”\n","    plt.plot(epochs, val_acc, 'b', label='验证准确率')\n","    plt.title('训练和验证准确率')\n","    plt.xlabel('迭代次数')\n","    plt.ylabel('')\n","    plt.legend()\n","\n","    plt.show()\n","    model_text.save(\"model_text\")\n","\n","model_text = tf.keras.models.load_model(\"model_text\")\n","# model_text.predict(\"123\")\n","# model_text.predict(train_d[:10])[0].shape\n","model_text.evaluate(test_d, test_l)\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n","Building prefix dict from the default dictionary ...\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding (Embedding)        (None, None, 200)         2000200   \n","_________________________________________________________________\n","bidirectional (Bidirectional (None, None, 800)         1923200   \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 400)               1601600   \n","_________________________________________________________________\n","dense (Dense)                (None, 512)               205312    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 32)                16416     \n","=================================================================\n","Total params: 5,746,728\n","Trainable params: 5,746,728\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"stream","text":["Dumping model to file cache /tmp/jieba.cache\n","Loading model cost 0.871 seconds.\n","Prefix dict has been built successfully.\n"],"name":"stderr"},{"output_type":"stream","text":["feature_keywords: ['无毛', '基部', '柔毛', '先端', '圆形', '楔形', '侧脉', '椭圆形', '叶片', '锯齿', '卵形', '针形', '披针', '小叶', '纸质', '裂片', '花序', '深绿色', '淡绿色', '明显', '圆状', '倒卵形', '边缘', '叶腋', '腺体', '卵状', '短枝', '叶面', '图版', '树皮', '樟脑', '革质', '全缘', '长枝', '锐尖', '小枝', '支脉', '花梗', '花枝', '苞片', '大叶', '长圆', '绿色', '直径', '心形', '灰绿色', '细小', '樟树', '落叶', '樟油']\n","{'30.txt': ['小枝、芽、叶下面', '叶柄、均密被褐色或灰褐色短绒毛（幼树的叶下面无毛）', '叶厚革质', '椭圆形', '长圆状椭圆形或倒卵状椭圆形', '长10-20厘米', '宽4-7（10）厘米', '先端钝或短钝尖', '基部楔形', '叶面深绿色', '有光泽'], '12.txt': ['叶聚生于枝顶', '二年生', '革质', '嫩时上下两面有柔毛', '以后变秃净', '倒卵形或倒卵状披针形', '长4-9厘米', '宽1.5-4厘米', '上面深绿色', '发亮、干后暗晦无光', '先端圆形或钝', '常微凹入或为微心形', '基部窄楔形', '侧脉6-8对', '在靠近边缘处相结合', '有时因侧脉间的支脉较明显而呈多脉状', '网脉稍明显', '网眼细小', '全缘', '干后反卷', '叶柄长达2厘米', '叶革质', '倒卵形', '先端圆', '簇生于枝顶呈假轮生状', '经长期栽培', '雄蕊常表现退化而不育', '结实率亦低'], '4.txt': ['叶纸质', '近圆形或三角状圆形', '长5-10厘米', '宽与长相若或略短于长', '先端急尖', '基部浅至深心形', '两面通常无毛', '嫩叶绿色', '仅叶柄略带紫色', '叶缘膜质透明', '新鲜时明显可见'], '31.txt': ['60.加杨（北方通称）加拿大杨（中国高等植物图鉴）、欧美杨、加拿大白杨、美国大叶白杨（中国树木分类学）图版22', '叶三角形或三角状卵形', '长7-10厘米', '长枝和萌枝叶较大', '长10-20厘米', '一般长大于宽', '先端渐尖', '基部截形或宽楔形', '无或有1-2腺体', '边缘半透明', '有圆锯齿', '近基部较疏', '具短缘毛', '上面暗绿色', '下面淡绿色', '叶柄侧扁而长', '带红色（苗期特明显）'], '7.txt': ['叶薄革质', '倒卵状阔披针形或长圆状倒披针形', '长8-18厘米', '宽(2)3.5-6(10)厘米', '先端渐尖或短尖', '基部楔形', '不下延', '上面无毛或沿中脉有毛', '老时完全无毛', '下面被黄褐色短柔毛', '中脉粗壮', '上面下陷', '侧脉每边6-8（10）条', '弧形', '在边缘网结并渐消失', '横脉及小脉在下面联结成明显的网状'], '1.txt': ['末级小枝具2-4叶', '叶耳不明显', '鞘口繸毛存在而为脱落性', '叶片较小较薄', '披针形', '长4-11厘米', '宽0.5-1.2厘米', '下表面在沿中脉基部具柔毛', '次脉3-6对', '再次脉9条', '花枝穗状', '长5-7厘米', '基部托以4-6片逐渐稍较大的微小鳞片状苞片', '有时花枝下方尚有1-3片近于正常发达的叶', '当此时则花枝呈顶生状', '佛焰苞通常在10片以上', '常偏于一侧', '呈整齐的复瓦状排列', '下部数片不孕而早落', '致使花枝下部露出而类似花枝之柄', '上部的边缘生纤毛及微毛', '无叶耳', '具易落的鞘口繸毛', '缩小叶小', '披针形至锥状', '每片孕性佛焰苞内具1-3枚假小穗', '颖1片', '长15-28毫米', '顶端常具锥状缩小叶有如佛焰苞', '下部、上部以及边缘常生毛茸'], '27.txt': ['叶革质、狭倒卵形、狭椭圆状倒卵形', '或倒披针形', '长8-17厘米', '宽2.5-5.5厘米', '先端短急尖', '通常尖头钝', '基部楔形', '沿叶柄稍下延', '边缘稍内卷', '下面疏生红褐色短毛', '叶柄长1-3厘米', '基部稍膨大', '托叶痕半椭圆形', '长3-4毫米', '本种与海南木莲极相似', '但叶质地比较厚', '干后两面叶脉不明显', '花柄有褐色毛', '心皮有长约1毫米的缘'], '24.txt': ['15.女贞（神农本草经）青蜡树（江苏）', '大叶蜡树（江西）', '白蜡树（广西）', '蜡树（湖南）', '叶片常绿', '革质', '卵形、长卵形或椭圆形至宽椭圆形', '长6-17厘米', '宽3-8厘米', '先端锐尖至渐尖或钝', '基部圆形或近圆形', '有时宽楔形或渐狭', '叶缘平坦', '上面光亮', '两面无毛', '中脉在上面凹入', '下面凸起', '侧脉4-9对', '两面稍凸起或有时不明显', '叶柄长1-3厘米', '上面具沟', '无毛', '花序基部苞片常与叶同型', '小苞片披针形或线形', '长0.5-6厘米', '宽0.2-1.5厘米', '凋落'], '10.txt': ['1.栾树（正字通）木栾（救荒本草）、栾华（植物名实图考）', '五乌拉叶（甘肃）', '乌拉（河北）', '乌拉胶', '黑色叶树（河北）', '石栾树（浙江）', '黑叶树、木栏牙（河南）图版', '小枝具疣点', '与叶轴、叶柄均被皱曲的短柔毛或无毛', '小叶(7-)11-18片(顶生小叶有时与最上部的一对小叶在中部以下合生)', '无柄或具极短的柄', '对生或互生', '纸质', '卵形、阔卵形至卵状披针形', '长(3-)5-10厘米', '宽3-6厘米', '顶端短尖或短渐尖', '基部钝至近截形', '边缘有不规则的钝锯齿', '齿端具小尖头', '有时近基部的齿疏离呈缺刻状', '或羽状深裂达中肋而形成二回羽状复叶', '上面仅中脉上散生皱曲的短柔毛', '下面在脉腋具髯毛', '有时小叶背面被茸毛'], '22.txt': ['叶螺旋状着生', '条状披针形', '微弯', '长7-12厘米', '宽7-10毫米', '先端尖', '基部楔形', '上面深绿色', '有光泽', '中脉显著隆起', '下面带白色、灰绿色或淡绿色', '雌球花单生叶腋', '有梗', '基部有少数苞片'], '9.txt': ['21.天竺桂（开宝本草）大叶天竺桂、竺香（浙江）', '山肉桂、土肉桂（台湾）、土桂、山玉桂（福建）图版52', '叶近对生或在枝条上部者互生', '卵圆状长圆形至长圆状披针形', '长7-10厘米', '宽3-3.5厘米', '先端锐尖至渐尖', '基部宽楔形或钝形', '革质', '上面绿色', '光亮', '下面灰绿色', '晦暗', '两面无毛', '离基三出脉', '中脉直贯叶端', '在叶片上部有少数支脉', '基生侧脉自叶基1-1.5厘米处斜向生出', '向叶缘一侧有少数支脉', '有时自叶基处生出一对稍为明显隆起的附加支脉', '中脉及侧脉两面隆起', '细脉在上面密集而呈明显的网结状但在下面呈细小的网孔', '叶柄粗壮', '腹凹背凸', '红褐色', '无毛', '枝叶及树皮可提取芳香油', '供制各种香精及香料的原料'], '28.txt': ['叶纸质', '基部近于圆形或楔形', '外貌椭圆形或倒卵形', '长6-10厘米', '通常浅3裂', '裂片向前延伸', '稀全缘', '中央裂片三角卵形', '急尖、锐尖或短渐尖', '上面深绿色', '下面黄绿色或淡绿色', '被白粉', '略被毛', '在叶脉上较密', '初生脉3条', '稀基部叶脉也发育良好', '致成5条', '在上面不显著', '在下面显著', '叶柄长2.5-5厘米', '淡紫绿色', '细瘦', '无毛', '花多数常成顶生被短柔毛的伞房花序', '直径约3厘米', '总花梗长1.5-2厘米', '开花在叶长大以后'], '26.txt': ['冬芽圆锥形', '顶端钝', '外被短柔毛', '常2-3个簇生', '中间为叶芽', '两侧为花芽', '叶片长圆披针形、椭圆披针形或倒卵状披针形', '长7-15厘米', '宽2-3.5厘米', '先端渐尖', '基部宽楔形', '上面无毛', '下面在脉腋间具少数短柔毛或无毛', '叶边具细锯齿或粗锯齿', '齿端具腺体或无腺体', '叶柄粗壮', '长1-2厘米', '常具1至数枚腺体', '有时无腺体', '花单生', '先于叶开放', '直径2.5-3.5厘米'], '23.txt': ['叶片卵状椭圆形或倒卵椭圆形', '长5-9厘米', '宽2.5-5厘米', '先端渐尖', '基部圆形', '边有渐尖单锯齿及重锯齿', '齿尖有小腺体', '上面深绿色', '无毛', '下面淡绿色', '无毛', '有侧脉6-8对', '叶柄长1-1.5厘米', '无毛', '先端有1-3圆形腺体'], '6.txt': ['158)1845.落叶小乔木', '叶纸质', '外貌圆形', '直径7-10厘米', '基部心脏形或近于心脏形稀截形', '5-9掌状分裂', '通常7裂', '裂片长圆卵形或披针形', '先端锐尖或长锐尖', '边缘具紧贴的尖锐锯齿', '裂片间的凹缺钝尖或锐尖', '深达叶片的直径的1/2或1/3', '下面淡绿色', '在叶脉的脉腋被有白色丛毛', '叶柄长4-6厘米', '细瘦', '无毛', '花紫色', '杂性', '雄花与两性花同株', '生于无毛的伞房花序', '总花梗长2-3厘米', '叶发出以后才开花'], '17.txt': ['叶片革质', '椭圆形、长椭圆形或椭圆状披针形', '长7-14.5厘米', '宽2.6-4.5厘米', '先端渐尖', '基部渐狭呈楔形或宽楔形', '全缘或通常上半部具细锯齿', '两面无毛', '腺点在两面连成小水泡状突起', '中脉在上面凹入', '下面凸起', '侧脉6-8对', '多达10对', '在上面凹入', '下面凸起', '叶柄长0.8-1.2厘米', '最长可达15厘米', '无毛', '聚伞花序簇生于叶腋', '或近于帚状', '每腋内有花多朵', '.其极大部分花序簇生于叶腋', '而在原始文献中', '', '的描述“花序簇生于叶腋”'], '29.txt': ['19．阔叶十大功劳图版47', '叶狭倒卵形至长圆形', '长27-51厘米', '宽10-20厘米', '具4-10对小叶', '最下一对小叶距叶柄基部0.5-2.5厘米', '上面暗灰绿色', '背面被白霜', '有时淡黄绿色或苍白色', '两面叶脉不显', '叶轴粗2-4毫米', '节间长3-10厘米', '小叶厚革质', '硬直', '自叶下部往上小叶渐次变长而狭', '最下一对小叶卵形', '长1.2-3.5厘米', '宽1-2厘米', '具1-2粗锯齿', '往上小叶近圆形至卵形或长圆形', '长2-10.5厘米', '宽2-6厘米', '基部阔楔形或圆形', '偏斜', '有时心形', '边缘每边具2-6粗锯齿', '先端具硬尖', '顶生小叶较大', '长7-13厘米', '宽3.5-10厘米', '具柄', '长1-6厘米'], '18.txt': ['叶在长枝上辐射伸展', '短枝之叶成簇生状（每年生出新叶约15-20枚）', '针形', '坚硬', '淡绿色或深绿色', '长2.5-5厘米', '宽1-1.5毫米', '上部较宽', '先端锐尖', '下部渐窄', '常成三棱形', '稀背脊明显', '叶之腹面两侧各有2-3条气孔线', '背面4-6条', '幼时气孔线有白粉'], '5.txt': ['小叶4-6对', '对生', '倒卵状长圆形或倒卵形', '长1.5-3厘米'], '11.txt': ['叶在长枝上互生', '在短枝上为1-4片簇生', '叶片纸质至坚纸质', '卵形、卵状椭圆形', '稀长圆状椭圆形', '长4-13(-15)厘米', '宽（3-）4-6厘米', '先端渐尖至短渐尖', '基部圆形或钝', '边缘具细锯齿', '叶面深绿色', '背面浅绿色', '两面无毛', '或叶面幼时疏被短的微柔毛', '主脉在叶面平或下陷', '疏被细小微柔毛或无毛', '背面隆起', '无毛或有时疏被细小微柔毛', '侧脉8-10对', '在叶面平坦或稍凸起', '在背面凸起', '于叶缘附近网结', '网状脉两面明显', '叶柄长1-1.2厘米', '上面具狭沟', '疏被细小微柔毛', '托叶胼胝质', '很小', '不明显', '单花或2-5花的聚伞花序', '单生或簇生于当年生或二年生枝的叶腋内', '或生于短枝的鳞片腋内或叶腋内', '总花梗长2-3毫米', '花梗长3-7毫米', '均无毛', '单生于叶腋或鳞片腋内', '花梗长6-18毫米', '无毛', '基部具2枚卵状小苞片', '本种的主要特点为小枝和花序无毛', '果柄与叶柄近等长或稍长于叶柄', '区别于它的各变种'], '14.txt': ['大叶蜡梅（广西桂林）图版3', '140.1925.落叶灌木', '高达4米', '鳞芽通常着生于第二年生的枝条叶腋内', '芽鳞片近圆形', '覆瓦状排列', '外面被短柔毛', '叶纸质至近革质', '卵圆形、椭圆形、宽椭圆形至卵状椭圆形', '有时长圆状披针形', '长5-25厘米', '宽2-8厘米', '顶端急尖至渐尖', '有时具尾尖', '基部急尖至圆形', '除叶背脉上被疏微毛外无毛', '花着生于第二年生枝条叶腋内', '先花后叶', '芳香', '直径2-4厘米'], '0.txt': ['单身复叶', '翼叶通常狭窄', '或仅有痕迹', '叶片披针形', '椭圆形或阔卵形', '大小变异较大', '顶端常有凹口', '中脉由基部至凹口附近成叉状分枝', '叶缘至少上半段通常有钝或圆裂齿', '很少全缘', '种子或多或少数', '稀无籽', '通常卵形', '顶部狭尖', '基部浑圆', '子叶深绿、淡绿或间有近于乳白色', '合点紫色', '多胚', '少有单胚'], '19.txt': ['短枝密被叶痕', '黑灰色', '短枝上亦可长出长枝', '叶扇形', '有长柄', '淡绿色', '无毛', '有多数叉状并列细脉', '顶端宽5-8厘米', '在短枝上常具波状缺刻', '在长枝上常2裂', '幼树及萌生枝上的叶常较而深裂(叶片长达13厘米', '有时裂片再分裂(这与较原始的化石种类之叶相似)', '在短枝上3-8叶呈簇生状', '秋季落叶前变为黄色.球花雌雄异株', '生于短枝顶端的鳞片状叶的腋内'], '3.txt': ['1986.落叶灌木', '高1-2米', '叶薄纸质', '近圆形或宽椭圆形', '叶片长2-6厘米', '宽1.5-3厘米', '先端圆钝', '基部楔形', '下延', '上面深绿色', '中脉和侧脉隆起', '背面淡绿色', '中脉和侧脉明显隆起', '两面网脉显著', '无毛', '叶缘平展', '每边具15-40刺齿'], '20.txt': ['972.图3674.1972.落叶灌木或小乔木', '高可达7米', '叶互生或有时对生', '纸质', '椭圆形、阔矩圆形或倒卵形', '长2.5-7厘米', '宽1.5-4厘米', '顶端短尖或钝形', '有时微凹', '基部阔楔形或近圆形', '无毛或下面沿中脉有微柔毛', '侧脉3-7对', '小脉不明显', '树皮、叶及花为强泻剂'], '13.txt': ['叶马褂状', '长4-12（18）厘米', '近基部每边具1侧裂片', '先端具2浅裂', '下面苍白色', '叶柄长4-8（-16）厘米', '叶和树皮入药'], '16.txt': ['图1156.1959.叶倒卵状矩圆形至矩圆形', '很少倒卵形', '长7-13(-16)厘米', '顶端钝或急狭而钝头', '基部宽楔形', '边缘常有较规则的波状浅钝锯齿', '侧脉6-8对', '圆锥花序通常生于具两对叶的幼技顶', '长9-15厘米', '直径8-13厘米'], '2.txt': ['1.七叶树（河北习见树木图说）图版83', '33.1960.落叶乔木', '高达25米', '树皮深褐色或灰褐色', '小枝、圆柱形', '黄褐色或灰褐色', '无毛或嫩时有微柔毛', '有圆形或椭圆形淡黄色的皮孔', '掌状复叶', '由5-7小组成', '叶柄长10-12厘米', '有灰色微柔毛', '小叶纸质', '长圆披针形至长圆倒披针形', '稀长椭圆形钾先端短锐尖', '基部楔形或阔楔形', '边缘有钝尖形的细锯齿', '长8-16厘米', '宽3-5厘米', '上面深绿色', '无毛', '下面除中肋及侧脉的基部嫩时有疏柔毛外', '其余部分无毛', '中央小叶的小叶柄长1-1.8厘米', '两侧的小叶柄长5-10毫米', '有灰色微柔毛'], '21.txt': ['叶3-4枚轮生', '下枝为对生', '窄披针形', '顶端急尖', '基部楔形', '叶缘反卷', '长11-15厘米', '宽2-2.5厘米', '叶面深绿', '无毛', '叶背浅绿色', '有多数洼点', '幼时被疏微毛', '老时毛渐脱落', '中脉在叶面陷入', '在叶背凸起', '侧脉两面扁平', '纤细', '密生而平行', '每边达120条', '直达叶缘', '叶柄扁平', '基部稍宽', '长5-8毫米', '幼时被微毛', '老时毛脱落', '叶柄内具腺体', '叶、树皮、根、花、种子均含有多种配醣体', '毒性极强', '人、畜误食能致死'], '8.txt': ['37.1935.落叶乔木', '高约10米', '最高可达30米', '胸径达70厘米以上', '树皮暗灰棕色', '叶片纸质', '在长枝上互生', '在短枝上簇生', '圆形或近圆形', '直径9-25厘米', '稀达35厘米', '掌状5-7浅裂', '裂片阔三角状卵形至长圆状卵形', '长不及全叶片的1/2', '茁壮枝上的叶片分裂较深', '裂片长超过全叶片的1/2', '先端渐尖', '基部心形', '上面深绿色', '无毛或几无毛', '下面淡绿色', '幼时疏生短柔毛', '边缘有细锯齿', '放射状主脉5-7条', '两面均明显', '叶柄细长', '长8-50厘米', '无毛', '树皮及叶含鞣酸', '可提制栲胶', '种子可榨油', '供工业用', '叶形多变化', '有时浅裂', '裂片阔三角状卵形', '有时分裂较深', '裂片长圆状卵形', '稀倒卵状长圆形', '长不及全叶片的1/2', '茁壮枝上的叶片', '分裂更深', '往往超过全叶片长的1/2'], '25.txt': ['小叶16-20', '对生或互生', '纸质', '卵状披针形或卵状长椭圆形', '长9-15厘米', '宽2.5-4厘米', '先端尾尖', '基部一侧圆形', '另一侧楔形', '不对称', '边全缘或有疏离的小锯齿', '两面均无毛', '无斑点', '背面常呈粉绿色', '侧脉每边18-24条', '平展', '与中脉几成直角开出', '背面略凸起', '圆锥花序与叶等长或更长', '被稀疏的锈色短柔毛或有时近无毛', '小聚伞花序生于短的小枝上', '多花'], '15.txt': ['枝、叶及木材均有樟脑气味', '叶互生', '卵状椭圆形', '长6-12厘米', '宽2.5-5.5厘米', '先端急尖', '基部宽楔形至近圆形', '边缘全缘', '软骨质', '有时呈微波状', '上面绿色或黄绿色', '有光泽', '下面黄绿色或灰绿色', '晦暗', '两面无毛或下面幼时略被微柔毛', '具离基三出脉', '有时过渡到基部具不显的5脉', '中脉两面明显', '上部每边有侧脉1-3-5(7)条.基生侧脉向叶缘一侧有少数支脉', '侧脉及支脉脉腋上面明显隆起下面有明显腺窝', '窝内常被柔毛', '叶柄纤细', '长2-3厘米', '腹凹背凸', '无毛', '木材及根、枝、叶可提取樟脑和樟油', '樟脑和樟油供医药及香料工业用', '从其樟油化学成分看', '可分三个类型', '即本樟（含樟脑为主）', '芳樟（含芳樟醇为主）和油樟（含松油醇为主）', '各个类型的经济价值不尽相同', '为结合生产应进行细分', '可依据樟树形态上的微细差异再结合枝、叶和木材的气味加以鉴别', '本樟树皮桃红', '裂片较大', '树身较矮.枝桠敞开而茂密', '占空间面积较大', '叶柄发红', '叶身较薄', '叶两面黄绿色', '出叶较迟', '枝、叶或木材嗅之有强烈的樟脑气味', '木髓带红', '将木片放入口中咀嚼后有苦涩味感觉', '这可证明有大量樟脑的存在', ')则树皮黄色', '质薄', '裂片少而浅', '树身较高', '枝桠直上', '分枝较疏', '叶柄绿色', '叶身厚', '叶背面灰白色', '出叶较早', '枝、叶或木材均有清香的芳樟醇气味', '.与樟相近', '但不同在于花及花序被微柔毛', '叶较狭而短', '芽鳞少数而小', '但根据我们对樟树的野外观察结果', '樟树常可根据花及花序无毛至近无毛或被灰白至黄褐色微柔毛分出两个类型', '此两个类型在同一生长地域内同时并存', '此外叶形、叶脉以及芽鳞的情况在同一植株中亦多有变异', '故我们认为', '归入黄樟是错误的', '后者的圆锥花序无毛', '叶下面侧脉脉腋腺窝通常不明显']}\n","dict_keys(['30.txt', '12.txt', '4.txt', '31.txt', '7.txt', '1.txt', '27.txt', '24.txt', '10.txt', '22.txt', '9.txt', '28.txt', '26.txt', '23.txt', '6.txt', '17.txt', '29.txt', '18.txt', '5.txt', '11.txt', '14.txt', '0.txt', '19.txt', '3.txt', '20.txt', '13.txt', '16.txt', '2.txt', '21.txt', '8.txt', '25.txt', '15.txt'])\n","{'30.txt': ['小枝、芽、叶下面', '叶柄、均密被褐色或灰褐色短绒毛（幼树的叶下面无毛）', '叶厚革质', '椭圆形', '长圆状椭圆形或倒卵状椭圆形', '长10-20厘米', '宽4-7（10）厘米', '先端钝或短钝尖', '基部楔形', '叶面深绿色', '有光泽'], '12.txt': ['叶聚生于枝顶', '二年生', '革质', '嫩时上下两面有柔毛', '以后变秃净', '倒卵形或倒卵状披针形', '长4-9厘米', '宽1.5-4厘米', '上面深绿色', '发亮、干后暗晦无光', '先端圆形或钝', '常微凹入或为微心形', '基部窄楔形', '侧脉6-8对', '在靠近边缘处相结合', '有时因侧脉间的支脉较明显而呈多脉状', '网脉稍明显', '网眼细小', '全缘', '干后反卷', '叶柄长达2厘米', '叶革质', '倒卵形', '先端圆', '簇生于枝顶呈假轮生状', '经长期栽培', '雄蕊常表现退化而不育', '结实率亦低'], '4.txt': ['叶纸质', '近圆形或三角状圆形', '长5-10厘米', '宽与长相若或略短于长', '先端急尖', '基部浅至深心形', '两面通常无毛', '嫩叶绿色', '仅叶柄略带紫色', '叶缘膜质透明', '新鲜时明显可见'], '31.txt': ['60.加杨（北方通称）加拿大杨（中国高等植物图鉴）、欧美杨、加拿大白杨、美国大叶白杨（中国树木分类学）图版22', '叶三角形或三角状卵形', '长7-10厘米', '长枝和萌枝叶较大', '长10-20厘米', '一般长大于宽', '先端渐尖', '基部截形或宽楔形', '无或有1-2腺体', '边缘半透明', '有圆锯齿', '近基部较疏', '具短缘毛', '上面暗绿色', '下面淡绿色', '叶柄侧扁而长', '带红色（苗期特明显）'], '7.txt': ['叶薄革质', '倒卵状阔披针形或长圆状倒披针形', '长8-18厘米', '宽(2)3.5-6(10)厘米', '先端渐尖或短尖', '基部楔形', '不下延', '上面无毛或沿中脉有毛', '老时完全无毛', '下面被黄褐色短柔毛', '中脉粗壮', '上面下陷', '侧脉每边6-8（10）条', '弧形', '在边缘网结并渐消失', '横脉及小脉在下面联结成明显的网状'], '1.txt': ['末级小枝具2-4叶', '叶耳不明显', '鞘口繸毛存在而为脱落性', '叶片较小较薄', '披针形', '长4-11厘米', '宽0.5-1.2厘米', '下表面在沿中脉基部具柔毛', '次脉3-6对', '再次脉9条', '花枝穗状', '长5-7厘米', '基部托以4-6片逐渐稍较大的微小鳞片状苞片', '有时花枝下方尚有1-3片近于正常发达的叶', '当此时则花枝呈顶生状', '佛焰苞通常在10片以上', '常偏于一侧', '呈整齐的复瓦状排列', '下部数片不孕而早落', '致使花枝下部露出而类似花枝之柄', '上部的边缘生纤毛及微毛', '无叶耳', '具易落的鞘口繸毛', '缩小叶小', '披针形至锥状', '每片孕性佛焰苞内具1-3枚假小穗', '颖1片', '长15-28毫米', '顶端常具锥状缩小叶有如佛焰苞', '下部、上部以及边缘常生毛茸'], '27.txt': ['叶革质、狭倒卵形、狭椭圆状倒卵形', '或倒披针形', '长8-17厘米', '宽2.5-5.5厘米', '先端短急尖', '通常尖头钝', '基部楔形', '沿叶柄稍下延', '边缘稍内卷', '下面疏生红褐色短毛', '叶柄长1-3厘米', '基部稍膨大', '托叶痕半椭圆形', '长3-4毫米', '本种与海南木莲极相似', '但叶质地比较厚', '干后两面叶脉不明显', '花柄有褐色毛', '心皮有长约1毫米的缘'], '24.txt': ['15.女贞（神农本草经）青蜡树（江苏）', '大叶蜡树（江西）', '白蜡树（广西）', '蜡树（湖南）', '叶片常绿', '革质', '卵形、长卵形或椭圆形至宽椭圆形', '长6-17厘米', '宽3-8厘米', '先端锐尖至渐尖或钝', '基部圆形或近圆形', '有时宽楔形或渐狭', '叶缘平坦', '上面光亮', '两面无毛', '中脉在上面凹入', '下面凸起', '侧脉4-9对', '两面稍凸起或有时不明显', '叶柄长1-3厘米', '上面具沟', '无毛', '花序基部苞片常与叶同型', '小苞片披针形或线形', '长0.5-6厘米', '宽0.2-1.5厘米', '凋落'], '10.txt': ['1.栾树（正字通）木栾（救荒本草）、栾华（植物名实图考）', '五乌拉叶（甘肃）', '乌拉（河北）', '乌拉胶', '黑色叶树（河北）', '石栾树（浙江）', '黑叶树、木栏牙（河南）图版', '小枝具疣点', '与叶轴、叶柄均被皱曲的短柔毛或无毛', '小叶(7-)11-18片(顶生小叶有时与最上部的一对小叶在中部以下合生)', '无柄或具极短的柄', '对生或互生', '纸质', '卵形、阔卵形至卵状披针形', '长(3-)5-10厘米', '宽3-6厘米', '顶端短尖或短渐尖', '基部钝至近截形', '边缘有不规则的钝锯齿', '齿端具小尖头', '有时近基部的齿疏离呈缺刻状', '或羽状深裂达中肋而形成二回羽状复叶', '上面仅中脉上散生皱曲的短柔毛', '下面在脉腋具髯毛', '有时小叶背面被茸毛'], '22.txt': ['叶螺旋状着生', '条状披针形', '微弯', '长7-12厘米', '宽7-10毫米', '先端尖', '基部楔形', '上面深绿色', '有光泽', '中脉显著隆起', '下面带白色、灰绿色或淡绿色', '雌球花单生叶腋', '有梗', '基部有少数苞片'], '9.txt': ['21.天竺桂（开宝本草）大叶天竺桂、竺香（浙江）', '山肉桂、土肉桂（台湾）、土桂、山玉桂（福建）图版52', '叶近对生或在枝条上部者互生', '卵圆状长圆形至长圆状披针形', '长7-10厘米', '宽3-3.5厘米', '先端锐尖至渐尖', '基部宽楔形或钝形', '革质', '上面绿色', '光亮', '下面灰绿色', '晦暗', '两面无毛', '离基三出脉', '中脉直贯叶端', '在叶片上部有少数支脉', '基生侧脉自叶基1-1.5厘米处斜向生出', '向叶缘一侧有少数支脉', '有时自叶基处生出一对稍为明显隆起的附加支脉', '中脉及侧脉两面隆起', '细脉在上面密集而呈明显的网结状但在下面呈细小的网孔', '叶柄粗壮', '腹凹背凸', '红褐色', '无毛', '枝叶及树皮可提取芳香油', '供制各种香精及香料的原料'], '28.txt': ['叶纸质', '基部近于圆形或楔形', '外貌椭圆形或倒卵形', '长6-10厘米', '通常浅3裂', '裂片向前延伸', '稀全缘', '中央裂片三角卵形', '急尖、锐尖或短渐尖', '上面深绿色', '下面黄绿色或淡绿色', '被白粉', '略被毛', '在叶脉上较密', '初生脉3条', '稀基部叶脉也发育良好', '致成5条', '在上面不显著', '在下面显著', '叶柄长2.5-5厘米', '淡紫绿色', '细瘦', '无毛', '花多数常成顶生被短柔毛的伞房花序', '直径约3厘米', '总花梗长1.5-2厘米', '开花在叶长大以后'], '26.txt': ['冬芽圆锥形', '顶端钝', '外被短柔毛', '常2-3个簇生', '中间为叶芽', '两侧为花芽', '叶片长圆披针形、椭圆披针形或倒卵状披针形', '长7-15厘米', '宽2-3.5厘米', '先端渐尖', '基部宽楔形', '上面无毛', '下面在脉腋间具少数短柔毛或无毛', '叶边具细锯齿或粗锯齿', '齿端具腺体或无腺体', '叶柄粗壮', '长1-2厘米', '常具1至数枚腺体', '有时无腺体', '花单生', '先于叶开放', '直径2.5-3.5厘米'], '23.txt': ['叶片卵状椭圆形或倒卵椭圆形', '长5-9厘米', '宽2.5-5厘米', '先端渐尖', '基部圆形', '边有渐尖单锯齿及重锯齿', '齿尖有小腺体', '上面深绿色', '无毛', '下面淡绿色', '无毛', '有侧脉6-8对', '叶柄长1-1.5厘米', '无毛', '先端有1-3圆形腺体'], '6.txt': ['158)1845.落叶小乔木', '叶纸质', '外貌圆形', '直径7-10厘米', '基部心脏形或近于心脏形稀截形', '5-9掌状分裂', '通常7裂', '裂片长圆卵形或披针形', '先端锐尖或长锐尖', '边缘具紧贴的尖锐锯齿', '裂片间的凹缺钝尖或锐尖', '深达叶片的直径的1/2或1/3', '下面淡绿色', '在叶脉的脉腋被有白色丛毛', '叶柄长4-6厘米', '细瘦', '无毛', '花紫色', '杂性', '雄花与两性花同株', '生于无毛的伞房花序', '总花梗长2-3厘米', '叶发出以后才开花'], '17.txt': ['叶片革质', '椭圆形、长椭圆形或椭圆状披针形', '长7-14.5厘米', '宽2.6-4.5厘米', '先端渐尖', '基部渐狭呈楔形或宽楔形', '全缘或通常上半部具细锯齿', '两面无毛', '腺点在两面连成小水泡状突起', '中脉在上面凹入', '下面凸起', '侧脉6-8对', '多达10对', '在上面凹入', '下面凸起', '叶柄长0.8-1.2厘米', '最长可达15厘米', '无毛', '聚伞花序簇生于叶腋', '或近于帚状', '每腋内有花多朵', '.其极大部分花序簇生于叶腋', '而在原始文献中', '', '的描述“花序簇生于叶腋”'], '29.txt': ['19．阔叶十大功劳图版47', '叶狭倒卵形至长圆形', '长27-51厘米', '宽10-20厘米', '具4-10对小叶', '最下一对小叶距叶柄基部0.5-2.5厘米', '上面暗灰绿色', '背面被白霜', '有时淡黄绿色或苍白色', '两面叶脉不显', '叶轴粗2-4毫米', '节间长3-10厘米', '小叶厚革质', '硬直', '自叶下部往上小叶渐次变长而狭', '最下一对小叶卵形', '长1.2-3.5厘米', '宽1-2厘米', '具1-2粗锯齿', '往上小叶近圆形至卵形或长圆形', '长2-10.5厘米', '宽2-6厘米', '基部阔楔形或圆形', '偏斜', '有时心形', '边缘每边具2-6粗锯齿', '先端具硬尖', '顶生小叶较大', '长7-13厘米', '宽3.5-10厘米', '具柄', '长1-6厘米'], '18.txt': ['叶在长枝上辐射伸展', '短枝之叶成簇生状（每年生出新叶约15-20枚）', '针形', '坚硬', '淡绿色或深绿色', '长2.5-5厘米', '宽1-1.5毫米', '上部较宽', '先端锐尖', '下部渐窄', '常成三棱形', '稀背脊明显', '叶之腹面两侧各有2-3条气孔线', '背面4-6条', '幼时气孔线有白粉'], '5.txt': ['小叶4-6对', '对生', '倒卵状长圆形或倒卵形', '长1.5-3厘米'], '11.txt': ['叶在长枝上互生', '在短枝上为1-4片簇生', '叶片纸质至坚纸质', '卵形、卵状椭圆形', '稀长圆状椭圆形', '长4-13(-15)厘米', '宽（3-）4-6厘米', '先端渐尖至短渐尖', '基部圆形或钝', '边缘具细锯齿', '叶面深绿色', '背面浅绿色', '两面无毛', '或叶面幼时疏被短的微柔毛', '主脉在叶面平或下陷', '疏被细小微柔毛或无毛', '背面隆起', '无毛或有时疏被细小微柔毛', '侧脉8-10对', '在叶面平坦或稍凸起', '在背面凸起', '于叶缘附近网结', '网状脉两面明显', '叶柄长1-1.2厘米', '上面具狭沟', '疏被细小微柔毛', '托叶胼胝质', '很小', '不明显', '单花或2-5花的聚伞花序', '单生或簇生于当年生或二年生枝的叶腋内', '或生于短枝的鳞片腋内或叶腋内', '总花梗长2-3毫米', '花梗长3-7毫米', '均无毛', '单生于叶腋或鳞片腋内', '花梗长6-18毫米', '无毛', '基部具2枚卵状小苞片', '本种的主要特点为小枝和花序无毛', '果柄与叶柄近等长或稍长于叶柄', '区别于它的各变种'], '14.txt': ['大叶蜡梅（广西桂林）图版3', '140.1925.落叶灌木', '高达4米', '鳞芽通常着生于第二年生的枝条叶腋内', '芽鳞片近圆形', '覆瓦状排列', '外面被短柔毛', '叶纸质至近革质', '卵圆形、椭圆形、宽椭圆形至卵状椭圆形', '有时长圆状披针形', '长5-25厘米', '宽2-8厘米', '顶端急尖至渐尖', '有时具尾尖', '基部急尖至圆形', '除叶背脉上被疏微毛外无毛', '花着生于第二年生枝条叶腋内', '先花后叶', '芳香', '直径2-4厘米'], '0.txt': ['单身复叶', '翼叶通常狭窄', '或仅有痕迹', '叶片披针形', '椭圆形或阔卵形', '大小变异较大', '顶端常有凹口', '中脉由基部至凹口附近成叉状分枝', '叶缘至少上半段通常有钝或圆裂齿', '很少全缘', '种子或多或少数', '稀无籽', '通常卵形', '顶部狭尖', '基部浑圆', '子叶深绿、淡绿或间有近于乳白色', '合点紫色', '多胚', '少有单胚'], '19.txt': ['短枝密被叶痕', '黑灰色', '短枝上亦可长出长枝', '叶扇形', '有长柄', '淡绿色', '无毛', '有多数叉状并列细脉', '顶端宽5-8厘米', '在短枝上常具波状缺刻', '在长枝上常2裂', '幼树及萌生枝上的叶常较而深裂(叶片长达13厘米', '有时裂片再分裂(这与较原始的化石种类之叶相似)', '在短枝上3-8叶呈簇生状', '秋季落叶前变为黄色.球花雌雄异株', '生于短枝顶端的鳞片状叶的腋内'], '3.txt': ['1986.落叶灌木', '高1-2米', '叶薄纸质', '近圆形或宽椭圆形', '叶片长2-6厘米', '宽1.5-3厘米', '先端圆钝', '基部楔形', '下延', '上面深绿色', '中脉和侧脉隆起', '背面淡绿色', '中脉和侧脉明显隆起', '两面网脉显著', '无毛', '叶缘平展', '每边具15-40刺齿'], '20.txt': ['972.图3674.1972.落叶灌木或小乔木', '高可达7米', '叶互生或有时对生', '纸质', '椭圆形、阔矩圆形或倒卵形', '长2.5-7厘米', '宽1.5-4厘米', '顶端短尖或钝形', '有时微凹', '基部阔楔形或近圆形', '无毛或下面沿中脉有微柔毛', '侧脉3-7对', '小脉不明显', '树皮、叶及花为强泻剂'], '13.txt': ['叶马褂状', '长4-12（18）厘米', '近基部每边具1侧裂片', '先端具2浅裂', '下面苍白色', '叶柄长4-8（-16）厘米', '叶和树皮入药'], '16.txt': ['图1156.1959.叶倒卵状矩圆形至矩圆形', '很少倒卵形', '长7-13(-16)厘米', '顶端钝或急狭而钝头', '基部宽楔形', '边缘常有较规则的波状浅钝锯齿', '侧脉6-8对', '圆锥花序通常生于具两对叶的幼技顶', '长9-15厘米', '直径8-13厘米'], '2.txt': ['1.七叶树（河北习见树木图说）图版83', '33.1960.落叶乔木', '高达25米', '树皮深褐色或灰褐色', '小枝、圆柱形', '黄褐色或灰褐色', '无毛或嫩时有微柔毛', '有圆形或椭圆形淡黄色的皮孔', '掌状复叶', '由5-7小组成', '叶柄长10-12厘米', '有灰色微柔毛', '小叶纸质', '长圆披针形至长圆倒披针形', '稀长椭圆形钾先端短锐尖', '基部楔形或阔楔形', '边缘有钝尖形的细锯齿', '长8-16厘米', '宽3-5厘米', '上面深绿色', '无毛', '下面除中肋及侧脉的基部嫩时有疏柔毛外', '其余部分无毛', '中央小叶的小叶柄长1-1.8厘米', '两侧的小叶柄长5-10毫米', '有灰色微柔毛'], '21.txt': ['叶3-4枚轮生', '下枝为对生', '窄披针形', '顶端急尖', '基部楔形', '叶缘反卷', '长11-15厘米', '宽2-2.5厘米', '叶面深绿', '无毛', '叶背浅绿色', '有多数洼点', '幼时被疏微毛', '老时毛渐脱落', '中脉在叶面陷入', '在叶背凸起', '侧脉两面扁平', '纤细', '密生而平行', '每边达120条', '直达叶缘', '叶柄扁平', '基部稍宽', '长5-8毫米', '幼时被微毛', '老时毛脱落', '叶柄内具腺体', '叶、树皮、根、花、种子均含有多种配醣体', '毒性极强', '人、畜误食能致死'], '8.txt': ['37.1935.落叶乔木', '高约10米', '最高可达30米', '胸径达70厘米以上', '树皮暗灰棕色', '叶片纸质', '在长枝上互生', '在短枝上簇生', '圆形或近圆形', '直径9-25厘米', '稀达35厘米', '掌状5-7浅裂', '裂片阔三角状卵形至长圆状卵形', '长不及全叶片的1/2', '茁壮枝上的叶片分裂较深', '裂片长超过全叶片的1/2', '先端渐尖', '基部心形', '上面深绿色', '无毛或几无毛', '下面淡绿色', '幼时疏生短柔毛', '边缘有细锯齿', '放射状主脉5-7条', '两面均明显', '叶柄细长', '长8-50厘米', '无毛', '树皮及叶含鞣酸', '可提制栲胶', '种子可榨油', '供工业用', '叶形多变化', '有时浅裂', '裂片阔三角状卵形', '有时分裂较深', '裂片长圆状卵形', '稀倒卵状长圆形', '长不及全叶片的1/2', '茁壮枝上的叶片', '分裂更深', '往往超过全叶片长的1/2'], '25.txt': ['小叶16-20', '对生或互生', '纸质', '卵状披针形或卵状长椭圆形', '长9-15厘米', '宽2.5-4厘米', '先端尾尖', '基部一侧圆形', '另一侧楔形', '不对称', '边全缘或有疏离的小锯齿', '两面均无毛', '无斑点', '背面常呈粉绿色', '侧脉每边18-24条', '平展', '与中脉几成直角开出', '背面略凸起', '圆锥花序与叶等长或更长', '被稀疏的锈色短柔毛或有时近无毛', '小聚伞花序生于短的小枝上', '多花'], '15.txt': ['枝、叶及木材均有樟脑气味', '叶互生', '卵状椭圆形', '长6-12厘米', '宽2.5-5.5厘米', '先端急尖', '基部宽楔形至近圆形', '边缘全缘', '软骨质', '有时呈微波状', '上面绿色或黄绿色', '有光泽', '下面黄绿色或灰绿色', '晦暗', '两面无毛或下面幼时略被微柔毛', '具离基三出脉', '有时过渡到基部具不显的5脉', '中脉两面明显', '上部每边有侧脉1-3-5(7)条.基生侧脉向叶缘一侧有少数支脉', '侧脉及支脉脉腋上面明显隆起下面有明显腺窝', '窝内常被柔毛', '叶柄纤细', '长2-3厘米', '腹凹背凸', '无毛', '木材及根、枝、叶可提取樟脑和樟油', '樟脑和樟油供医药及香料工业用', '从其樟油化学成分看', '可分三个类型', '即本樟（含樟脑为主）', '芳樟（含芳樟醇为主）和油樟（含松油醇为主）', '各个类型的经济价值不尽相同', '为结合生产应进行细分', '可依据樟树形态上的微细差异再结合枝、叶和木材的气味加以鉴别', '本樟树皮桃红', '裂片较大', '树身较矮.枝桠敞开而茂密', '占空间面积较大', '叶柄发红', '叶身较薄', '叶两面黄绿色', '出叶较迟', '枝、叶或木材嗅之有强烈的樟脑气味', '木髓带红', '将木片放入口中咀嚼后有苦涩味感觉', '这可证明有大量樟脑的存在', ')则树皮黄色', '质薄', '裂片少而浅', '树身较高', '枝桠直上', '分枝较疏', '叶柄绿色', '叶身厚', '叶背面灰白色', '出叶较早', '枝、叶或木材均有清香的芳樟醇气味', '.与樟相近', '但不同在于花及花序被微柔毛', '叶较狭而短', '芽鳞少数而小', '但根据我们对樟树的野外观察结果', '樟树常可根据花及花序无毛至近无毛或被灰白至黄褐色微柔毛分出两个类型', '此两个类型在同一生长地域内同时并存', '此外叶形、叶脉以及芽鳞的情况在同一植株中亦多有变异', '故我们认为', '归入黄樟是错误的', '后者的圆锥花序无毛', '叶下面侧脉脉腋腺窝通常不明显']}\n","dict_keys(['30.txt', '12.txt', '4.txt', '31.txt', '7.txt', '1.txt', '27.txt', '24.txt', '10.txt', '22.txt', '9.txt', '28.txt', '26.txt', '23.txt', '6.txt', '17.txt', '29.txt', '18.txt', '5.txt', '11.txt', '14.txt', '0.txt', '19.txt', '3.txt', '20.txt', '13.txt', '16.txt', '2.txt', '21.txt', '8.txt', '25.txt', '15.txt'])\n","(51200, 150) (51200, 32)\n","(6400, 150) (6400, 32)\n","Epoch 1/40\n","1600/1600 [==============================] - 144s 86ms/step - loss: 2.8653 - accuracy: 0.1991 - val_loss: 1.9680 - val_accuracy: 0.4317\n","Epoch 2/40\n","1600/1600 [==============================] - 137s 86ms/step - loss: 1.3751 - accuracy: 0.6214 - val_loss: 0.8167 - val_accuracy: 0.8067\n","Epoch 3/40\n","1600/1600 [==============================] - 137s 86ms/step - loss: 0.5046 - accuracy: 0.8971 - val_loss: 0.2959 - val_accuracy: 0.9506\n","Epoch 4/40\n","1600/1600 [==============================] - 137s 86ms/step - loss: 0.1892 - accuracy: 0.9696 - val_loss: 0.1397 - val_accuracy: 0.9725\n","Epoch 5/40\n","1600/1600 [==============================] - 137s 86ms/step - loss: 0.0907 - accuracy: 0.9853 - val_loss: 0.0563 - val_accuracy: 0.9931\n","Epoch 6/40\n","1600/1600 [==============================] - 137s 86ms/step - loss: 0.0529 - accuracy: 0.9912 - val_loss: 0.0342 - val_accuracy: 0.9944\n","Epoch 7/40\n","1600/1600 [==============================] - 137s 86ms/step - loss: 0.0336 - accuracy: 0.9943 - val_loss: 0.0366 - val_accuracy: 0.9914\n","Epoch 8/40\n","1600/1600 [==============================] - 138s 86ms/step - loss: 0.0243 - accuracy: 0.9956 - val_loss: 0.0201 - val_accuracy: 0.9961\n","Epoch 9/40\n","1600/1600 [==============================] - 137s 86ms/step - loss: 0.0217 - accuracy: 0.9953 - val_loss: 0.0115 - val_accuracy: 0.9983\n","Epoch 10/40\n","1600/1600 [==============================] - 138s 86ms/step - loss: 0.0145 - accuracy: 0.9972 - val_loss: 0.0178 - val_accuracy: 0.9961\n","Epoch 11/40\n","1600/1600 [==============================] - 138s 86ms/step - loss: 0.0139 - accuracy: 0.9972 - val_loss: 0.0109 - val_accuracy: 0.9983\n","Epoch 12/40\n","1600/1600 [==============================] - 138s 86ms/step - loss: 0.0107 - accuracy: 0.9979 - val_loss: 0.0074 - val_accuracy: 0.9989\n","Epoch 13/40\n","1600/1600 [==============================] - 137s 86ms/step - loss: 0.0093 - accuracy: 0.9981 - val_loss: 0.0071 - val_accuracy: 0.9989\n","Epoch 14/40\n","1600/1600 [==============================] - 137s 86ms/step - loss: 0.0083 - accuracy: 0.9981 - val_loss: 0.0206 - val_accuracy: 0.9937\n","Epoch 15/40\n","1600/1600 [==============================] - 137s 86ms/step - loss: 0.0086 - accuracy: 0.9980 - val_loss: 0.0032 - val_accuracy: 0.9994\n","Epoch 16/40\n","1600/1600 [==============================] - 137s 86ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 0.0069 - val_accuracy: 0.9978\n","Epoch 17/40\n","1600/1600 [==============================] - 137s 86ms/step - loss: 0.0071 - accuracy: 0.9983 - val_loss: 0.0361 - val_accuracy: 0.9919\n","Epoch 18/40\n","1600/1600 [==============================] - 137s 86ms/step - loss: 0.0083 - accuracy: 0.9981 - val_loss: 0.0062 - val_accuracy: 0.9981\n"],"name":"stdout"},{"output_type":"stream","text":["findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 35757 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 32451 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 21644 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 39564 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 35777 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 25439 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 22833 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans.\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 36845 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 20195 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 27425 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 25968 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 36845 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 20195 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 27425 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 25968 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 25439 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 22833 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 35757 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 32451 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 21644 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 39564 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 35777 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYgAAAEdCAYAAAAW6PDWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3RcZb3/8fc3F+glLRRoS21JCwjlXqABmgCnLFqBqogIusTKAY8QxdXfQQU9SEEqUDkicvh5A7pEC23kfpObAv7oQaQg4SaWS7WUhBYovVGaNr0l398fzwyZTPZMkjKzZzL5vNaaNbP3fmbPl810Pnn2fvbe5u6IiIikKyt0ASIiUpwUECIiEkkBISIikRQQIiISSQEhIiKRFBAiIhJJASEiIpEqCl2ASJzMbDJwI7ApYvFm4G/AZKA9bVkFMA+4DlgEtES8fxAwDZgEzAS2RLRZ6u6nmtm9wJ4Ry3cAZgPPAI8AGyPaVAEHAt8GzgS2pS0vA/7X3f9PxHtFekwBIf3NQOA+d78ofYGZNQLDgS+5++tpy6YCUwEDVrn7pIj3zwcqgSHAz939hrTlFcBTiclRwCR335TW5pzE+yuBF9z9yxGf81SijmHAhe7+x7TlnwSuzLgFRHpIu5hERCSSAkJERCIpIEREJJICQkREIikgREQkkgJCREQiKSBERCSSAkJERCIpIEREJJICQkREIulSG9LfrAOmJi6rkW4VsASYb2ZR751LuEZTRYb3A7QC7wMXJy6bke6VxPNrwFMZPufHifV8MsvntAPLgCvNLOqyGn/K8D6RHjN3j+eDwnVqpgCDgfeAq939Nxnafgf4L8LFz+4CznP3zbEUKiIiQLwBcSDwL3ffbGb7AQuAz7j782ntTgRuAY4H3gHuBZ6JuriaiIjkT2y7mNx9Uepk4rE38Hxa07OAm5LtzewKoAHIGhC77babjxs3Lmf1ioj0B88///wqdx8etSzWYxBm9mvgbMIll18EHo5odiBwf8r0y8BIM9vV3Venra8eqAeorq6msTHT7loREYliZk2ZlsU6isndv0W41v2xwD2EG7SkqyIcSExKvh4Ssb457l7j7jXDh0cGoIiIbKfYh7m6e5u7PwWMAc6LaNICDE2ZTr5en+/aRESkQyHPg6ggHINItwiYkDI9AViRvntJRETyK5aAMLMRZvZlM6sys/LESKUzgD9HNL8F+LqZHWBmOwOXEMafi4hIjOI6SO2E3Uk3EEKpCfi2u//BzKqBV4ED3L3Z3f9oZlcDTxAOZt8NXBZTnSLSR7W3t7Ns2TI2bNhQ6FKKSmVlJSNGjGDo0KHdN04TS0C4+0pgcoZlzYQD06nzrgWuzXddDQ0wcyY0N0N1NcyeDdOn5/tTRSQfVq1ahZkxfvx4ysp0FSEAd6e1tZXly5cD9Dok+u1WbGiA+npoagL38FxfH+aLSN/zwQcfMHLkSIVDCjNj0KBBjB49mvfff7/X7++3W3LmTNi4sfO8jRvDfBHpe9ra2qisrCx0GUVp4MCBbN26tdfv67cB0dzcu/kiUvwyXPyw39ve7dJvA6K6unfzRUT6m34bELNnw6BBnecNGhTmi4hIP74fRHK0kkYxiUi+3X///fz0pz/tMv+EE07g0Ucf7TJ/1KhR3HnnnZxyyimsXt31HOG77rqLG264gccff7zLspkzZzJt2rSc1N1vAwJCGCgQRPq3OIa7v/vuu8yaNYupU6d+NK+lpYVzzjmH4447jiuv7HzPp9NPPx0I5zA89dRTnZZdeOGFbNq0iddff50FCxZQUdHxM/7ggw+yYsWKnNXdrwNCRPq35HD35IjG5HB30B+P0I+PQYiIaLh7dgoIEem3NNw9OwWEiPRbGu6enQJCRPotDXfPTgEhIv3W9OkwZw6MHQtm4XnOHB2gTtIoJhHp1zTcPTP1IEREJJJ6ECIiMbjgggsYNmzYR9NtbW2MHj2aefPmdTkZLnn29CuvvMJxxx3XadmSJUuYMWMGAFOmTOl0Ib7Vq1dzwQUX5Kxmc/ecrayQampqvLGxsdBliEiBvPbaa+y///6FLqNoZdo+Zva8u9dEvUe7mEREJJICQkREIikgREQkkgJCREQiKSBERCSSAkJERCIpIEREJFIsJ8qZ2Y7Ar4GpwC7AEuAH7v5IRNuzgZuA1pTZn3X3BfmvVEQk93TL0e4/521gMtAMfBq4w8wOdve3ItovdPdjYqpNRCSvdMvRLNx9AzArZdaDZrYUmAi8FUcNIiLSOwW5FpOZjQT2BRZlaHKYma0C1gDzgKvcfVvEeuqBeoBq3eFDRBK+/W146aX8fsahh8J11+X3Mwot9oPUZlYJNAA3u/vrEU2eBA4CRgCnAWcA34tal7vPcfcad68ZPnx4vkoWEemXYu1BmFkZoUewBZgR1cbd30yZfMXMLicExFX5r1BESkGp/2Ufl9gCwsI1aW8CRgKfdvetPXyrA9ZtKxERyak4dzFdD+wPnOzurZkamdm0xDEKzGw/4FLg/nhKFBGRpFgCwszGAt8ADgXeM7OWxGO6mVUnXiePMk8B/m5mG4CHgXuAH8dRp4iIdIhrmGsT2XcTVaW0vRC4MO9FiYhIVrrlqIhIDHTL0QLSLUdF+jfdcjQ73XJURERyRgEhIiKRFBAiUjJKZZd5rrW3t2/X+xQQIlISBgwYwOrVqxUSKdydLVu2sHz5cgYPHtzr92sUk4iUhDFjxrBs2TJWrlxZ6FKKSkVFBTvttBO77bZb79+bh3pERGJXWVnJnnvuWegySop2MYmISCQFhIiIRFJAiIhIJAWEiIhEUkCIiEgkBYSIiERSQIiISCQFhIiIRFJAiIhIJAWEiIhEUkCIiEgkBYSIiERSQIiISCQFhIiIRFJAiIhIJAWEiIhEUkCIiEikWALCzHY0s5vMrMnM1pvZS2Y2LUv775jZe2b2oZn91sx2jKNOERHpEFcPogJ4G5gM7ARcAtxhZuPSG5rZicBFwBRgLLAX8KOY6hQRkYRYAsLdN7j7LHd/y93b3f1BYCkwMaL5WcBN7r7I3dcCVwBnx1GniIh0KMgxCDMbCewLLIpYfCDwcsr0y8BIM9s1Yj31ZtZoZo0rV67MT7EiIv1U7AFhZpVAA3Czu78e0aQKWJcynXw9JL2hu89x9xp3rxk+fPh21fPkk/Dd74L7dr1dRKRkxRoQZlYGzAO2ADMyNGsBhqZMJ1+vz0dN//gH/M//QFNTPtYuItJ3xRYQZmbATcBI4DR335qh6SJgQsr0BGCFu6/OR111deF54cJ8rF1EpO+KswdxPbA/cLK7t2ZpdwvwdTM7wMx2Jox4mpuvog46CKqq4Omn8/UJIiJ9U1znQYwFvgEcCrxnZi2Jx3Qzq068rgZw9z8CVwNPAM1AE3BZvmqrqIAjj1RAiIikq4jjQ9y9CbAsTarS2l8LXJvXolLU1cFVV8GGDTB4cFyfKiJS3HSpDUJAtLXBc88VuhIRkeKhgAAmTQrP2s0kItJBAQEMGwb776+RTCIiqRQQCbW1oQehE+ZERAIFREJdHaxZA4sXF7oSEZHioIBI0AlzIiKdKSASxo+HnXfWgWoRkSQFREJZWTgOoR6EiEiggEhRVweLFsEHHxS6EhGRwlNApKitDaOYnn220JWIiBSeAiLFkUeGXU3azSQiooDoZMgQOOQQHagWEQEFRBe1tfDMM+HaTCIi/ZkCIk1dHaxfD6++WuhKREQKSwGRprY2PGs3k4j0dwqINHvtBSNGKCBERBQQaczCbiaNZBKR/k4BEaG2Fv75T1i5stCViIgUjgIigi7cJyKigIg0cSJUViogRKR/U0BEGDgQDjtMB6pFpH9TQGRQVwfPPQdbtxa6EhGRwlBAZFBbC62t8PLLha5ERKQwFBAZJA9UazeTiPRXsQWEmc0ws0Yz22xmc7O0O9vM2sysJeVxXFx1Jo0ZA3vsoQPVItJ/VcT4We8AVwInAgO7abvQ3Y/Jf0nZ1daqByEi/VdsPQh3v8fd7wNWx/WZH1ddHTQ3w/Llha5ERCR+xXoM4jAzW2Vmi83sUjOLs6fzEZ0wJyL9Wbc/vGZ2O2DdNQP2dfcJOajpSeAgoAk4ELgd2AZcFVFbPVAPUF1dnYOP7mzCBBgwIOxmOv30nK9eRKSo9eQv80+4+7HdNTKzJ3JQD+7+ZsrkK2Z2OfA9IgLC3ecAcwBqamo8F5+faocd4IgjdBxCRPqnnuxi6ukPb85/oFPW210PJm/q6uCFF2DTpkJVICJSGHEOc60wswFAOVBuZgOiji2Y2TQzG5l4vR9wKXB/XHWmq60NZ1M//3yhKhARKYw4D1JfArQCFwFfTby+xMyqE+c6JA8iTAH+bmYbgIeBe4Afx1hnJ7rDnIj0Vz05BmGJ4wDd2TPbQnefBczKsLgqpd2FwIU9+LxYjBgBe++dfSRTQwPMnBmGxFZXw+zZMH16fDWKiORDTwLiUeCAHrQr2cGgdXXw6KPgHu44l6qhAerrYePGMN3UFKZBISEifVtPAuJTwElkP1BswEM5qagI1dXBvHmwdGm4Z3WqmTM7wiFp48YwXwEhIn1Zj3YxufvGbhuZteegnqKUPA6xcGHXgGhujn5PpvkiIn1FXxjmWnAHHQRVVdEHqjOdn5eH8/ZERGJVrJfaKCrl5TBpUnRAzJ4NgwZ1njdoUJgvItKX9XQU0/HdtQGG5aCeolVbG370W1pCbyIpeZxBo5hEpNT0JCB+CuzTg3Y3fMxailpdHbS3w9/+BsenxeX06QoEESk93QaEu/8hjkKK3VFHheeFC7sGhIhIKdIxiB4aNgwOOEBnVItI/6GA6IW6utCDaC/ZAb0iIh0UEL1QWwtr18LixYWuREQk/xQQvZC8w5x2M4lIf6CA6IV994VddlFAiEj/oIDohbKycMKc7lEtIv2BAqKX6urg1VfDsQgRkVKmgOil5HGIZ54pbB0iIvmmgOilI44Iu5q0m0lESp0CopeqqmDCBB2oFpHSp4DYDrW18Oyz0NZW6EpERPJHAbEd6urCVV3/8Y9CVyIikj8KiO2gE+ZEpD9QQGyHceNg5EgFhIiUNgXEdjDruHCfiEipUkBsp7o6WLIE3n+/0JWIiOSHAmI71daGZ/UiRKRUxRYQZjbDzBrNbLOZze2m7XfM7D0z+9DMfmtmO8ZUZo9NnAiVlToOISKlK84exDvAlcBvszUysxOBi4ApwFhgL+BHea+ulwYMgMMPV0CISOmKLSDc/R53vw9Y3U3Ts4Cb3H2Ru68FrgDOznd926OuDhobYcuWQlciIpJ7xXgM4kDg5ZTpl4GRZrZrekMzq0/stmpcuXJlbAUm1dXBpk3w0kuxf7SISN4VY0BUAetSppOvh6Q3dPc57l7j7jXDhw+PpbhUyQPV2s0kIqWoGAOiBRiaMp18vb4AtWQ1ejRUV2skk4iUpmIMiEXAhJTpCcAKd+/u2EVB1NWpByEipSnOYa4VZjYAKAfKzWyAmVVENL0F+LqZHWBmOwOXAHPjqrO3amth2TJ4++1CVyIikltx9iAuAVoJQ1i/mnh9iZlVm1mLmVUDuPsfgauBJ4BmoAm4LMY6eyV54T7tZhKRUhPnMNdZ7m5pj1nu3uzuVe7enNL2Wncf6e5D3f1r7r45rjp7a8IEGDhQu5lEpPQU4zGIPqWyMtyGVAEhIqVGAZEDdXXw4ovQ2lroSkREckcBkQO1tbBtWzirWkSkVCggckAnzIlIKVJA5MDw4bDPPhrJJCKlRQGRI3V18Je/hGsziYiUAgVEjpx5JqxZAzfdVOhKRERyQwGRI8cfD8ccA1ddBZuL9qwNEZGeU0DkiBlcdhksX65ehIiUBgVEDk2ZAkcfrV6EiJQGBUQOJXsRy5bB735X6GpERD4eBUSOTZ0azov48Y/VixCRvk0BkWNmMGtWuPz33LmFrkZEZPspIPLgU5+CSZNCL2LLlkJXIyKyfRQQeZDsRTQ3qxchIn2XAiJPTjgBjjpKvQgR6bsUEHmSHNHU1AQ331zoakREek8BkUcnnQRHHqlehIj0TQqIPEr2It56C265pdDViIj0jgIiz6ZNC7cknT0btm4tdDUiIj2ngMiz1F7EvHmFrkZEpOcUEDH49KehpgauvFK9CBHpOxQQMUj2IpYuhfnzC12NiEjPKCBi8pnPwMSJ6kWISN8RW0CY2S5mdq+ZbTCzJjP7SoZ2s8xsq5m1pDz2iqvOfEn2It58Exoaots0NMC4cVBWFp4ztRMRiUOcPYhfAVuAkcB04HozOzBD29vdvSrl8WZsVebRZz8Lhx8eehHbtnVe1tAA9fXhxDr38Fxfr5AQkcKJJSDMbDBwGnCpu7e4+1PAH4Az4/j8YmEGP/whLFnS9Yd/5kzYuLHzvI0bw3wRkUKIqwexL7DN3RenzHsZyNSDONnM1pjZIjM7L//lxedzn4NDD+3ai2hujm6fab6ISL7FFRBVwIdp89YBQyLa3gHsDwwHzgV+aGZnRK3UzOrNrNHMGleuXJnLevMmeSziX/+C3/++Y351dXT7TPNFRPItroBoAYamzRsKrE9v6O6vuvs77t7m7k8D/xc4PWql7j7H3WvcvWb48OE5LzpfTjmlay9i9mwYNKhzu0GDwnwRkUKIKyAWAxVmtk/KvAnAoh681wHLS1UFkjwW8c9/wq23hnnTp8OcOTB2bFg+dmyYnj69sLWKSP9l7h7PB5ndRvixPwc4FHgYqHP3RWntTgGeBD4AjgDuBS5296wXza6pqfHGxsZ8lJ4X7e1hRFNrK7z6KpSXF7oiEemPzOx5d6+JWhbnMNdvAQOB94FbgfPcfZGZHWtmLSntvgz8i7D76RbgJ92FQ19UVhZ6EYsXw223FboaEZGuYutB5Ftf60FA6EUcemi4V8SiRepFiEj8iqUHIWnKysKIpjfegNtvL3Q1IiKdKSAK7NRT4aCD4PLLoa2t0NWIiHRQQBRYai/ijjsKXY2ISAcFRBH4whdCL+KKK9SLEJHioYAoAmVlcOml8NprcOedha5GRCRQQBSJ00+HAw5QL0JEiocCokgkz4t49VW4665CVyMiooAoKslexPnnh5AokVNURKSPUkAUkfLycG2mUaPgi1+EE08Mo5tERApBAVFkDjkEnnsOfvEL+Nvf4OCD4eKLYcOGQlcmIv2NAqIIVVTAjBmh9/CVr8BVV4VdT/feq91OIhIfBUQRGzkS5s6FJ5+EnXYK50t85jPhZkMiIvmmgOgDjj0WXngBrrsOnnoKDjwwjHhKv4d1UkMDjBsXRkaNG9f1/tciIj2hgOgjKirC6KY33ggHsK+4IgTFH/7QuV1DA9TXQ1NT2B3V1BSmFRIi0lsKiD5m1CiYPx8WLIDBg8PtS08+Gd58MyyfObNrz2LjxjBfRKQ3FBB91OTJ8OKLcM01ISwOOAB+9KPQY4jS3BxreSJSAhQQfVhlJVxwAbz+erhs+KxZYVdUlOrqWEsTkRKggCgBo0eHE+z+/GcYMaLr8kGDYPbs+OsSkb5NAVFCjj8eli6FL38ZzMK8yko46SSYMEHnUIhI7yggSswOO4TexPLlcO21cNRR4QS7gw+G8ePhoovCmdo9CQsNlxXp38xL5M/Kmpoab2xsLHQZRem99+C+++Cee+CJJ2DbNthjj3Dc4rTT4Oijw3WgUiWHy6aOiBo0CObMgenT461fRPLHzJ5395qoZepB9AO77w7f/CY8+iisWBHOzj7sMLjxxjAaatSoEAZ/+hNs2RLek8vhsuqJiPRN6kH0Yy0t8PDDoWfx0ENheuedw3kV8+ZFv8cM2tt7/hnqiYgUN/UgJFJVFXzpS3DbbbByZTgr+/OfhwcfzPyekSPh7bdh06aefUYx9kTUoxHpIXcvicfEiRNdcmPLFveLLnKvqHAPh7OjH0OHuu+9t3ttrfspp7ife677xRe7X3ed++9/7/7449nfv3mze2ur+4YN7h9+6P7BB+5r1rivXOm+YoX7u++6L1vm3twc1jlgQOf3DxrkPn9+7/7b5s8P7/u460mua+xYd7PwXKh1lPJ6SlV7e/iet7YWuhJ3oNEz/K7G9gMO7ALcC2wAmoCvZGhnwE+A1YnHT0jsCsv2UEDk3vz57tXV4VvyiU+4X3aZ+333uc+Z437lle7nn+9+xhnuU6a4H3KI++67u5eXZw+FXD/Ky93328990iT3adNCPeed5/6DH7hffXWo9Y473B97zP2558J/R9R6xo7t/bb5uEGTq7Aq1fUk15X8DhZDYPVkPe3t7qtWub/4ovsDD7j/+tfh+/jVr7pPnhz+qKqs7Ng2ZWXue+7pfuqp7v/5n+7XXBO+swsXui9f7t7Wlt//rmwBEdsxCDO7lbBL6+vAocBDQJ27L0pr9w3gu8AUwIHHgJ+7+w3Z1q9jEMWhvR3Wrg27rN5/H+6+G66/HrZu7WhTWQmf+1w4UF5eHnb1lJVlf33OOZk/84tfhA8+CI+1azteb9vWu9rHj4eBA2HAgO6ff/nL8BnpdtoJzjsPNm8OB/xTH+nzFi4M89LtuCMcc0x43mGH8Jz6SJ939dXhvzvdbrvBb34Ttl9FRXhOfZ3+fNJJ8M47XdczejQ8/XTnecnzbNJfA0yaFIZZpxsxImy3Dz+E9evDI/k6at7KlbBuXdfPra6GvfaCYcM6Hrvs0nk6df5DD4VBGr09DuYObW3he7RtWxg+fv750Nra0WaHHWDatPBZb78dHsuWdd2tWl4etuMee4R1NjZ2/n6WlYXBIuvWhWOBqSoqYMyY8N+9xx4dz0uWwK9+1Xl37/Yc38t2DCKWgDCzwcBa4CB3X5yYNw9Y7u4XpbV9Gpjr7nMS018HznX3Sdk+QwFRvBoawjGH5ubw5Z49u/cHqMeNi77O1Nix8NZbXee7h3+kqYGxdi187WuwenXX9oMHh3tttLaGf3Cpz1Hzstlhh86P5I96+vSTT2ZeR11dR6Bs3tz5kZzX2wAsRgMHwpAh4TF0aOfnBx7o+mOZfM/hh4f/n2vXwpo10UHbnfJyGD48bMfUIEi+7s1gjOSP/5gx4Tn9MXJkx1DybN/lpUtDSDQ3h7Bpbu78OhlA2f7fZ/o3kUkxBMRhwF/dfVDKvAuBye5+clrbdcAJ7v5sYroGeMLdh0Sstx6oB6iurp7YlOlKddLn5Wo0VC7W4x7+kUddALG6OvMFE9P1NvTStbWFsBg/PvxwpBs1Kvz1nPqjl/qcPu+882DVqq7r2XXX0EtJSv3JSP/5cA8nY65Z03U9u+8Ojz3WEQBDhmS+dhiEv6qjfp6iRtK1tnYERmpwrF0L3/lO5s8499xQQ2pvKtPr738/83p68zPam/+uKG1t4dymMWOil/d2pGG2gIjr+MOxwHtp884FFkS0bQP2S5neh7CrKetxCB2DKH3FdABVxyDyv56xY3NzvEjryY5CH6QGDgM2ps27AHggou064MiU6YnA+u4+QwEhcdMopvyup9gCq1TXUwwBMRjYAuyTMu8W4L8j2j5NOOaQnP4P4JnuPkMBIVJ6iimwSnU92QIizlFMtxF2FZ1DGMX0MNGjmL4JnA9MpWMU0y9co5hERHKuWM6k/hYwEHgfuBU4z90XmdmxZpY6VuFG4AHgFeAfhOGwN8ZYp4iIAFnGEOSWu68BPh8x/y9AVcq0A99PPEREpEB0LSYREYmkgBARkUgKCBERiVQy94Mws5WEiwAWu92AiPNVi5pqzr++Vi+o5rjku+ax7j48akHJBERfYWaNmYaUFSvVnH99rV5QzXEpZM3axSQiIpEUECIiEkkBEb85hS5gO6jm/Otr9YJqjkvBatYxCBERiaQehIiIRFJAiIhIJAWEiIhEUkDkmJntaGY3mVmTma03s5fMbFqGtmebWZuZtaQ8jou5ZMxsgZltSqnhjQztzMx+YmarE4+fmKXfsj6WelvSHm1m9osMbQu2jc1shpk1mtlmM5ubtmyKmb1uZhvN7AkzG5tlPeMSbTYm3jM1znrNbJKZPWZma8xspZndaWajsqynR9+nPNc8zsw87f/7pVnWE8s27qbm6Wn1bkz8N0zMsJ68b2cFRO5VAG8Dk4GdgEuAO8xsXIb2C929KuWxIJYqu5qRUsP4DG3qCVfknQAcApwMfCOuApNStxewO9AK3JnlLYXaxu8AVwK/TZ1pZrsB9wCXArsAjcDtWdZzK/AisCswE7jLzCLPfM1HvcAwwkiaccBYYD3wu27W1ZPvUy5kqjlp55Q6rsiynri2MWSo2d0b0r7b3wLeBF7Isq68bmcFRI65+wZ3n+Xub7l7u7s/CCwl3Dq1rzsL+Jm7L3P35cDPgLMLWxKnEe4x8pcC19GFu9/j7vcBq9MWfQFY5O53uvsmYBYwwcz2S1+Hme0LHA5c5u6t7n434V4pp8VVr7s/kqj1Q3ffCPwSODrXn789smzjHotzG0Ovaj4LuMULONRUAZFnZjYS2BdYlKHJYWa2yswWm9mlZhbbPTrSXJWo469ZdsEcCLycMv1yYl4h9eQfUbFs46RO29HdNwBLiN6WBwJvuvv6lHmF3u7/Rubvc1JPvk9xaDKzZWb2u0TPLUrRbePELsd/I9yaOZu8bmcFRB6ZWSXQANzs7q9HNHkSOAgYQfhr5Qzge/FV+JH/AvYCRhN2JTxgZntHtKsC1qVMrwOqCnEcAj76RzQZuDlLs2LZxqnStyOJ6SEfs23emdkhwA/Jvg17+n3Kp1XAEYRdYhMJ26shQ9ui2sYJ/w78xd2XZmmT9+2sgMgTMysD5gFbgBlRbdz9TXdfmtgV9QpwOXB6jGUm63jW3de7+2Z3vxn4K/DpiKYtwNCU6aFASwG7wGcCT2X7R1Qs2zhN+nYkMb3+Y7bNKzP7JPAIcH7iTpCRevF9yht3b3H3Rnff5u4rCP8GTzCzqB/9otnGKf6d7H/4xLKdFRB5kPiL+iZgJHCau2/t4VsdKMhf42ky1bGIcIA6aQLd72rIp27/EUUohm3caTua2WBgb6K35SJgr7Qftti3e6K39jhwhbvP6+Xbi2GbJ/+IifrNK4ptnGRmRwOfAO7q5Vtzvp0VEPlxPbA/cLK7t2ZqZGbTEscoSBygvBS4P54SP6phZzM70cwGmFmFmU0n7Pv8Y3E6itUAAAI+SURBVETzW4DvmtloM/sEcAEwN8ZyP2JmdYSudbbRSwXdxontOQAoB8qT2xi4FzjIzE5LLP8h8Peo3ZDuvhh4Cbgs8f5TCSPI7o6rXjMbDfw/4JfufkM36+jN9ymfNR9lZuPNrMzMdgV+Dixw9/RdSbFu42w1pzQ5C7g77ZhI+jri2c7urkcOH4R9ng5sInRdk4/pQHXidXWi7TXACmADYTjb5UBlzPUOB54jdKc/AJ4BPpVYdixhF1KyrQFXA2sSj6tJXM+rANv5RmBexPyi2caE0Ume9piVWDYVeJ0wRHcBMC7lfTcAN6RMj0u0aQXeAKbGWS9wWeJ16vc59XtxMfBId9+nmGs+gzB6cAPwLuGPm90LvY178L0YkNhuUyLeF/t21sX6REQkknYxiYhIJAWEiIhEUkCIiEgkBYSIiERSQIiISCQFhIiIRFJAiIhIpEJf1VKkTzGzU4i+UN2jwAkR89919y+a2f2Eew2kOx34JuHEuXSz3f2R7S5W5GNSQIj0zijCWa+PJ2eYWRXwG8KlHC5JbWxmyevpbHX3Y9KWXUM4c3Y/4Dh335ay7LOEa3mJFIx2MYmISCQFhIiIRFJAiIhIJAWEiIhEUkCIiEgkBYSIiERSQIiISCQFhIiIRNKJciK99zMzW5syXQ4sB840s2PS2ibPnj7YzBakLdsb+GXi9Z/NLPX2jrsCP8tRvSLbRbccFRGRSNrFJCIikRQQIiISSQEhIiKRFBAiIhJJASEiIpEUECIiEun/A5djdcdt5xNZAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 20934 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 30830 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 29575 missing from current font.\n","  font.set_text(s, 0.0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 20934 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 30830 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n","/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 29575 missing from current font.\n","  font.set_text(s, 0, flags=flags)\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_4_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: model_text/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: model_text/assets\n"],"name":"stderr"},{"output_type":"stream","text":["200/200 [==============================] - 6s 26ms/step - loss: 0.0062 - accuracy: 0.9981\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.006159215699881315, 0.9981250166893005]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N8vWoLCujXi6","executionInfo":{"elapsed":9080,"status":"ok","timestamp":1626506251028,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"b9042820-8beb-41ef-e1d7-f7e6b6bcde69"},"source":["print(test_d[:2])\n","tokenizer.sequences_to_texts(test_d[0:2])\n","\n","def create_wbData1(liang=3):\n","    global wb_d\n","    wb_data = []\n","    wb_label = []\n","    wb_d = {}\n","    for wbf in os.listdir(pathofflaviatxt):\n","        wb_d[wbf] = gettxtdata_wb(wbf)\n","    # print(wb_d)\n","    # print(wb_d.keys())\n","    for _ in range(liang):\n","        for x in wb_d.keys():\n","            desc_txt = wb_d[x]\n","            feature1 = []\n","            a = getrandstr(desc_txt, 8)\n","            # print(a)\n","            feature1.append([y[0] for y in tokenizer.texts_to_sequences(\n","                [f\"<start>\"] + jieba.lcut(a, HMM=False) + [f\"<end>\"])])\n","            feature1 = tf.keras.preprocessing.sequence.pad_sequences(feature1, padding='post', maxlen=150)[0]\n","            wb_data.extend([feature1])\n","\n","            # wb_data.extend([getrandstr(desc_txt, 8)])\n","            wb_label.extend([int(x.split(\".\")[0])])\n","            # break\n","    return np.array(wb_data), np.array(wb_label)\n","\n","\n","test_d1, test_l1 = create_wbData1(200)\n","test_l1 = tf.one_hot(test_l1, 32)\n","model_text.evaluate(test_d1, test_l1)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[4275   46   40  135 1377   13  324  238  104   13  748  112 1204   67\n","   707   13   46   40  135 1377   13  226  735   13  177  144   83   56\n","    18  160   13  204    1   13  270   18 1196  230  395   44    4  448\n","     7   13   97 1422 1784   13  195   97 1199 4276    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0]\n"," [4275 1210  162 2019 2020   13 1210  162 2019 2020   13   46  553   13\n","  3208  395   44  286   44    4  448    7   35    4  448   15   13  229\n","   268   18    7    9   15   67   13  960   13 3208  395   44  286   44\n","     4  448    7   35    4  448   15   13   54   43   13  211   54   43\n","    44  329  240  293   13   11    9   19 1509  838 4276    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","     0    0    0    0    0    0    0    0    0    0]]\n","200/200 [==============================] - 5s 26ms/step - loss: 0.0243 - accuracy: 0.9941\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.024268990382552147, 0.9940624833106995]"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w0TIHKTMevEQ","executionInfo":{"elapsed":1449032,"status":"ok","timestamp":1626525931622,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"6774fce0-13d0-4ebc-d9f7-97f48027977b"},"source":["# train_d, train_l = create_wbData(300)\n","# train_l = tf.one_hot(train_l, 32)\n","# test_d, test_l = create_wbData(200)\n","# test_l = tf.one_hot(test_l, 32)\n","\n","\n","early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)\n","\n","def decay(epoch):\n","    return 2e-7\n","\n","history = model_text.fit(train_d,\n","                             train_l,\n","                             epochs=10,\n","                             batch_size=32,\n","                             validation_data=(test_d, test_l),\n","                             callbacks=[tf.keras.callbacks.LearningRateScheduler(decay), early_stop],\n","                             verbose=1)\n","model_text.save(\"model_text\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/10\n","3200/3200 [==============================] - 291s 89ms/step - loss: 0.0501 - accuracy: 0.9863 - val_loss: 0.0097 - val_accuracy: 0.9995\n","Epoch 2/10\n","3200/3200 [==============================] - 283s 89ms/step - loss: 0.0076 - accuracy: 0.9993 - val_loss: 0.0050 - val_accuracy: 0.9995\n","Epoch 3/10\n","3200/3200 [==============================] - 283s 88ms/step - loss: 0.0047 - accuracy: 0.9995 - val_loss: 0.0031 - val_accuracy: 0.9998\n","Epoch 4/10\n","3200/3200 [==============================] - 282s 88ms/step - loss: 0.0034 - accuracy: 0.9996 - val_loss: 0.0023 - val_accuracy: 0.9998\n","Epoch 5/10\n","3200/3200 [==============================] - 283s 88ms/step - loss: 0.0028 - accuracy: 0.9996 - val_loss: 0.0019 - val_accuracy: 0.9998\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:absl:Found untraced functions such as lstm_cell_7_layer_call_fn, lstm_cell_7_layer_call_and_return_conditional_losses, lstm_cell_8_layer_call_fn, lstm_cell_8_layer_call_and_return_conditional_losses, lstm_cell_10_layer_call_fn while saving (showing 5 of 20). These functions will not be directly callable after loading.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: model_text/assets\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Assets written to: model_text/assets\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"28_XlGbVoH2o"},"source":["!rm /content/drive/MyDrive/data/model_text_p9920_07171830 -fr\n","!cp /content/model_text /content/drive/MyDrive/data/model_text_p9920_07171830 -fr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SqY-P9lZ_HrZ","executionInfo":{"elapsed":7457,"status":"ok","timestamp":1627088733430,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"c24dd122-47bf-4a1a-92db-fcd7b84267a0"},"source":["# encoding : utf-8 \n","# @title image 训练的各种头库,拆分1,\n","\n","# 首先要加载各种使用的库\n","!pip install thulac\n","!pip install annoy\n","!pip install mxnet\n","\n","import thulac\n","import tensorflow as tf\n","# You ll generate plots of attention in order to see which parts of an image\n","# our model focuses on during captioning\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","# Scikit-learn includes many helpful utilities\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import shuffle\n","import collections\n","import random\n","import re\n","import numpy as np\n","import os\n","import time\n","import json\n","from glob import glob\n","from PIL import Image\n","import pickle\n","import codecs\n","import gc\n","from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\n","from tensorflow.keras.layers import Conv1D, LSTM, Conv2D, MaxPooling2D\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n","from tensorflow.keras.models import Sequential\n","import numpy as np\n","import tensorflow.keras as keras\n","import shutil\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","import jieba\n","import jieba.analyse\n","import jieba.posseg as pseg\n","import json\n","from collections import OrderedDict\n","from gensim.models import KeyedVectors\n","# from annoy import AnnoyIndex\n","from keras.layers import Embedding\n","import gensim\n","from gensim.test.utils import common_texts, get_tmpfile\n","from gensim.models import Word2Vec\n","from gensim.models import KeyedVectors\n","from gensim.test.utils import datapath\n","import time\n","# import mxnet\n","import os\n","# print(\"list mxnet's vector:\", mxnet.contrib.text.embedding.get_pretrained_file_names())\n","# below line can get the pretrained file's url, like\n","# https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/embeddings/fasttext/wiki.ch.zip\n","# if 0:\n","#   mxnet.contrib.text.embedding.FastText(pretrained_file_name=\"wiki.ch.vec\")\n","\n","# @title 复制腾讯词嵌入，wiki.zh.vec。\n","# 后者应该不用了，因为它没有腾讯词嵌入的东西多。\n","import os\n","# if os.path.isfile(\"Tencent_AILab_ChineseEmbedding.tar.gz\") == False:\n","#   !cp /content/drive/MyDrive/data/Tencent_AILab_ChineseEmbedding.tar.gz ./\n","#   !tar -zxvf Tencent_AILab_ChineseEmbedding.tar.gz\n","#\n","# if os.path.isfile(\"/content/wiki.zh.vec\") == False:\n","#   !cp /content/drive/MyDrive/data/wiki.zh.vec .\n","\n","from pylab import mpl\n","import pickle as p\n","# from keras.layers import Input, Dense\n","# from keras.models import Model\n","\n","loss_plot = []\n","accu_plot = []\n","\n","from tensorflow.keras.layers.experimental.preprocessing import RandomFlip\n","from tensorflow.keras.layers.experimental.preprocessing import RandomRotation\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: thulac in /usr/local/lib/python3.7/dist-packages (0.2.1)\n","Requirement already satisfied: annoy in /usr/local/lib/python3.7/dist-packages (1.17.0)\n","Requirement already satisfied: mxnet in /usr/local/lib/python3.7/dist-packages (1.8.0.post0)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet) (0.8.4)\n","Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.19.5)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"id":"QHVjsWd82Sy8","executionInfo":{"elapsed":15868,"status":"ok","timestamp":1626491839116,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"3defd210-204f-44d9-c02f-05030adf4e7f"},"source":[""],"execution_count":null,"outputs":[{"output_type":"stream","text":["mv: cannot stat '/content/Leaves2/33': No such file or directory\n","mv: cannot stat '/content/Leaves2/32': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"id":"7zhQzXPgbhha","executionInfo":{"elapsed":1616,"status":"ok","timestamp":1627090563553,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"bd9c756d-95db-421f-cd69-53bac39982e3"},"source":["# @title 获取文本featrue\n","alltxtdata = \"\"\n","\n","def collectalltxt(pathofflaviatxt):\n","    alltxtdata = \"\"\n","    for x in os.listdir(pathofflaviatxt):\n","        # print (x)\n","        alltxtdata += codecs.open(os.path.join(pathofflaviatxt, x), encoding=\"gbk\").read()\n","    return alltxtdata\n","\n","alltxtdata = collectalltxt(pathofflaviatxt)\n","\n","# Choose the top 5000 words from the vocabulary\n","top_k = 10000\n","tokenizer = \"\"\n","keyfeature = \"叶\"\n","BUFFER_SIZE = 1000\n","BATCH_SIZE = 64\n","targetLen = 150\n","debugm = 0 # if 0,means only 20% data, if 1 ,means all data .\n","\n","if os.path.isfile(\"tokenizer.bin\") == True:\n","    tokenizer = p.load(open(\"tokenizer.bin\", \"rb\"))\n","\n","def getTokenizerAndKeyword(top_k, alltxtdata, keyfeature):\n","    #                           filters='!\"#$%&*+,:;=?@[\\]^_`{|}~ ')\n","    # tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n","    #                                                   oov_token=\"<unk>\",\n","    #                                                   filters='')\n","    # # tokenizer.fit_on_texts(alltxtdata)\n","    # tokenizer.fit_on_texts(jieba.lcut(alltxtdata) + list(alltxtdata))\n","    # tokenizer.word_index['<pad>'] = 0\n","    # tokenizer.index_word[0] = '<pad>'\n","    # # print(len(tokenizer.word_counts))\n","    # tokenizer.word_index['<start>'] = len(tokenizer.word_counts)\n","    # tokenizer.index_word[len(tokenizer.word_counts)] = '<start>'\n","    # tokenizer.word_index['<end>'] = len(tokenizer.word_counts) + 1\n","    # tokenizer.index_word[len(tokenizer.word_counts) + 1] = '<end>'\n","\n","    # 从所有的文本feature中，找到包含叶的句子，然后抽取关键词。\n","    allfiletrue = re.sub(r\"\\r|\\n\", r\"\", alltxtdata)\n","    allfeature = list(\n","        x.lstrip().rstrip().replace(\" \", \"\") if keyfeature in x else \"\" for x in re.split(r\"[。;；,]\", allfiletrue))\n","    allfeature = \",\".join([x for x in allfeature if x != \"\"])\n","    feature_keywords = jieba.analyse.extract_tags(allfeature, 50, allowPOS=(\"na\"))\n","    feature_keywords_set = set(feature_keywords)\n","    # print(jieba.lcut(feature))\n","    print(\"feature_keywords:\", feature_keywords)\n","    return tokenizer, feature_keywords, feature_keywords_set\n","tokenizer, feature_keywords, feature_keywords_set = getTokenizerAndKeyword(top_k, alltxtdata, keyfeature)\n","\n","# 从文本中抽取包含叶的关键句子，然后向量化。\n","# 找出文本中包含关键字的那些句子，\n","def gettxtfeature(file1):\n","    filetxt = codecs.open(file1, encoding=\"gbk\").read()\n","    filetxt = re.sub(r\"\\r|\\n\", r\"\", filetxt)\n","    feature1 = []\n","    str1 = []\n","    feature = list(\n","        x.lstrip().rstrip().replace(\" \", \"\") if keyfeature in x else \"\" for x in re.split(r\"[。;；,：:a-zA-Z]\", filetxt))\n","    for x in feature:\n","        if x != \"\":\n","            for x1 in x.split(\"，\"):\n","                # if list(feature_keywords_set & set(jieba.lcut(x1))) != []:\n","                if list(feature_keywords_set & set(jieba.lcut(x1, HMM=False))) != []:\n","                    str1.append(x1)\n","    return str1\n","\n","# for x in os.listdir(\"/content/flaviatxt\"):\n","#   print(gettxtdatatest(os.path.join(\"/content/flaviatxt\", x))[1])\n","# gettxtdatatest(\"/content/flaviatxt/1.txt\")\n","\n","# @title get image feature， 准备img dataset\n","# InceptionV3\n","def encodeimgModel():\n","  # model of get image features\n","  image_model = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n","  new_input = image_model.input\n","  hidden_layer = image_model.layers[-1].output\n","  image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n","  return image_features_extract_model\n","\n","# InceptionV3 guhua\n","# def encodeimgModel():\n","#   model_image = tf.keras.models.load_model(\"model_image\")\n","#   image_model = model_image.layers[:-1]\n","#   image_model = tf.keras.Model(model_image.input, model_image.layers[-2].output)\n","#   return image_model\n","\n","# # 给定图片文件，加载图片转化为 model 使用的格式\n","def load_image(image_path):\n","  img = tf.io.read_file(image_path)\n","  img = tf.image.decode_jpeg(img, channels=3)\n","  img = tf.image.resize(img, (224, 224))\n","  img = tf.keras.applications.vgg19.preprocess_input(img)\n","  return img, image_path\n","\n","# InceptionV3\n","# def encodeimgModel():\n","#   # model of get image features\n","#   image_model = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')\n","#   new_input = image_model.input\n","#   hidden_layer = image_model.layers[-1].output\n","#   image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n","#   return image_features_extract_model\n","\n","# # InceptionV3 guhua\n","# # def encodeimgModel():\n","# #   model_image = tf.keras.models.load_model(\"model_image\")\n","# #   image_model = model_image.layers[:-1]\n","# #   image_model = tf.keras.Model(model_image.input, model_image.layers[-2].output)\n","# #   return image_model\n","\n","# # # 给定图片文件，加载图片转化为 model 使用的格式\n","# def load_image(image_path):\n","#   img = tf.io.read_file(image_path)\n","#   img = tf.image.decode_jpeg(img, channels=3)\n","#   img = tf.image.resize(img, (299, 299))\n","#   img = tf.keras.applications.inception_v3.preprocess_input(img)\n","#   return img, image_path\n","\n","# below code use desnet201 as feature extract, output is (1, 7,7, 1920)\n","# def encodeimgModel():\n","#     image_model = tf.keras.applications.DenseNet201(include_top=False, weights='imagenet')\n","#     new_input = image_model.input\n","#     hidden_layer = image_model.layers[-1].output\n","#     image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n","#     return image_features_extract_model\n","\n","\n","# def load_image(image_path):\n","#     # print(image_path)\n","#     img = tf.io.read_file(image_path)\n","#     img = tf.image.decode_jpeg(img, channels=3)\n","#     img = tf.image.resize(img, (224, 224))\n","#     # img = tf.keras.applications.DenseNet201..  preprocess_input(img)\n","#     return img, image_path\n","image_features_extract_model = encodeimgModel()\n","\n","data_augmentation = tf.keras.Sequential([\n","  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n","  tf.keras.layers.experimental.preprocessing.RandomRotation(0.3),\n","])\n","# image_features_extract_model.summary()\n","# Load the numpy files，获取 image_features_extract_model 过后的 feature，转化为[1, 64, 2048]\n","\n","def create_wbData(liang=3):\n","    global wb_d\n","    wb_data = []\n","    wb_label = []\n","    wb_d = {}\n","    for wbf in os.listdir(pathofflaviatxt):\n","        wb_d[wbf] = gettxtdata_wb(wbf)\n","    print(wb_d)\n","    print(wb_d.keys())\n","    for _ in range(liang):\n","        for x in wb_d.keys():\n","            desc_txt = wb_d[x]\n","            feature1 = []\n","            feature1.append([y[0] for y in tokenizer.texts_to_sequences(\n","                [f\"<start>\"] + jieba.lcut(getrandstr(desc_txt, 10), HMM=False) + [f\"<end>\"])])\n","            feature1 = tf.keras.preprocessing.sequence.pad_sequences(feature1, padding='post', maxlen=150)[0]\n","            wb_data.extend([feature1])\n","\n","            # wb_data.extend([getrandstr(desc_txt, 8)])\n","            wb_label.extend([int(x.split(\".\")[0])])\n","            # break\n","    return np.array(wb_data), np.array(wb_label)\n","def getrandstr(text1, num1):\n","    # for _ in range(num1):\n","    str2 = \"\"\n","    rn = np.random.randint(low=0, high=len(text1), size=num1)\n","    # print (rn)\n","    for _i in rn:\n","        tempstr = text1[_i]\n","        # print(\"tempstr:\",tempstr)\n","        if str2 != \"\":\n","            str2 += \",\"\n","        str2 += tempstr\n","    return str2\n","\n","def map_func_img_txt(img_name, labclass, filename):\n","  global wb_d\n","  feature1 = []\n","  img_name = img_name.decode()\n","  labclass = labclass.decode()\n","#   print (labclass)\n","  # img_tensor = load_image(os.path.join(labclass, img_name))\n","  img_tensor = load_image(os.path.join(os.path.join(imgrootpath, labclass), img_name))\n","#   img_tensor = tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal')(tf.expand_dims(img_tensor[0], 0))\n","  img_tensor = data_augmentation(tf.expand_dims(img_tensor[0], 0))\n","#   print(img_tensor[0])\n","  img_tensor = image_features_extract_model(tf.expand_dims(img_tensor[0], 0))\n","#   print(\"img_tensor:\", img_tensor.shape)\n","  img_tensor = tf.reshape(img_tensor, (img_tensor.shape[0], -1, img_tensor.shape[3]))\n","  img_tensor = tf.squeeze(img_tensor, 0)\n","\n","#   txtfeature = gettxtfeature(os.path.join(pathofflaviatxt, \"%d.txt\" % int(labclass)))\n","  desc_txt = wb_d[\"%d\"%int(labclass) +\".txt\"]\n","  str2 = \"\"\n","  while len(str2) < targetLen - 5:\n","      rn = np.random.randint(low=0, high=len(desc_txt), size=1)\n","      tempstr = desc_txt[rn[0]]\n","      if str2 != \"\":\n","          str2 += \",\"\n","      str2 += tempstr\n","  # print(str2)\n","  # print (jieba.lcut(str2, HMM=False))\n","  feature1.append([y[0] for y in tokenizer.texts_to_sequences([f\"<start>\"] + jieba.lcut(str2, HMM=False)+ [f\"<end>\"])])\n","#   feature1.append([y[0] for y in tokenizer.texts_to_sequences([f\"<start>\"] + jieba.lcut(getrandstr(desc_txt, np.random.randint(10,11)), HMM=False)+ [f\"<end>\"])])\n","  feature1 = tf.keras.preprocessing.sequence.pad_sequences(feature1, padding='post', maxlen=targetLen)[0]\n","  # print(feature1)\n","  filename = os.path.join(labclass, filename)\n","  return img_tensor, feature1, filename\n","\n","# below code is for val_dataset is same for every epoch, if below code is run only once.\n","\n","def prepareds(imgrootpath, data_times):\n","  img_to_labelclass = collections.defaultdict(list)\n","  for root, dirs, files in os.walk(imgrootpath, topdown=False):\n","      for f in files:\n","        img_to_labelclass[f].append(root.split(\"/\")[-1])\n","  # get random filenamelist to shuffle ，同时将做开关将样本数减少以利于调试。\n","  if debugm == 0:\n","    img_keys = list(img_to_labelclass.keys())* data_times\n","  else:\n","    img_keys = list(img_to_labelclass.keys())\n","    img_keys = img_keys[:int(len(img_keys) / 5)]\n","  random.seed(a=123)\n","  random.shuffle(img_keys)\n","  print(\"len(img_keys):\", len(img_keys))\n","  # prepare pair [filename, label]\n","  slice_index = int(len(img_keys) * 0.8)\n","  img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n","  img_name_train = []\n","  cap_train = []\n","  filname_train = []\n","  print(\"start get feature of train data\")\n","  for imgt in tqdm(img_name_train_keys):\n","      # imgt_d, imgt_l = map_func_img(imgt.encode(), img_to_labelclass[imgt][0].encode())\n","      imgt_d, imgt_l, imgt_f = map_func_img_txt(imgt.encode(), img_to_labelclass[imgt][0].encode(), imgt)\n","      img_name_train.extend([imgt_d])\n","      cap_train.extend([imgt_l])\n","      filname_train.extend([imgt_f])\n","  print(\"start get feature of val data\")\n","  img_name_val = []\n","  cap_val = []\n","  filname_val = []\n","  for imgv in tqdm(img_name_val_keys):\n","      # imgv_d, imgv_l = map_func_img(imgv.encode(), img_to_labelclass[imgv][0].encode())\n","      imgv_d, imgv_l, imgv_f = map_func_img_txt(imgv.encode(), img_to_labelclass[imgv][0].encode(), imgv)\n","      img_name_val.extend([imgv_d])\n","      cap_val.extend([imgv_l])\n","      filname_val.extend([imgv_f])\n","\n","  print(\"len(img_name_train), len(cap_train), len(img_name_val), len(cap_val):\")\n","  print(len(img_name_train), len(cap_train), len(img_name_val), len(cap_val))\n","  # here, the cap_train, cap_val are label\n","  train_dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train, filname_train))\n","  val_dataset = tf.data.Dataset.from_tensor_slices((img_name_val, cap_val, filname_val))\n","  print(\"all image feature have done \")\n","  print(\"start change item2 to txt feature\")\n","  # Shuffle and batch\n","  train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","  train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n","  # Shuffle and batch\n","  val_dataset = val_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n","  val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n","  return train_dataset, val_dataset\n","\n","# train_dataset, val_dataset = prepareds(imgrootpath, 1)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["feature_keywords: ['无毛', '基部', '柔毛', '先端', '圆形', '楔形', '侧脉', '椭圆形', '叶片', '锯齿', '卵形', '披针', '针形', '小叶', '纸质', '裂片', '花序', '深绿色', '淡绿色', '明显', '圆状', '倒卵形', '边缘', '叶腋', '腺体', '卵状', '短枝', '叶面', '图版', '树皮', '樟脑', '长枝', '革质', '锐尖', '全缘', '小枝', '支脉', '花梗', '花枝', '苞片', '长圆', '大叶', '绿色', '直径', '心形', '药用', '木材', '灰绿色', '细小', '樟树']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3QJ-B5IVCuHF"},"source":["!rm /content/checkpoints -fr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"code","id":"z0W_11Q7bA9H"},"source":["# model_text.trainable = True\n","# @title model 1 define， 拆分3.1\n","# embedding_dim = 256\n","# units = 512\n","top_k = 10000\n","vocab_size = top_k + 1\n","embedding_dim = 1024\n","units = 512\n","\n","\n","class BahdanauAttention(tf.keras.Model):\n","    def __init__(self, units):\n","        super(BahdanauAttention, self).__init__()\n","        self.W1 = tf.keras.layers.Dense(units)\n","        self.W2 = tf.keras.layers.Dense(units)\n","        self.V = tf.keras.layers.Dense(1)\n","\n","    def call(self, features, hidden):\n","        # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n","\n","        # hidden shape == (batch_size, hidden_size)\n","        # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n","        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","        # print(\"hidden_with_time_axis.shape\", hidden_with_time_axis.shape)\n","        # print(\"features.shape\", features.shape)\n","\n","        # attention_hidden_layer shape == (batch_size, 64, units)\n","        attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n","                                             self.W2(hidden_with_time_axis)))\n","        # print(\"attention_hidden_layer.shape\", attention_hidden_layer.shape)\n","        # score shape == (batch_size, 64, 1)\n","        # This gives you an unnormalized score for each image feature.\n","        score = self.V(attention_hidden_layer)\n","        # print(\"score.shape\", score.shape)\n","\n","        # attention_weights shape == (batch_size, 64, 1)\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","\n","        # context_vector shape after sum == (batch_size, hidden_size)\n","        context_vector = attention_weights * features\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","        return context_vector, attention_weights\n","\n","\n","class CNN_Encoder(tf.keras.Model):\n","    # Since you have already extracted the features and dumped it using pickle\n","    # This encoder passes those features through a Fully connected layer\n","    def __init__(self, embedding_dim):\n","        super(CNN_Encoder, self).__init__()\n","        # shape after fc == (batch_size, 64, embedding_dim)\n","        self.fc = tf.keras.layers.Dense(embedding_dim)\n","        # self.fc1 = tf.keras.layers.Dense(4096)\n","        # self.dp = tf.keras.layers.Dropout(0.3)\n","\n","    def call(self, x):\n","        # print(\"x.shape:\", x.shape)\n","        # x = tf.keras.layers.GlobalAveragePooling2D()(x)\n","        # x = self.fc(x)\n","        # print(\"x.shape:\", x.shape)\n","        # x = tf.keras.layers.Dense(2048)(x)\n","        # x = self.fc1(x)\n","        # x = self.dp(x)\n","        x = self.fc(x)\n","        # print(\"x.shape:\", x.shape)\n","        x = tf.nn.relu(x)\n","        # print(\"x.shape:\", x.shape)\n","        return x\n","\n","\n","class RNN_Decoder(tf.keras.Model):\n","    def __init__(self, embedding_dim, units, vocab_size):\n","        super(RNN_Decoder, self).__init__()\n","        self.units = units\n","\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = tf.keras.layers.GRU(self.units,\n","                                       return_sequences=True,\n","                                       return_state=True,\n","                                       recurrent_initializer='glorot_uniform')\n","        self.gru1 = tf.keras.layers.GRU(self.units,\n","                                       return_sequences=True,\n","                                       return_state=True,\n","                                       recurrent_initializer='glorot_uniform')\n","        self.fc1 = tf.keras.layers.Dense(self.units)\n","        self.fc2 = tf.keras.layers.Dense(vocab_size)\n","\n","        self.attention = BahdanauAttention(self.units)\n","\n","    def call(self, x, features, hidden):\n","        # defining attention as a separate model\n","        context_vector, attention_weights = self.attention(features, hidden)\n","\n","        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","        x = self.embedding(x)\n","\n","        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","        # passing the concatenated vector to the GRU\n","        output, state = self.gru(x)\n","        # output, state = self.gru1(output)\n","\n","        # shape == (batch_size, max_length, hidden_size)\n","        x = self.fc1(output)\n","\n","        # x shape == (batch_size * max_length, hidden_size)\n","        x = tf.reshape(x, (-1, x.shape[2]))\n","\n","        # output shape == (batch_size * max_length, vocab)\n","        x = self.fc2(x)\n","\n","        return x, state, attention_weights\n","\n","    def reset_state(self, batch_size):\n","        return tf.zeros((batch_size, self.units))\n","\n","\n","def loss_function(real, pred):\n","    mask = tf.math.logical_not(tf.math.equal(real, 0))\n","    # print (mask)\n","    # print(\"real:\", real.shape, \"pred:\", pred.shape)\n","    loss_ = loss_object(real, pred)\n","    mask = tf.cast(mask, dtype=loss_.dtype)\n","    # print (loss_)\n","    loss_ *= mask\n","    # print(loss_.shape)\n","    # print(\"tf.reduce_mean(loss_):\", tf.reduce_mean(loss_))\n","    return tf.reduce_mean(loss_)\n","\n","\n","def decay(epoch):\n","  if epoch < 20:\n","    print(\"learning rate: 1e-3\")\n","    return 1e-3\n","  elif epoch >= 20 and epoch < 60:\n","    print(\"learning rate: 1e-3\")\n","    return 1e-4\n","  elif epoch >= 60 and epoch < 90:\n","    print(\"learning rate: 1e-3\")\n","    return 1e-5\n","  else:\n","    print(\"learning rate: 1e-3\")\n","    return 1e-6\n","\n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","  def __init__(self):\n","    super(CustomSchedule, self).__init__()\n","\n","    # self.d_model = d_model\n","    # self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","    # self.warmup_steps = warmup_steps\n","\n","#   def __call__(self, step):\n","#     arg1 = tf.math.rsqrt(step)\n","#     arg2 = step * (self.warmup_steps ** -1.5)\n","\n","#     return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n","  \n","  def __call__(self, step):\n","    if step < 20:\n","        print(\"learning rate: 1e-3\")\n","        return 1e-3\n","    elif step >= 20 and step < 60:\n","        print(\"learning rate: 1e-4\")\n","        return 1e-4\n","    elif step >= 60 and step < 90:\n","        print(\"learning rate: 1e-5\")\n","        return 1e-5\n","    else:\n","        print(\"learning rate: 1e-6\")\n","        return 1e-6\n","\n","learning_rate1 = CustomSchedule()\n","# learning_rate_fn = tf.keras.optimizers.schedules.LearningRateSchedule(decay)\n","\n","boundaries1 = [20, 40, 60, 80]\n","boundaries1 = list(np.asarray(boundaries1)*23)\n","values1 = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n","learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(\n","    boundaries1, values1)\n","\n","encoder = CNN_Encoder(embedding_dim)\n","decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n","optimizer = tf.keras.optimizers.Adam()\n","# optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00001)\n","# optimizer = tf.keras.optimizers.Adam()\n","# optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate_fn)\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","# loss_object1 = tf.keras.losses.SparseCategoricalCrossentropy(\n","#     reduction='none')\n","loss_object1 = tf.keras.losses.mean_squared_error\n","loss_object2 = tf.keras.losses.categorical_crossentropy\n","\n","checkpoint_path = \"./checkpoints/train\"\n","ckpt = tf.train.Checkpoint(encoder=encoder,\n","                           decoder=decoder,\n","                           optimizer=optimizer)\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n","start_epoch = 0\n","if ckpt_manager.latest_checkpoint:\n","    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n","    print(\"start_epoch:\", start_epoch)\n","    # restoring the latest checkpoint in checkpoint_path\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n","\n","# adding this in a separate cell because if you run the training cell\n","# many times, the loss_plot array will be reset\n","# loss_plot = []\n","# accu_plot = []\n","\n","# base_model_DenseNet201 = tf.keras.applications.DenseNet201(input_shape=(224,224,3),\n","#   include_top=False,\n","#   weights='imagenet'\n","#   )\n","# base_model = base_model_DenseNet201\n","# base_model.trainable = False\n","\n","# global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n","# prediction_layer = tf.keras.layers.Dense(32)\n","# model_image = Sequential()\n","# # model.add(rescale)\n","# # model.add(base_model)\n","# model_image.add(Flatten())\n","# # model_image.add(global_average_layer)\n","# model_image.add(tf.keras.layers.Dense(1024))\n","# model_image.add(tf.keras.layers.Dense(256))\n","# model_image.add(prediction_layer)\n","\n","\n","@tf.function\n","def train_step(img_tensor, target, fname):\n","  global predictions_txtlist\n","#   print(img_tensor.shape)\n","  loss = 0\n","  # initializing the hidden state for each batch\n","  # because the captions are not related from image to image\n","  hidden = decoder.reset_state(batch_size=target.shape[0])\n","  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n","  predictions_txtlist = dec_input\n","  predictions_txtlist = tf.cast(predictions_txtlist, dtype=tf.int64)\n","  # predictions_txtlist = tf.squeeze(predictions_txtlist, 1)\n","#   img_tensor = tf.reshape(img_tensor, [64, 8,8,2048])\n","  with tf.GradientTape() as tape:\n","    features = encoder(img_tensor)\n","\n","    for i in range(1, target.shape[1]):\n","      # passing the features through the decoder\n","    #   print (dec_input)\n","      predictions, hidden, _ = decoder(dec_input, features, hidden)\n","    #   print(\"predictions:\", predictions)\n","      predicted_id = tf.random.categorical(predictions, 1)\n","      # predicted_id = tf.case(predicted_id, dtype=tf.int64)\n","    #   print(\"predicted_id:\", predicted_id)\n","\n","      # print (tf.equal(predictions_txtlist , 0))\n","      # if tf.equal(predictions_txtlist , 0):\n","      #     predictions_txtlist = tf.squeeze(predicted_id)\n","      # else:\n","      #     print (\"predictions_txtlist:\", predictions_txtlist)\n","      #     print (\"tf.squeeze(predicted_id):\", tf.squeeze(predicted_id))\n","      # temp1 = tf.squeeze(predicted_id)\n","      # print (temp1)\n","    #   print (\"before predictions_txtlist:\", predictions_txtlist)\n","      predictions_txtlist = tf.concat([predictions_txtlist, predicted_id], axis = 1)\n","      \n","      # predictions_txtlist = tf.squeeze(predictions_txtlist, -1)\n","      # predictions_txtlist = tf.reshape(predictions_txtlist, [predictions_txtlist[0], predictions_txtlist[-1]])\n","    #   print (\"predictions_txtlist:\", predictions_txtlist)\n","          # predictions_txtlist = tf.concat((predictions_txtlist, predicted_id), axis=1)\n","          # tf.stack((a, b), axis=2)\n","          # tf.stack((a, b), axis=1)\n","      # predicted_id = predicted_id[0][0]\n","      # print(predicted_id)\n","      # predicted_id = predicted_id.numpy()\n","      # print(predicted_id)\n","      # predictions_txtlist.append(tokenizer.index_word.get(predicted_id, \" \"))\n","      # predictions_list.append(predictions)\n","      loss += loss_function(target[:, i], predictions)\n","      # print (loss)\n","      # using teacher forcing\n","      dec_input = tf.expand_dims(target[:, i], 1)\n","    #   print (\"\\n\")\n","    if 0:\n","        # tf.reduce_mean(loss_object(fname1, predictresult))\n","        # print(\"predictions_txtlist:\",predictions_txtlist)\n","        # fname1= [int(x.numpy().split(b\"/\")[0]) for x in fname]\n","        # print(\"fname1:\",fname)\n","        # p.dump(predictions_txtlist, open(\"predictions_txtlist.bin\", \"wb\"))\n","        predictresult = tf.math.softmax(model_text(predictions_txtlist))\n","        # print (\"predictresult 1:\", predictresult)\n","        # print(\"fname:\", fname)\n","        # print(\"tf.argmax(predictresult):\", tf.argmax(predictresult, axis=-1))\n","        l = loss_object1(fname, tf.argmax(predictresult, axis=-1))\n","        # l[tf.is_nan(l)]=0\n","        # l = tf.clip_by_value(l, -10, 10)\n","        # p.dump(fname, open(\"fname.bin\", \"wb\"))\n","        # p.dump(predictresult, open(\"predictresult.bin\", \"wb\"))\n","        # loss1 = tf.reduce_mean(l)\n","        loss1 = l\n","        loss1 = tf.cast(loss1, dtype=loss.dtype) * 0.1\n","\n","        m = loss_object2(tf.cast(tf.one_hot(tf.cast(fname, dtype=tf.int32), 32), dtype=tf.float32), tf.cast(predictresult, dtype=tf.float32))\n","        loss2 = tf.reduce_mean(m)\n","        # print (\"loss2:\", loss2)\n","        # print (\"loss1:\", loss1)\n","        # print (\"lossl:\", l)\n","        if 0:\n","            loss += loss1\n","        if 1:\n","            loss += loss2.numpy()\n","        # print (\"loss 2:\", loss)\n","    # l = loss_object1(fname, tf.argmax(predictresult, axis=-1))\n","    # loss1 = l\n","    # loss1 = tf.cast(loss1, dtype=loss.dtype) * 0.1\n","\n","    # predictresult = tf.math.softmax(model_image(img_tensor))\n","    # m = loss_object2(tf.cast(tf.one_hot(tf.cast(fname, dtype=tf.int32), 32), dtype=tf.float32), tf.cast(predictresult, dtype=tf.float32))\n","    # loss2 = tf.reduce_mean(m)\n","    # loss += loss2.numpy()\n","\n","  # print(\"loss:\", loss.shape)\n","  # print(\"loss:\", loss.shape)\n","  # print(\"loss1:\", loss_object(fname1, predictresult))\n","  # for x,y in zip(iter(fname), iter(predictresult)):\n","    # print (x,y)\n","  # raise(\"456\")\n","  total_loss = (loss / int(target.shape[1]))\n","#   total_loss/=0.1\n","#   tf.print(\"loss:\", loss)\n","#   loss=loss*0.1\n","#   tf.print(\"loss:\", loss)\n","  trainable_variables = encoder.trainable_variables + decoder.trainable_variables # + model_image.trainable_variables\n","  gradients = tape.gradient(loss, trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, trainable_variables))\n","  return loss, total_loss\n","# train_step(img_tensor, target, flist)\n","# fname1= [int(x.numpy().split(b\"/\")[0]) for x in fname]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"code","id":"SONKCEt7WeZA"},"source":["#@title evaluate 函数\n","max_length = 100\n","attention_features_shape = 49 # 64 for inceptionV3, 49 for desnet201\n","\n","# checkpoint_path = \"./checkpoints/train\"\n","# ckpt = tf.train.Checkpoint(encoder=encoder,\n","#                            decoder=decoder,\n","#                            optimizer=optimizer)\n","# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n","# start_epoch = 0\n","# if ckpt_manager.latest_checkpoint:\n","#     start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n","#     print(\"start_epoch:\", start_epoch)\n","#     # restoring the latest checkpoint in checkpoint_path\n","#     ckpt.restore(ckpt_manager.latest_checkpoint)\n","\n","@tf.function\n","def evaluate(images_tensor, target, fname):\n","    attention_plot = np.zeros((max_length, attention_features_shape))\n","    # print(\"image.shape\", image.shape)\n","\n","    # hidden = decoder.reset_state(batch_size=1)\n","    hidden = decoder.reset_state(batch_size=target.shape[0])\n","\n","    # temp_input = tf.expand_dims(load_image(image)[0], 0)\n","    # temp_input = tf.expand_dims(image, 0)\n","    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n","    dec_end = tf.expand_dims([tokenizer.word_index['<end>']] * target.shape[0], 1)\n","    dec_end = tf.cast(dec_end, dtype=tf.int64)\n","    dec_zeros = tf.zeros((target.shape[0], 150 - 110 + 2),dtype=tf.int64) # 2 means length of start and end \n","\n","\n","    # img_tensor_val = image_features_extract_model(temp_input)\n","    # img_tensor_val = temp_input\n","    # img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n","\n","    # print(\"images_tensor.shape\", images_tensor.shape)\n","    # images_tensor = tf.reshape(images_tensor, [64, 8,8,2048])\n","    features = encoder(images_tensor)\n","    # print(\"features.shape:\", features.shape)\n","\n","    # dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n","    # print (\"hidden:\",hidden.shape)\n","    # print (\"dec_input:\",dec_input.shape)\n","    # print (\"features:\",features.shape)\n","    # raise (\"123\")\n","    \n","    result = []\n","    # predictions_txtlist = []\n","\n","    predictions_txtlist = dec_input\n","    predictions_txtlist = tf.cast(predictions_txtlist, dtype=tf.int64)\n","\n","    for i in range(max_length):\n","        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n","        # print(attention_weights.shape)\n","\n","        # attention_plot[i] = tf.reshape(attention_weights, (-1,)).numpy()\n","\n","        # predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n","        # result.append(tokenizer.index_word.get(predicted_id, \" \"))\n","\n","        predicted_id = tf.random.categorical(predictions, 1)\n","        predictions_txtlist = tf.concat([predictions_txtlist, predicted_id], axis = 1)\n","\n","\n","        # if tokenizer.index_word[predicted_id] == '<end>':\n","        #     return result, attention_plot\n","\n","        # if tokenizer.index_word.get(predicted_id, \" \") == '<end>':\n","        #     return result, attention_plot\n","\n","        # dec_input = tf.expand_dims([predicted_id], 0)\n","        dec_input = predicted_id\n","    predictions_txtlist = tf.concat([predictions_txtlist, dec_end], axis = 1)\n","    predictions_txtlist = tf.concat([predictions_txtlist, dec_zeros], axis = 1)\n","    \n","    # attention_plot = attention_plot[:len(result), :]\n","    return predictions_txtlist, attention_plot\n","\n","# @tf.function\n","def testonvalimage():\n","    fname1_list = []\n","    result_list = []\n","    # for imgs, ls, fl in tqdm(val_dataset,ncols=80):\n","    # for imgs, ls, fl in val_dataset:\n","    for (batch, (imgs, ls, fl)) in enumerate(val_dataset):\n","    # for (batch, (imgs, ls, fl)) in enumerate(train_dataset):\n","    # for (batch, (imgs, ls, fl)) in enumerate(train_dataset1):\n","        # print (imgs, ls, fl)\n","        # raise (\"123\")\n","        if len(imgs) != 64:\n","            break\n","        \n","        # for x in fl:\n","        #     print(tf.strings.split(x,b\"/\"))\n","            # int(x.numpy().split(b\"/\")[0]\n","        # fname1= [int(x.split(b\"/\")[0]) for x in fl]\n","        fname1= [int(tf.strings.split(x,b\"/\")[0]) for x in fl]\n","        # fname1 = np.array(fname1, dtype=np.float32)\n","        fname1 = tf.convert_to_tensor(fname1, dtype=tf.float32)\n","        # print (\"fname1:\", fname1)\n","\n","        # batch_loss, t_loss = train_step(img_tensor, target, fname1)\n","        # print(imgs, ls, fl)\n","        # print(imgs.shape)\n","        # print(ls.shape)\n","\n","        # rid = np.random.randint(0, len(imgs))\n","        # print(rid)\n","        # image = imgs[rid]\n","        # print(imgs[rid])\n","        # print(ls[rid])\n","        # print(fl[rid])\n","        # print(\"image.shape\", image.shape)\n","        # print(image,ls[rid])\n","        # real_caption = ' '.join([tokenizer.index_word[i] for i in np.array(ls[rid]) if i not in [0]])\n","        # print(real_caption)\n","        result = []\n","        # for _ in range(10):\n","        #   result1, attention_plot = evaluate(image)\n","        #   result.extend(result1[:-1])\n","        # while len(result) <50:\n","        result, attention_plot = evaluate(imgs, ls, fname1)\n","        # print(len(result[0]))\n","        # print(fl[rid])\n","        # print(fl[rid].decode())\n","        # real_caption = codecs.open(\"./flaviatxt/\" + \"%0d\" % int(fl[rid].numpy().decode().split(\"/\")[0]) + \".txt\", encoding=\"gbk\").read()\n","        # print('Real Caption:', real_caption)\n","        # print(result)\n","        # tokenizer.text\n","        # print (tokenizer.text  result)\n","        # print(\"result:\",result)\n","        # print(\"tf.one_hot(fname1, 32):\", tf.one_hot(tf.cast(fname1,dtype=tf.int32), 32))\n","        # if fname1_list == []:\n","        #     fname1_list = fname1\n","        # else:\n","        #     fname1_list = tf.concat([fname1_list, fname1], axis = 1)\n","        fname1_list.append(fname1)\n","        result_list.append(result)\n","        # if result_list == []:\n","        #     result_list = result\n","        # else:\n","        #     result_list = tf.concat([result_list, result], axis = 1)\n","        \n","        # print(tokenizer.texts_to_sequences([result]))\n","        # result_seq = tokenizer.texts_to_sequences([result])\n","\n","\n","        # np.argmax(model_text.predict(result_seq))\n","        # print(np.argmax(model_text.predict(result_seq)))\n","\n","        # print('Prediction Caption:', ''.join(result))\n","        # result, attention_plot = evaluate(image)\n","        # PIL.open\n","        # print(fl[rid])\n","        # print(fl[rid].decode())\n","        # plt.title(\"实际图片\")\n","        # print(result)\n","        # print(result[0])\n","        # print(tokenizer.index_word.get(result[0], \" \"))\n","        # print(tokenizer.sequences_to_texts(result.numpy()))\n","        # result.append(tokenizer.index_word.get(result, \" \"))\n","\n","        # plt.imshow(Image.open(os.path.join(imgrootpath, fl[rid].numpy().decode())))\n","        # testimagepath = os.path.join(imgrootpath, fl[rid].numpy().decode())\n","        # plot_attention(testimagepath, result, attention_plot)\n","    fname1_list = tf.convert_to_tensor(fname1_list, dtype=tf.int32)\n","    result_list = tf.convert_to_tensor(result_list, dtype=tf.int32)\n","\n","    # print(\"fname1_list:\", fname1_list)\n","    # print(\"result_list:\", result_list)\n","    # print ((1,) + tuple(result_list.shape[-1:]))\n","    # print (result_list[1:])\n","    fname1_list = tf.reshape(fname1_list, (-1,))\n","    # fname1_list = tf.reshape(fname1_list, tuple(fname1_list.shape[-1]))\n","    result_list = tf.reshape(result_list, ((-1,) + tuple(result_list.shape[-1:])))\n","    # result_list = tf.reshape(tf.convert_to_tensor(result_list), 0)\n","    # print(\"fname1_list:\", fname1_list)\n","    # print(\"result_list:\", result_list)\n","    \n","    fname1_onehot = tf.one_hot(tf.cast(fname1_list,dtype=tf.int32), 32)\n","    evaresult = model_text.evaluate(tf.cast(result_list, dtype=tf.int32), fname1_onehot)\n","    return evaresult\n","    # print(classification_report(tf.argmax(tf.convert_to_tensor(model_text.predict(tf.cast(result_list, dtype=tf.int32))),axis=1), fname1_list))\n","    # fname1_list = list(fname1_list.numpy())\n","    # print(\"fname1_list:\", fname1_list)\n","    # for x in range(1,34):\n","    #     print(x, fname1_list.count(x))\n","\n","def testontrainimage():\n","    fname1_list = []\n","    result_list = []\n","    # for imgs, ls, fl in tqdm(val_dataset,ncols=80):\n","    # for imgs, ls, fl in val_dataset:\n","    for (batch, (imgs, ls, fl)) in enumerate(train_dataset):\n","    # for (batch, (imgs, ls, fl)) in enumerate(train_dataset):\n","    # for (batch, (imgs, ls, fl)) in enumerate(train_dataset1):\n","        # print (imgs, ls, fl)\n","        # raise (\"123\")\n","        if len(imgs) != 64:\n","            break\n","        \n","        # for x in fl:\n","        #     print(tf.strings.split(x,b\"/\"))\n","            # int(x.numpy().split(b\"/\")[0]\n","        # fname1= [int(x.split(b\"/\")[0]) for x in fl]\n","        fname1= [int(tf.strings.split(x,b\"/\")[0]) for x in fl]\n","        # fname1 = np.array(fname1, dtype=np.float32)\n","        fname1 = tf.convert_to_tensor(fname1, dtype=tf.float32)\n","        # print (\"fname1:\", fname1)\n","\n","        # batch_loss, t_loss = train_step(img_tensor, target, fname1)\n","        # print(imgs, ls, fl)\n","        # print(imgs.shape)\n","        # print(ls.shape)\n","\n","        # rid = np.random.randint(0, len(imgs))\n","        # print(rid)\n","        # image = imgs[rid]\n","        # print(imgs[rid])\n","        # print(ls[rid])\n","        # print(fl[rid])\n","        # print(\"image.shape\", image.shape)\n","        # print(image,ls[rid])\n","        # real_caption = ' '.join([tokenizer.index_word[i] for i in np.array(ls[rid]) if i not in [0]])\n","        # print(real_caption)\n","        result = []\n","        # for _ in range(10):\n","        #   result1, attention_plot = evaluate(image)\n","        #   result.extend(result1[:-1])\n","        # while len(result) <50:\n","        result, attention_plot = evaluate(imgs, ls, fname1)\n","        # print(len(result[0]))\n","        # print(fl[rid])\n","        # print(fl[rid].decode())\n","        # real_caption = codecs.open(\"./flaviatxt/\" + \"%0d\" % int(fl[rid].numpy().decode().split(\"/\")[0]) + \".txt\", encoding=\"gbk\").read()\n","        # print('Real Caption:', real_caption)\n","        # print(result)\n","        # tokenizer.text\n","        # print (tokenizer.text  result)\n","        # print(\"result:\",result)\n","        # print(\"tf.one_hot(fname1, 32):\", tf.one_hot(tf.cast(fname1,dtype=tf.int32), 32))\n","        # if fname1_list == []:\n","        #     fname1_list = fname1\n","        # else:\n","        #     fname1_list = tf.concat([fname1_list, fname1], axis = 1)\n","        fname1_list.append(fname1)\n","        result_list.append(result)\n","        # if result_list == []:\n","        #     result_list = result\n","        # else:\n","        #     result_list = tf.concat([result_list, result], axis = 1)\n","        \n","        # print(tokenizer.texts_to_sequences([result]))\n","        # result_seq = tokenizer.texts_to_sequences([result])\n","\n","\n","        # np.argmax(model_text.predict(result_seq))\n","        # print(np.argmax(model_text.predict(result_seq)))\n","\n","        # print('Prediction Caption:', ''.join(result))\n","        # result, attention_plot = evaluate(image)\n","        # PIL.open\n","        # print(fl[rid])\n","        # print(fl[rid].decode())\n","        # plt.title(\"实际图片\")\n","        # print(result)\n","        # print(result[0])\n","        # print(tokenizer.index_word.get(result[0], \" \"))\n","        # print(tokenizer.sequences_to_texts(result.numpy()))\n","        # result.append(tokenizer.index_word.get(result, \" \"))\n","\n","        # plt.imshow(Image.open(os.path.join(imgrootpath, fl[rid].numpy().decode())))\n","        # testimagepath = os.path.join(imgrootpath, fl[rid].numpy().decode())\n","        # plot_attention(testimagepath, result, attention_plot)\n","    fname1_list = tf.convert_to_tensor(fname1_list, dtype=tf.int32)\n","    result_list = tf.convert_to_tensor(result_list, dtype=tf.int32)\n","\n","    # print(\"fname1_list:\", fname1_list)\n","    # print(\"result_list:\", result_list)\n","    # print ((1,) + tuple(result_list.shape[-1:]))\n","    # print (result_list[1:])\n","    fname1_list = tf.reshape(fname1_list, (-1,))\n","    # fname1_list = tf.reshape(fname1_list, tuple(fname1_list.shape[-1]))\n","    result_list = tf.reshape(result_list, ((-1,) + tuple(result_list.shape[-1:])))\n","    # result_list = tf.reshape(tf.convert_to_tensor(result_list), 0)\n","    # print(\"fname1_list:\", fname1_list)\n","    # print(\"result_list:\", result_list)\n","    \n","    fname1_onehot = tf.one_hot(tf.cast(fname1_list,dtype=tf.int32), 32)\n","    evaresult = model_text.evaluate(tf.cast(result_list, dtype=tf.int32), fname1_onehot)\n","    return evaresult\n","    # print(classification_report(tf.argmax(tf.convert_to_tensor(model_text.predict(tf.cast(result_list, dtype=tf.int32))),axis=1), fname1_list))\n","    # fname1_list = list(fname1_list.numpy())\n","    # print(\"fname1_list:\", fname1_list)\n","    # for x in range(1,34):\n","    #     print(x, fname1_list.count(x))\n","# testontrainimage()\n","# testonvalimage()\n","# testonvalimage()\n","# testonvalimage()\n","# testonvalimage()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yCeMLKEvYk_y","executionInfo":{"elapsed":263332,"status":"ok","timestamp":1626586076947,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"d25b7b56-dfdd-4667-edce-ba75b3213696"},"source":["max_length = 50\n","print(\"testonvalimage50:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 60\n","print(\"testonvalimage60:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 70\n","print(\"testonvalimage70:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 80\n","print(\"testonvalimage80:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 90\n","print(\"testonvalimage90:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 100\n","print(\"testonvalimage100:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 110\n","print(\"testonvalimage110:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 120\n","print(\"testonvalimage120:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 130\n","print(\"testonvalimage130:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 140\n","print(\"testonvalimage140:\", testonvalimage(),testonvalimage(),testonvalimage())\n","# max_length = 150\n","# print(\"testonvalimage150:\", testonvalimage(),testonvalimage(),testonvalimage())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["12/12 [==============================] - 0s 19ms/step - loss: 3.7736 - accuracy: 0.4297\n","12/12 [==============================] - 0s 19ms/step - loss: 3.7847 - accuracy: 0.4193\n","12/12 [==============================] - 0s 18ms/step - loss: 3.5975 - accuracy: 0.4375\n","testonvalimage50: [3.7736175060272217, 0.4296875] [3.784722089767456, 0.4192708432674408] [3.597476005554199, 0.4375]\n","12/12 [==============================] - 0s 20ms/step - loss: 2.9222 - accuracy: 0.5182\n","12/12 [==============================] - 0s 19ms/step - loss: 3.0438 - accuracy: 0.5234\n","12/12 [==============================] - 0s 19ms/step - loss: 2.9788 - accuracy: 0.5078\n","testonvalimage60: [2.922229051589966, 0.5182291865348816] [3.0437793731689453, 0.5234375] [2.978846311569214, 0.5078125]\n","12/12 [==============================] - 0s 20ms/step - loss: 2.2862 - accuracy: 0.6328\n","12/12 [==============================] - 0s 20ms/step - loss: 2.3730 - accuracy: 0.5938\n","12/12 [==============================] - 0s 22ms/step - loss: 2.2291 - accuracy: 0.6276\n","testonvalimage70: [2.2861592769622803, 0.6328125] [2.3729896545410156, 0.59375] [2.229055643081665, 0.6276041865348816]\n","12/12 [==============================] - 0s 22ms/step - loss: 1.5357 - accuracy: 0.7188\n","12/12 [==============================] - 0s 23ms/step - loss: 1.4975 - accuracy: 0.7318\n","12/12 [==============================] - 0s 23ms/step - loss: 1.4053 - accuracy: 0.7135\n","testonvalimage80: [1.5356531143188477, 0.71875] [1.497470498085022, 0.7317708134651184] [1.405260682106018, 0.7135416865348816]\n","12/12 [==============================] - 0s 23ms/step - loss: 1.1265 - accuracy: 0.8073\n","12/12 [==============================] - 0s 23ms/step - loss: 0.8113 - accuracy: 0.8281\n","12/12 [==============================] - 0s 25ms/step - loss: 0.9324 - accuracy: 0.8229\n","testonvalimage90: [1.126487135887146, 0.8072916865348816] [0.8113112449645996, 0.828125] [0.9323765635490417, 0.8229166865348816]\n","12/12 [==============================] - 0s 26ms/step - loss: 0.7700 - accuracy: 0.8932\n","12/12 [==============================] - 0s 25ms/step - loss: 0.6377 - accuracy: 0.9036\n","12/12 [==============================] - 0s 25ms/step - loss: 0.8592 - accuracy: 0.8828\n","testonvalimage100: [0.7699747681617737, 0.8932291865348816] [0.6377186179161072, 0.9036458134651184] [0.8592268824577332, 0.8828125]\n","12/12 [==============================] - 0s 26ms/step - loss: 0.7292 - accuracy: 0.8828\n","12/12 [==============================] - 0s 28ms/step - loss: 0.8122 - accuracy: 0.8776\n","12/12 [==============================] - 0s 26ms/step - loss: 0.7361 - accuracy: 0.8932\n","testonvalimage110: [0.7291519641876221, 0.8828125] [0.8122147917747498, 0.8776041865348816] [0.7360541224479675, 0.8932291865348816]\n","12/12 [==============================] - 0s 29ms/step - loss: 1.0476 - accuracy: 0.7969\n","12/12 [==============================] - 0s 30ms/step - loss: 0.9204 - accuracy: 0.8125\n","12/12 [==============================] - 0s 28ms/step - loss: 1.0497 - accuracy: 0.7995\n","testonvalimage120: [1.047572374343872, 0.796875] [0.920369565486908, 0.8125] [1.0496865510940552, 0.7994791865348816]\n","12/12 [==============================] - 0s 31ms/step - loss: 1.5815 - accuracy: 0.6823\n","12/12 [==============================] - 0s 29ms/step - loss: 1.5717 - accuracy: 0.6849\n","12/12 [==============================] - 0s 29ms/step - loss: 1.5518 - accuracy: 0.6901\n","testonvalimage130: [1.5814896821975708, 0.6822916865348816] [1.5716776847839355, 0.6848958134651184] [1.5518193244934082, 0.6901041865348816]\n","12/12 [==============================] - 0s 33ms/step - loss: 1.8100 - accuracy: 0.6354\n","12/12 [==============================] - 0s 33ms/step - loss: 2.2313 - accuracy: 0.5807\n","12/12 [==============================] - 0s 32ms/step - loss: 2.0310 - accuracy: 0.6250\n","testonvalimage140: [1.8100119829177856, 0.6354166865348816] [2.2312581539154053, 0.5807291865348816] [2.030963897705078, 0.625]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2HQ7OSxuFfWw","executionInfo":{"elapsed":421773,"status":"ok","timestamp":1626592246275,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"c393568f-0def-4c3d-d3f3-3aa82a15765b"},"source":["max_length = 80\n","print(\"testonvalimage80:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 83\n","print(\"testonvalimage83:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 86\n","print(\"testonvalimage86:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 89\n","print(\"testonvalimage89:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 92\n","print(\"testonvalimage92:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 95\n","print(\"testonvalimage95:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 98\n","print(\"testonvalimage98:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 101\n","print(\"testonvalimage101:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 104\n","print(\"testonvalimage104:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 107\n","print(\"testonvalimage107:\", testonvalimage(),testonvalimage(),testonvalimage())\n","max_length = 110\n","print(\"testonvalimage110:\", testonvalimage(),testonvalimage(),testonvalimage())\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["48/48 [==============================] - 1s 25ms/step - loss: 0.1655 - accuracy: 0.9701\n","48/48 [==============================] - 1s 25ms/step - loss: 0.2101 - accuracy: 0.9635\n","48/48 [==============================] - 1s 26ms/step - loss: 0.1756 - accuracy: 0.9714\n","testonvalimage80: [0.1655111163854599, 0.9700520634651184] [0.21008090674877167, 0.9635416865348816] [0.1756054311990738, 0.9713541865348816]\n","48/48 [==============================] - 1s 26ms/step - loss: 0.1809 - accuracy: 0.9733\n","48/48 [==============================] - 1s 25ms/step - loss: 0.1757 - accuracy: 0.9720\n","48/48 [==============================] - 1s 26ms/step - loss: 0.1909 - accuracy: 0.9740\n","testonvalimage83: [0.18088819086551666, 0.9733073115348816] [0.17570118606090546, 0.9720051884651184] [0.1908537894487381, 0.9739583134651184]\n","48/48 [==============================] - 1s 26ms/step - loss: 0.1992 - accuracy: 0.9622\n","48/48 [==============================] - 1s 25ms/step - loss: 0.2051 - accuracy: 0.9642\n","48/48 [==============================] - 1s 26ms/step - loss: 0.2422 - accuracy: 0.9596\n","testonvalimage86: [0.19918513298034668, 0.9622395634651184] [0.2050970196723938, 0.9641926884651184] [0.24218523502349854, 0.9596354365348816]\n","48/48 [==============================] - 1s 26ms/step - loss: 0.2164 - accuracy: 0.9609\n","48/48 [==============================] - 1s 25ms/step - loss: 0.2030 - accuracy: 0.9642\n","48/48 [==============================] - 1s 26ms/step - loss: 0.1909 - accuracy: 0.9655\n","testonvalimage89: [0.21644961833953857, 0.9609375] [0.20298552513122559, 0.9641926884651184] [0.19094650447368622, 0.9654948115348816]\n","48/48 [==============================] - 1s 25ms/step - loss: 0.2096 - accuracy: 0.9648\n","48/48 [==============================] - 1s 25ms/step - loss: 0.2079 - accuracy: 0.9661\n","48/48 [==============================] - 1s 26ms/step - loss: 0.1814 - accuracy: 0.9701\n","testonvalimage92: [0.2096148580312729, 0.96484375] [0.20786087214946747, 0.9661458134651184] [0.18139827251434326, 0.9700520634651184]\n","48/48 [==============================] - 1s 25ms/step - loss: 0.2051 - accuracy: 0.9648\n","48/48 [==============================] - 1s 25ms/step - loss: 0.1898 - accuracy: 0.9661\n","48/48 [==============================] - 1s 25ms/step - loss: 0.2011 - accuracy: 0.9661\n","testonvalimage95: [0.20507031679153442, 0.96484375] [0.1897733360528946, 0.9661458134651184] [0.2010985165834427, 0.9661458134651184]\n","48/48 [==============================] - 1s 26ms/step - loss: 0.1908 - accuracy: 0.9701\n","48/48 [==============================] - 1s 26ms/step - loss: 0.1718 - accuracy: 0.9720\n","48/48 [==============================] - 1s 26ms/step - loss: 0.2309 - accuracy: 0.9616\n","testonvalimage98: [0.19077670574188232, 0.9700520634651184] [0.17180299758911133, 0.9720051884651184] [0.230854332447052, 0.9615885615348816]\n","48/48 [==============================] - 1s 25ms/step - loss: 0.1603 - accuracy: 0.9733\n","48/48 [==============================] - 1s 25ms/step - loss: 0.1958 - accuracy: 0.9688\n","48/48 [==============================] - 1s 26ms/step - loss: 0.1742 - accuracy: 0.9727\n","testonvalimage101: [0.16026978194713593, 0.9733073115348816] [0.19576388597488403, 0.96875] [0.17422010004520416, 0.97265625]\n","48/48 [==============================] - 1s 26ms/step - loss: 0.1677 - accuracy: 0.9753\n","48/48 [==============================] - 1s 25ms/step - loss: 0.1665 - accuracy: 0.9701\n","48/48 [==============================] - 1s 26ms/step - loss: 0.1602 - accuracy: 0.9759\n","testonvalimage104: [0.16766895353794098, 0.9752604365348816] [0.16646818816661835, 0.9700520634651184] [0.16020861268043518, 0.9759114384651184]\n","48/48 [==============================] - 1s 26ms/step - loss: 0.1855 - accuracy: 0.9688\n","48/48 [==============================] - 1s 25ms/step - loss: 0.1728 - accuracy: 0.9701\n","48/48 [==============================] - 1s 26ms/step - loss: 0.1709 - accuracy: 0.9740\n","testonvalimage107: [0.18545715510845184, 0.96875] [0.1728000044822693, 0.9700520634651184] [0.17088580131530762, 0.9739583134651184]\n","48/48 [==============================] - 1s 25ms/step - loss: 0.1563 - accuracy: 0.9740\n","48/48 [==============================] - 1s 25ms/step - loss: 0.2175 - accuracy: 0.9661\n","48/48 [==============================] - 1s 26ms/step - loss: 0.2025 - accuracy: 0.9616\n","testonvalimage110: [0.15625108778476715, 0.9739583134651184] [0.21745383739471436, 0.9661458134651184] [0.20250241458415985, 0.9615885615348816]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GwbFzaehdL_v","executionInfo":{"elapsed":40267,"status":"ok","timestamp":1626591768742,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"9acadea0-ca4b-416f-f3ac-57c463efb4b9"},"source":["max_length = 100\n","print(\"testonvalimage100:\", testonvalimage(),testonvalimage(),testonvalimage())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["48/48 [==============================] - 3s 26ms/step - loss: 0.2116 - accuracy: 0.9655\n","48/48 [==============================] - 1s 25ms/step - loss: 0.2063 - accuracy: 0.9674\n","48/48 [==============================] - 1s 25ms/step - loss: 0.2065 - accuracy: 0.9681\n","testonvalimage100: [0.21163101494312286, 0.9654948115348816] [0.20630019903182983, 0.9674479365348816] [0.2065468579530716, 0.9680989384651184]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cBoqX0w5jv3o"},"source":["!cp /content/checkpoints /content/checkpoints1 -fr\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"30CW8v51j0_w"},"source":["!rm /content/checkpoints -fr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sGI7bS0okF1R"},"source":["!cp /content/drive/MyDrive/data/model_inceptionV3_07131727/checkpoints ./checkpoints -fr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZeEoj-RDrcKz","executionInfo":{"elapsed":718361,"status":"ok","timestamp":1626529676414,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"b61a0247-faa1-4c3a-a8ff-3d2318e59169"},"source":["train_dataset, val_dataset = prepareds(imgrootpath, 3)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 1/4608 [00:00<11:00,  6.97it/s]"],"name":"stderr"},{"output_type":"stream","text":["len(img_keys): 5760\n","start get feature of train data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 4608/4608 [09:33<00:00,  8.03it/s]\n","  0%|          | 1/1152 [00:00<02:16,  8.45it/s]"],"name":"stderr"},{"output_type":"stream","text":["start get feature of val data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 1152/1152 [02:22<00:00,  8.09it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["len(img_name_train), len(cap_train), len(img_name_val), len(cap_val):\n","4608 4608 1152 1152\n","all image feature have done \n","start change item2 to txt feature\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"cellView":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"4zALvNpge5zz","executionInfo":{"elapsed":9329591,"status":"ok","timestamp":1627102739745,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"6b0a076a-d0c3-4c23-c9a0-50e1c11c245f"},"source":["# import os\n","# os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n","# import tensorflow as tf\n","# gpus = tf.config.list_physical_devices('GPU')\n","# gpu = gpus[0]\n","\n","# tf.config.experimental.set_memory_growth(gpu, True)\n","\n","# @title train 拆分3.2\n","                                                        \n","# if os.path.isdir(\"model_image\") == False:\n","#     model_image = tf.keras.models.load_model(\"model_image\")\n","\n","# def loss_function(real, pred):\n","#     mask = tf.math.logical_not(tf.math.equal(real, 0))\n","#     # print (mask)\n","#     print(\"real:\", real, \"pred:\", pred)\n","#     loss_ = loss_object(real, pred)\n","#     mask = tf.cast(mask, dtype=loss_.dtype)\n","#     # print (loss_)\n","#     loss_ *= mask\n","#     # print(loss_.shape)\n","#     return tf.reduce_mean(loss_)*0.01\n","\n","# optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00001)\n","\n","def start_train():\n","    global train_dataset, val_dataset\n","    # gc.collect()\n","\n","    EPOCHS = 100\n","    # train_dataset, val_dataset = prepareds(imgrootpath, 2)\n","\n","    # num_steps = len(train_dataset) // BATCH_SIZE\n","    num_steps = 1\n","    start_epoch = 0\n","    # if ckpt_manager.latest_checkpoint:\n","    #   start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n","    #   # restoring the latest checkpoint in checkpoint_path\n","    #   ckpt.restore(ckpt_manager.latest_checkpoint)\n","\n","    for epoch in range(start_epoch, EPOCHS+1):\n","        start = time.time()\n","        total_loss = 0\n","        for (batch, (img_tensor, target, flist)) in enumerate(train_dataset):\n","            # print(batch, (img_tensor, target, flist))\n","            # print(batch)\n","            fname1= [int(x.numpy().split(b\"/\")[0]) for x in flist]\n","            # fname1 = np.array(fname1, dtype=np.float32)\n","            fname1 = tf.convert_to_tensor(fname1, dtype=tf.float32)\n","            batch_loss, t_loss = train_step(img_tensor, target, fname1)\n","            # raise (\"123\")\n","            total_loss += t_loss\n","            if batch % 100 == 0:\n","                print('Epoch {} Batch {} Loss {:.4f}'.format(\n","                    epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n","        # storing the epoch end loss value to plot later\n","        print(\"total_loss, num_steps:\", total_loss, num_steps)\n","        loss_plot.append(total_loss / num_steps)\n","        if epoch % 5 == 0:\n","            ckpt_manager.save()\n","            # model_text.save(\"model_text\")\n","            if epoch != 0 and epoch % 10 == 0:\n","                train_dataset, val_dataset = prepareds(imgrootpath, 2)\n","        print('Epoch {} Loss {:.6f}'.format(epoch + 1, total_loss / num_steps))\n","        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n","        # print(\"testonvalimage:\", testonvalimage(),testonvalimage(),testonvalimage(),testonvalimage(),testonvalimage(),testonvalimage())\n","        ra = testontrainimage()[1]\n","        # rb = testontrainimage()[1]\n","        # rc = testontrainimage()[1]\n","        # rd = max(ra,rb,rc)\n","        print(\"train accuracy:\", ra)\n","        ra = testonvalimage()[1]\n","        rb = testonvalimage()[1]\n","        rc = testonvalimage()[1]\n","        rd = max(ra,rb,rc)\n","        accu_plot.append(rd)\n","        print(\"validation accuracy:\", rd)\n","\n","    # epochs = range(1, len(acc) + 1)\n","\n","    # “bo”代表 \"蓝点\"\n","    plt.plot(loss_plot, 'b', label='训练损失')\n","    # plt.plot(accu_plot, 'b', label='验证准确率')\n","    # b代表“蓝色实线”\n","    # plt.plot(epochs, val_loss, 'b', label='验证损失')\n","    plt.title('训练损失')\n","    plt.xlabel('迭代次数')\n","    plt.ylabel('损失')\n","    plt.legend()\n","    plt.show()\n","\n","    # “bo”代表 \"蓝点\"\n","    # plt.plot(loss_plot, 'b', label='训练损失')\n","    plt.plot(accu_plot, 'g.', label='验证准确率')\n","    # b代表“蓝色实线”\n","    # plt.plot(epochs, val_loss, 'b', label='验证损失')\n","    plt.title('验证准确率')\n","    plt.xlabel('迭代次数')\n","    plt.ylabel('准确率')\n","    plt.legend()\n","    plt.show()\n","\n","start_train()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 3/3072 [00:00<01:47, 28.43it/s]"],"name":"stderr"},{"output_type":"stream","text":["len(img_keys): 3840\n","start get feature of train data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3072/3072 [01:39<00:00, 30.89it/s]\n","  0%|          | 3/768 [00:00<00:26, 29.09it/s]"],"name":"stderr"},{"output_type":"stream","text":["start get feature of val data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 768/768 [00:25<00:00, 29.86it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["len(img_name_train), len(cap_train), len(img_name_val), len(cap_val):\n","3072 3072 768 768\n","all image feature have done \n","start change item2 to txt feature\n","Epoch 1 Batch 0 Loss 0.4859\n","total_loss, num_steps: tf.Tensor(25.462288, shape=(), dtype=float32) 1\n","Epoch 1 Loss 25.462288\n","Time taken for 1 epoch 38.41401672363281 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.3006 - accuracy: 0.9674\n","train accuracy: 0.9674479365348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.7160 - accuracy: 0.9297\n","24/24 [==============================] - 1s 25ms/step - loss: 0.7092 - accuracy: 0.9375\n","24/24 [==============================] - 1s 25ms/step - loss: 0.5936 - accuracy: 0.9362\n","validation accuracy: 0.9375\n","Epoch 2 Batch 0 Loss 0.4838\n","total_loss, num_steps: tf.Tensor(21.387323, shape=(), dtype=float32) 1\n","Epoch 2 Loss 21.387323\n","Time taken for 1 epoch 38.08797860145569 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0616 - accuracy: 0.9906\n","train accuracy: 0.9905598759651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3461 - accuracy: 0.9622\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3640 - accuracy: 0.9570\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4540 - accuracy: 0.9544\n","validation accuracy: 0.9622395634651184\n","Epoch 3 Batch 0 Loss 0.3838\n","total_loss, num_steps: tf.Tensor(20.006985, shape=(), dtype=float32) 1\n","Epoch 3 Loss 20.006985\n","Time taken for 1 epoch 37.878036975860596 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0425 - accuracy: 0.9945\n","train accuracy: 0.9944661259651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3627 - accuracy: 0.9661\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3900 - accuracy: 0.9544\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4151 - accuracy: 0.9557\n","validation accuracy: 0.9661458134651184\n","Epoch 4 Batch 0 Loss 0.4017\n","total_loss, num_steps: tf.Tensor(19.284578, shape=(), dtype=float32) 1\n","Epoch 4 Loss 19.284578\n","Time taken for 1 epoch 37.97336459159851 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0268 - accuracy: 0.9964\n","train accuracy: 0.9964192509651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.5073 - accuracy: 0.9544\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3953 - accuracy: 0.9609\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2968 - accuracy: 0.9674\n","validation accuracy: 0.9674479365348816\n","Epoch 5 Batch 0 Loss 0.3874\n","total_loss, num_steps: tf.Tensor(18.845428, shape=(), dtype=float32) 1\n","Epoch 5 Loss 18.845428\n","Time taken for 1 epoch 37.94287323951721 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0279 - accuracy: 0.9948\n","train accuracy: 0.9947916865348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4814 - accuracy: 0.9570\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3934 - accuracy: 0.9609\n","24/24 [==============================] - 1s 25ms/step - loss: 0.5006 - accuracy: 0.9531\n","validation accuracy: 0.9609375\n","Epoch 6 Batch 0 Loss 0.3826\n","total_loss, num_steps: tf.Tensor(18.441172, shape=(), dtype=float32) 1\n","Epoch 6 Loss 18.441172\n","Time taken for 1 epoch 38.3944354057312 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0250 - accuracy: 0.9951\n","train accuracy: 0.9951171875\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3921 - accuracy: 0.9557\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4837 - accuracy: 0.9492\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3349 - accuracy: 0.9622\n","validation accuracy: 0.9622395634651184\n","Epoch 7 Batch 0 Loss 0.3840\n","total_loss, num_steps: tf.Tensor(18.170002, shape=(), dtype=float32) 1\n","Epoch 7 Loss 18.170002\n","Time taken for 1 epoch 37.9313850402832 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0392 - accuracy: 0.9935\n","train accuracy: 0.9934895634651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4682 - accuracy: 0.9518\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3312 - accuracy: 0.9661\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4473 - accuracy: 0.9492\n","validation accuracy: 0.9661458134651184\n","Epoch 8 Batch 0 Loss 0.3811\n","total_loss, num_steps: tf.Tensor(17.85926, shape=(), dtype=float32) 1\n","Epoch 8 Loss 17.859261\n","Time taken for 1 epoch 38.118603467941284 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0191 - accuracy: 0.9964\n","train accuracy: 0.9964192509651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3260 - accuracy: 0.9622\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4307 - accuracy: 0.9505\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3969 - accuracy: 0.9492\n","validation accuracy: 0.9622395634651184\n","Epoch 9 Batch 0 Loss 0.3497\n","total_loss, num_steps: tf.Tensor(17.719154, shape=(), dtype=float32) 1\n","Epoch 9 Loss 17.719154\n","Time taken for 1 epoch 38.00011944770813 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0256 - accuracy: 0.9958\n","train accuracy: 0.9957682490348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2945 - accuracy: 0.9661\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4143 - accuracy: 0.9596\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4250 - accuracy: 0.9544\n","validation accuracy: 0.9661458134651184\n","Epoch 10 Batch 0 Loss 0.3620\n","total_loss, num_steps: tf.Tensor(17.446035, shape=(), dtype=float32) 1\n","Epoch 10 Loss 17.446035\n","Time taken for 1 epoch 38.04765009880066 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0172 - accuracy: 0.9964\n","train accuracy: 0.9964192509651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4469 - accuracy: 0.9544\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3345 - accuracy: 0.9596\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4523 - accuracy: 0.9596\n","validation accuracy: 0.9596354365348816\n","Epoch 11 Batch 0 Loss 0.3601\n","total_loss, num_steps: tf.Tensor(17.218838, shape=(), dtype=float32) 1\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 3/3072 [00:00<01:54, 26.73it/s]"],"name":"stderr"},{"output_type":"stream","text":["len(img_keys): 3840\n","start get feature of train data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3072/3072 [01:40<00:00, 30.70it/s]\n","  0%|          | 3/768 [00:00<00:26, 28.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["start get feature of val data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 768/768 [00:25<00:00, 30.43it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["len(img_name_train), len(cap_train), len(img_name_val), len(cap_val):\n","3072 3072 768 768\n","all image feature have done \n","start change item2 to txt feature\n","Epoch 11 Loss 17.218838\n","Time taken for 1 epoch 164.15731739997864 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.2415 - accuracy: 0.9749\n","train accuracy: 0.9749348759651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4311 - accuracy: 0.9596\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4854 - accuracy: 0.9531\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3907 - accuracy: 0.9622\n","validation accuracy: 0.9622395634651184\n","Epoch 12 Batch 0 Loss 0.4533\n","total_loss, num_steps: tf.Tensor(22.618898, shape=(), dtype=float32) 1\n","Epoch 12 Loss 22.618898\n","Time taken for 1 epoch 38.06480884552002 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.3147 - accuracy: 0.9697\n","train accuracy: 0.9697265625\n","24/24 [==============================] - 1s 25ms/step - loss: 0.6187 - accuracy: 0.9427\n","24/24 [==============================] - 1s 25ms/step - loss: 0.7491 - accuracy: 0.9336\n","24/24 [==============================] - 1s 24ms/step - loss: 0.7044 - accuracy: 0.9362\n","validation accuracy: 0.9427083134651184\n","Epoch 13 Batch 0 Loss 0.4362\n","total_loss, num_steps: tf.Tensor(20.285244, shape=(), dtype=float32) 1\n","Epoch 13 Loss 20.285244\n","Time taken for 1 epoch 38.06641888618469 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0757 - accuracy: 0.9893\n","train accuracy: 0.9892578125\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3108 - accuracy: 0.9661\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3945 - accuracy: 0.9583\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3273 - accuracy: 0.9557\n","validation accuracy: 0.9661458134651184\n","Epoch 14 Batch 0 Loss 0.4176\n","total_loss, num_steps: tf.Tensor(19.170233, shape=(), dtype=float32) 1\n","Epoch 14 Loss 19.170233\n","Time taken for 1 epoch 37.889800786972046 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0340 - accuracy: 0.9932\n","train accuracy: 0.9931640625\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3050 - accuracy: 0.9622\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2478 - accuracy: 0.9648\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3007 - accuracy: 0.9661\n","validation accuracy: 0.9661458134651184\n","Epoch 15 Batch 0 Loss 0.3955\n","total_loss, num_steps: tf.Tensor(18.53234, shape=(), dtype=float32) 1\n","Epoch 15 Loss 18.532339\n","Time taken for 1 epoch 38.02890944480896 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0278 - accuracy: 0.9964\n","train accuracy: 0.9964192509651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3004 - accuracy: 0.9661\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3043 - accuracy: 0.9661\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3710 - accuracy: 0.9544\n","validation accuracy: 0.9661458134651184\n","Epoch 16 Batch 0 Loss 0.3821\n","total_loss, num_steps: tf.Tensor(18.185974, shape=(), dtype=float32) 1\n","Epoch 16 Loss 18.185974\n","Time taken for 1 epoch 38.50374221801758 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0368 - accuracy: 0.9948\n","train accuracy: 0.9947916865348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2323 - accuracy: 0.9688\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1991 - accuracy: 0.9740\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3388 - accuracy: 0.9583\n","validation accuracy: 0.9739583134651184\n","Epoch 17 Batch 0 Loss 0.3669\n","total_loss, num_steps: tf.Tensor(17.897547, shape=(), dtype=float32) 1\n","Epoch 17 Loss 17.897547\n","Time taken for 1 epoch 38.137646198272705 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0154 - accuracy: 0.9967\n","train accuracy: 0.9967448115348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2472 - accuracy: 0.9701\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3689 - accuracy: 0.9674\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3134 - accuracy: 0.9635\n","validation accuracy: 0.9700520634651184\n","Epoch 18 Batch 0 Loss 0.3659\n","total_loss, num_steps: tf.Tensor(17.571909, shape=(), dtype=float32) 1\n","Epoch 18 Loss 17.571909\n","Time taken for 1 epoch 37.989675760269165 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0331 - accuracy: 0.9938\n","train accuracy: 0.9938151240348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1903 - accuracy: 0.9766\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3055 - accuracy: 0.9714\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4141 - accuracy: 0.9596\n","validation accuracy: 0.9765625\n","Epoch 19 Batch 0 Loss 0.3556\n","total_loss, num_steps: tf.Tensor(17.27636, shape=(), dtype=float32) 1\n","Epoch 19 Loss 17.276360\n","Time taken for 1 epoch 38.04729652404785 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0315 - accuracy: 0.9938\n","train accuracy: 0.9938151240348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2043 - accuracy: 0.9674\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4309 - accuracy: 0.9531\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3021 - accuracy: 0.9661\n","validation accuracy: 0.9674479365348816\n","Epoch 20 Batch 0 Loss 0.3732\n","total_loss, num_steps: tf.Tensor(16.998629, shape=(), dtype=float32) 1\n","Epoch 20 Loss 16.998629\n","Time taken for 1 epoch 38.00080680847168 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0282 - accuracy: 0.9928\n","train accuracy: 0.9928385615348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4416 - accuracy: 0.9531\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3083 - accuracy: 0.9570\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2286 - accuracy: 0.9635\n","validation accuracy: 0.9635416865348816\n","Epoch 21 Batch 0 Loss 0.3408\n","total_loss, num_steps: tf.Tensor(16.747185, shape=(), dtype=float32) 1\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 3/3072 [00:00<02:06, 24.29it/s]"],"name":"stderr"},{"output_type":"stream","text":["len(img_keys): 3840\n","start get feature of train data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3072/3072 [01:40<00:00, 30.51it/s]\n","  0%|          | 3/768 [00:00<00:28, 27.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["start get feature of val data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 768/768 [00:25<00:00, 29.73it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["len(img_name_train), len(cap_train), len(img_name_val), len(cap_val):\n","3072 3072 768 768\n","all image feature have done \n","start change item2 to txt feature\n","Epoch 21 Loss 16.747185\n","Time taken for 1 epoch 165.30370616912842 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.2314 - accuracy: 0.9710\n","train accuracy: 0.9710286259651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3164 - accuracy: 0.9674\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2384 - accuracy: 0.9714\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2821 - accuracy: 0.9714\n","validation accuracy: 0.9713541865348816\n","Epoch 22 Batch 0 Loss 0.4349\n","total_loss, num_steps: tf.Tensor(22.071062, shape=(), dtype=float32) 1\n","Epoch 22 Loss 22.071062\n","Time taken for 1 epoch 37.95425081253052 sec\n","\n","96/96 [==============================] - 2s 24ms/step - loss: 0.1244 - accuracy: 0.9847\n","train accuracy: 0.9847005009651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4430 - accuracy: 0.9570\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4152 - accuracy: 0.9622\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2954 - accuracy: 0.9609\n","validation accuracy: 0.9622395634651184\n","Epoch 23 Batch 0 Loss 0.4209\n","total_loss, num_steps: tf.Tensor(19.761517, shape=(), dtype=float32) 1\n","Epoch 23 Loss 19.761517\n","Time taken for 1 epoch 38.0401828289032 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0325 - accuracy: 0.9941\n","train accuracy: 0.994140625\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2500 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2580 - accuracy: 0.9714\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2182 - accuracy: 0.9766\n","validation accuracy: 0.9765625\n","Epoch 24 Batch 0 Loss 0.3891\n","total_loss, num_steps: tf.Tensor(18.773369, shape=(), dtype=float32) 1\n","Epoch 24 Loss 18.773369\n","Time taken for 1 epoch 37.870338439941406 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0383 - accuracy: 0.9928\n","train accuracy: 0.9928385615348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3348 - accuracy: 0.9648\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3041 - accuracy: 0.9688\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3288 - accuracy: 0.9648\n","validation accuracy: 0.96875\n","Epoch 25 Batch 0 Loss 0.3744\n","total_loss, num_steps: tf.Tensor(18.300356, shape=(), dtype=float32) 1\n","Epoch 25 Loss 18.300356\n","Time taken for 1 epoch 37.91520929336548 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0306 - accuracy: 0.9948\n","train accuracy: 0.9947916865348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2582 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2216 - accuracy: 0.9740\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2853 - accuracy: 0.9740\n","validation accuracy: 0.9752604365348816\n","Epoch 26 Batch 0 Loss 0.3631\n","total_loss, num_steps: tf.Tensor(17.91367, shape=(), dtype=float32) 1\n","Epoch 26 Loss 17.913670\n","Time taken for 1 epoch 38.26041507720947 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0279 - accuracy: 0.9961\n","train accuracy: 0.99609375\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2217 - accuracy: 0.9779\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2673 - accuracy: 0.9714\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2527 - accuracy: 0.9727\n","validation accuracy: 0.9778645634651184\n","Epoch 27 Batch 0 Loss 0.3631\n","total_loss, num_steps: tf.Tensor(17.574884, shape=(), dtype=float32) 1\n","Epoch 27 Loss 17.574884\n","Time taken for 1 epoch 37.95157432556152 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0198 - accuracy: 0.9951\n","train accuracy: 0.9951171875\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1738 - accuracy: 0.9818\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2211 - accuracy: 0.9766\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1943 - accuracy: 0.9792\n","validation accuracy: 0.9817708134651184\n","Epoch 28 Batch 0 Loss 0.3812\n","total_loss, num_steps: tf.Tensor(17.221363, shape=(), dtype=float32) 1\n","Epoch 28 Loss 17.221363\n","Time taken for 1 epoch 38.033570766448975 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0303 - accuracy: 0.9961\n","train accuracy: 0.99609375\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2592 - accuracy: 0.9766\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2429 - accuracy: 0.9727\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2265 - accuracy: 0.9792\n","validation accuracy: 0.9791666865348816\n","Epoch 29 Batch 0 Loss 0.3582\n","total_loss, num_steps: tf.Tensor(16.838829, shape=(), dtype=float32) 1\n","Epoch 29 Loss 16.838829\n","Time taken for 1 epoch 37.94725227355957 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0311 - accuracy: 0.9948\n","train accuracy: 0.9947916865348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2203 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3120 - accuracy: 0.9701\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3188 - accuracy: 0.9753\n","validation accuracy: 0.9752604365348816\n","Epoch 30 Batch 0 Loss 0.3353\n","total_loss, num_steps: tf.Tensor(16.609673, shape=(), dtype=float32) 1\n","Epoch 30 Loss 16.609673\n","Time taken for 1 epoch 38.042375802993774 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0349 - accuracy: 0.9948\n","train accuracy: 0.9947916865348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2457 - accuracy: 0.9714\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1711 - accuracy: 0.9805\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2126 - accuracy: 0.9766\n","validation accuracy: 0.98046875\n","Epoch 31 Batch 0 Loss 0.3458\n","total_loss, num_steps: tf.Tensor(16.349634, shape=(), dtype=float32) 1\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 3/3072 [00:00<01:55, 26.69it/s]"],"name":"stderr"},{"output_type":"stream","text":["len(img_keys): 3840\n","start get feature of train data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3072/3072 [01:42<00:00, 29.90it/s]\n","  0%|          | 3/768 [00:00<00:29, 25.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["start get feature of val data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 768/768 [00:25<00:00, 30.62it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["len(img_name_train), len(cap_train), len(img_name_val), len(cap_val):\n","3072 3072 768 768\n","all image feature have done \n","start change item2 to txt feature\n","Epoch 31 Loss 16.349634\n","Time taken for 1 epoch 166.50245070457458 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.2300 - accuracy: 0.9769\n","train accuracy: 0.9768880009651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3045 - accuracy: 0.9714\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2757 - accuracy: 0.9727\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3043 - accuracy: 0.9674\n","validation accuracy: 0.97265625\n","Epoch 32 Batch 0 Loss 0.4380\n","total_loss, num_steps: tf.Tensor(21.671711, shape=(), dtype=float32) 1\n","Epoch 32 Loss 21.671711\n","Time taken for 1 epoch 37.926268339157104 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.1144 - accuracy: 0.9844\n","train accuracy: 0.984375\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3114 - accuracy: 0.9688\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3793 - accuracy: 0.9583\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3749 - accuracy: 0.9661\n","validation accuracy: 0.96875\n","Epoch 33 Batch 0 Loss 0.4238\n","total_loss, num_steps: tf.Tensor(19.550499, shape=(), dtype=float32) 1\n","Epoch 33 Loss 19.550499\n","Time taken for 1 epoch 37.946407318115234 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0539 - accuracy: 0.9932\n","train accuracy: 0.9931640625\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3104 - accuracy: 0.9727\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2169 - accuracy: 0.9766\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2198 - accuracy: 0.9818\n","validation accuracy: 0.9817708134651184\n","Epoch 34 Batch 0 Loss 0.3848\n","total_loss, num_steps: tf.Tensor(18.621952, shape=(), dtype=float32) 1\n","Epoch 34 Loss 18.621952\n","Time taken for 1 epoch 37.97995114326477 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0159 - accuracy: 0.9971\n","train accuracy: 0.9970703125\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2151 - accuracy: 0.9792\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1268 - accuracy: 0.9831\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1475 - accuracy: 0.9792\n","validation accuracy: 0.9830729365348816\n","Epoch 35 Batch 0 Loss 0.3813\n","total_loss, num_steps: tf.Tensor(18.115276, shape=(), dtype=float32) 1\n","Epoch 35 Loss 18.115276\n","Time taken for 1 epoch 37.85084319114685 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0112 - accuracy: 0.9980\n","train accuracy: 0.998046875\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1832 - accuracy: 0.9844\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1196 - accuracy: 0.9857\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1529 - accuracy: 0.9844\n","validation accuracy: 0.9856770634651184\n","Epoch 36 Batch 0 Loss 0.3832\n","total_loss, num_steps: tf.Tensor(17.711704, shape=(), dtype=float32) 1\n","Epoch 36 Loss 17.711704\n","Time taken for 1 epoch 38.304930686950684 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0270 - accuracy: 0.9961\n","train accuracy: 0.99609375\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2131 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2924 - accuracy: 0.9740\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1925 - accuracy: 0.9805\n","validation accuracy: 0.98046875\n","Epoch 37 Batch 0 Loss 0.3537\n","total_loss, num_steps: tf.Tensor(17.339077, shape=(), dtype=float32) 1\n","Epoch 37 Loss 17.339077\n","Time taken for 1 epoch 38.03443479537964 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0404 - accuracy: 0.9948\n","train accuracy: 0.9947916865348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1808 - accuracy: 0.9792\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1636 - accuracy: 0.9805\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1775 - accuracy: 0.9831\n","validation accuracy: 0.9830729365348816\n","Epoch 38 Batch 0 Loss 0.3562\n","total_loss, num_steps: tf.Tensor(16.959105, shape=(), dtype=float32) 1\n","Epoch 38 Loss 16.959105\n","Time taken for 1 epoch 37.86378312110901 sec\n","\n","96/96 [==============================] - 2s 24ms/step - loss: 0.0258 - accuracy: 0.9958\n","train accuracy: 0.9957682490348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1484 - accuracy: 0.9818\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1895 - accuracy: 0.9766\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1919 - accuracy: 0.9766\n","validation accuracy: 0.9817708134651184\n","Epoch 39 Batch 0 Loss 0.3324\n","total_loss, num_steps: tf.Tensor(16.672962, shape=(), dtype=float32) 1\n","Epoch 39 Loss 16.672962\n","Time taken for 1 epoch 37.991825342178345 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0175 - accuracy: 0.9954\n","train accuracy: 0.9954426884651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2883 - accuracy: 0.9661\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2173 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1871 - accuracy: 0.9740\n","validation accuracy: 0.9752604365348816\n","Epoch 40 Batch 0 Loss 0.3299\n","total_loss, num_steps: tf.Tensor(16.453253, shape=(), dtype=float32) 1\n","Epoch 40 Loss 16.453253\n","Time taken for 1 epoch 37.94960570335388 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0248 - accuracy: 0.9945\n","train accuracy: 0.9944661259651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2201 - accuracy: 0.9727\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2574 - accuracy: 0.9688\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2760 - accuracy: 0.9779\n","validation accuracy: 0.9778645634651184\n","Epoch 41 Batch 0 Loss 0.3367\n","total_loss, num_steps: tf.Tensor(16.104708, shape=(), dtype=float32) 1\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 4/3072 [00:00<01:39, 30.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["len(img_keys): 3840\n","start get feature of train data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3072/3072 [01:39<00:00, 30.99it/s]\n","  0%|          | 3/768 [00:00<00:28, 26.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["start get feature of val data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 768/768 [00:25<00:00, 30.10it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["len(img_name_train), len(cap_train), len(img_name_val), len(cap_val):\n","3072 3072 768 768\n","all image feature have done \n","start change item2 to txt feature\n","Epoch 41 Loss 16.104708\n","Time taken for 1 epoch 172.6296124458313 sec\n","\n","96/96 [==============================] - 2s 24ms/step - loss: 0.1350 - accuracy: 0.9831\n","train accuracy: 0.9830729365348816\n","24/24 [==============================] - 1s 24ms/step - loss: 0.3399 - accuracy: 0.9674\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3177 - accuracy: 0.9714\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3014 - accuracy: 0.9753\n","validation accuracy: 0.9752604365348816\n","Epoch 42 Batch 0 Loss 0.4423\n","total_loss, num_steps: tf.Tensor(21.231907, shape=(), dtype=float32) 1\n","Epoch 42 Loss 21.231907\n","Time taken for 1 epoch 37.94659399986267 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0569 - accuracy: 0.9902\n","train accuracy: 0.990234375\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4260 - accuracy: 0.9648\n","24/24 [==============================] - 1s 25ms/step - loss: 0.5512 - accuracy: 0.9622\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4761 - accuracy: 0.9570\n","validation accuracy: 0.96484375\n","Epoch 43 Batch 0 Loss 0.4087\n","total_loss, num_steps: tf.Tensor(19.475967, shape=(), dtype=float32) 1\n","Epoch 43 Loss 19.475967\n","Time taken for 1 epoch 37.879640102386475 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0439 - accuracy: 0.9948\n","train accuracy: 0.9947916865348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2227 - accuracy: 0.9792\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2475 - accuracy: 0.9727\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3338 - accuracy: 0.9648\n","validation accuracy: 0.9791666865348816\n","Epoch 44 Batch 0 Loss 0.3867\n","total_loss, num_steps: tf.Tensor(18.52801, shape=(), dtype=float32) 1\n","Epoch 44 Loss 18.528009\n","Time taken for 1 epoch 38.01388430595398 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0193 - accuracy: 0.9967\n","train accuracy: 0.9967448115348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2149 - accuracy: 0.9727\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2783 - accuracy: 0.9727\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2094 - accuracy: 0.9805\n","validation accuracy: 0.98046875\n","Epoch 45 Batch 0 Loss 0.3924\n","total_loss, num_steps: tf.Tensor(18.018204, shape=(), dtype=float32) 1\n","Epoch 45 Loss 18.018204\n","Time taken for 1 epoch 37.876978158950806 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0305 - accuracy: 0.9954\n","train accuracy: 0.9954426884651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3138 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2432 - accuracy: 0.9740\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2272 - accuracy: 0.9766\n","validation accuracy: 0.9765625\n","Epoch 46 Batch 0 Loss 0.3687\n","total_loss, num_steps: tf.Tensor(17.64409, shape=(), dtype=float32) 1\n","Epoch 46 Loss 17.644091\n","Time taken for 1 epoch 38.40121841430664 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0366 - accuracy: 0.9958\n","train accuracy: 0.9957682490348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2444 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2438 - accuracy: 0.9740\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1967 - accuracy: 0.9792\n","validation accuracy: 0.9791666865348816\n","Epoch 47 Batch 0 Loss 0.3390\n","total_loss, num_steps: tf.Tensor(17.22416, shape=(), dtype=float32) 1\n","Epoch 47 Loss 17.224159\n","Time taken for 1 epoch 37.944371938705444 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0281 - accuracy: 0.9964\n","train accuracy: 0.9964192509651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2417 - accuracy: 0.9766\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2248 - accuracy: 0.9805\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2048 - accuracy: 0.9766\n","validation accuracy: 0.98046875\n","Epoch 48 Batch 0 Loss 0.3519\n","total_loss, num_steps: tf.Tensor(16.824167, shape=(), dtype=float32) 1\n","Epoch 48 Loss 16.824167\n","Time taken for 1 epoch 37.88737416267395 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0251 - accuracy: 0.9958\n","train accuracy: 0.9957682490348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2615 - accuracy: 0.9701\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2495 - accuracy: 0.9779\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3914 - accuracy: 0.9753\n","validation accuracy: 0.9778645634651184\n","Epoch 49 Batch 0 Loss 0.3379\n","total_loss, num_steps: tf.Tensor(16.411795, shape=(), dtype=float32) 1\n","Epoch 49 Loss 16.411795\n","Time taken for 1 epoch 37.87696599960327 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0421 - accuracy: 0.9925\n","train accuracy: 0.9925130009651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2407 - accuracy: 0.9740\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3259 - accuracy: 0.9661\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2094 - accuracy: 0.9818\n","validation accuracy: 0.9817708134651184\n","Epoch 50 Batch 0 Loss 0.3355\n","total_loss, num_steps: tf.Tensor(16.24159, shape=(), dtype=float32) 1\n","Epoch 50 Loss 16.241590\n","Time taken for 1 epoch 37.93512320518494 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0273 - accuracy: 0.9938\n","train accuracy: 0.9938151240348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2744 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2693 - accuracy: 0.9727\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2963 - accuracy: 0.9622\n","validation accuracy: 0.9752604365348816\n","Epoch 51 Batch 0 Loss 0.3301\n","total_loss, num_steps: tf.Tensor(16.012413, shape=(), dtype=float32) 1\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 3/3072 [00:00<01:54, 26.89it/s]"],"name":"stderr"},{"output_type":"stream","text":["len(img_keys): 3840\n","start get feature of train data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3072/3072 [01:41<00:00, 30.18it/s]\n","  0%|          | 3/768 [00:00<00:26, 28.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["start get feature of val data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 768/768 [00:24<00:00, 30.78it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["len(img_name_train), len(cap_train), len(img_name_val), len(cap_val):\n","3072 3072 768 768\n","all image feature have done \n","start change item2 to txt feature\n","Epoch 51 Loss 16.012413\n","Time taken for 1 epoch 165.40313005447388 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.1657 - accuracy: 0.9779\n","train accuracy: 0.9778645634651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2954 - accuracy: 0.9714\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2600 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2264 - accuracy: 0.9779\n","validation accuracy: 0.9778645634651184\n","Epoch 52 Batch 0 Loss 0.4371\n","total_loss, num_steps: tf.Tensor(21.932116, shape=(), dtype=float32) 1\n","Epoch 52 Loss 21.932116\n","Time taken for 1 epoch 38.126230001449585 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0901 - accuracy: 0.9893\n","train accuracy: 0.9892578125\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3050 - accuracy: 0.9661\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3524 - accuracy: 0.9622\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2301 - accuracy: 0.9727\n","validation accuracy: 0.97265625\n","Epoch 53 Batch 0 Loss 0.4197\n","total_loss, num_steps: tf.Tensor(19.56803, shape=(), dtype=float32) 1\n","Epoch 53 Loss 19.568029\n","Time taken for 1 epoch 37.91618824005127 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0362 - accuracy: 0.9938\n","train accuracy: 0.9938151240348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2798 - accuracy: 0.9701\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2555 - accuracy: 0.9779\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2239 - accuracy: 0.9714\n","validation accuracy: 0.9778645634651184\n","Epoch 54 Batch 0 Loss 0.3896\n","total_loss, num_steps: tf.Tensor(18.49997, shape=(), dtype=float32) 1\n","Epoch 54 Loss 18.499969\n","Time taken for 1 epoch 37.9650297164917 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0253 - accuracy: 0.9954\n","train accuracy: 0.9954426884651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2796 - accuracy: 0.9740\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2735 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2859 - accuracy: 0.9766\n","validation accuracy: 0.9765625\n","Epoch 55 Batch 0 Loss 0.3824\n","total_loss, num_steps: tf.Tensor(18.03401, shape=(), dtype=float32) 1\n","Epoch 55 Loss 18.034010\n","Time taken for 1 epoch 37.905720472335815 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0310 - accuracy: 0.9948\n","train accuracy: 0.9947916865348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2775 - accuracy: 0.9714\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1931 - accuracy: 0.9792\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2197 - accuracy: 0.9740\n","validation accuracy: 0.9791666865348816\n","Epoch 56 Batch 0 Loss 0.3690\n","total_loss, num_steps: tf.Tensor(17.54442, shape=(), dtype=float32) 1\n","Epoch 56 Loss 17.544420\n","Time taken for 1 epoch 38.422077655792236 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0454 - accuracy: 0.9932\n","train accuracy: 0.9931640625\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2449 - accuracy: 0.9818\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1939 - accuracy: 0.9831\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2632 - accuracy: 0.9779\n","validation accuracy: 0.9830729365348816\n","Epoch 57 Batch 0 Loss 0.3469\n","total_loss, num_steps: tf.Tensor(17.06268, shape=(), dtype=float32) 1\n","Epoch 57 Loss 17.062679\n","Time taken for 1 epoch 37.90874004364014 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0309 - accuracy: 0.9935\n","train accuracy: 0.9934895634651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2005 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1144 - accuracy: 0.9857\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2235 - accuracy: 0.9779\n","validation accuracy: 0.9856770634651184\n","Epoch 58 Batch 0 Loss 0.3464\n","total_loss, num_steps: tf.Tensor(16.647526, shape=(), dtype=float32) 1\n","Epoch 58 Loss 16.647526\n","Time taken for 1 epoch 37.94628143310547 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0367 - accuracy: 0.9941\n","train accuracy: 0.994140625\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1498 - accuracy: 0.9831\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2102 - accuracy: 0.9714\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2062 - accuracy: 0.9753\n","validation accuracy: 0.9830729365348816\n","Epoch 59 Batch 0 Loss 0.3450\n","total_loss, num_steps: tf.Tensor(16.23701, shape=(), dtype=float32) 1\n","Epoch 59 Loss 16.237009\n","Time taken for 1 epoch 37.932737588882446 sec\n","\n","96/96 [==============================] - 2s 24ms/step - loss: 0.0206 - accuracy: 0.9961\n","train accuracy: 0.99609375\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1835 - accuracy: 0.9831\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2259 - accuracy: 0.9818\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1558 - accuracy: 0.9753\n","validation accuracy: 0.9830729365348816\n","Epoch 60 Batch 0 Loss 0.3274\n","total_loss, num_steps: tf.Tensor(15.932396, shape=(), dtype=float32) 1\n","Epoch 60 Loss 15.932396\n","Time taken for 1 epoch 37.95858716964722 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0430 - accuracy: 0.9932\n","train accuracy: 0.9931640625\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2758 - accuracy: 0.9740\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1763 - accuracy: 0.9831\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1796 - accuracy: 0.9753\n","validation accuracy: 0.9830729365348816\n","Epoch 61 Batch 0 Loss 0.3290\n","total_loss, num_steps: tf.Tensor(15.65968, shape=(), dtype=float32) 1\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 3/3072 [00:00<01:56, 26.33it/s]"],"name":"stderr"},{"output_type":"stream","text":["len(img_keys): 3840\n","start get feature of train data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3072/3072 [01:40<00:00, 30.54it/s]\n","  0%|          | 3/768 [00:00<00:27, 27.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["start get feature of val data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 768/768 [00:25<00:00, 30.17it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["len(img_name_train), len(cap_train), len(img_name_val), len(cap_val):\n","3072 3072 768 768\n","all image feature have done \n","start change item2 to txt feature\n","Epoch 61 Loss 15.659680\n","Time taken for 1 epoch 164.9394462108612 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.1887 - accuracy: 0.9801\n","train accuracy: 0.9801432490348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2193 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2387 - accuracy: 0.9766\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2677 - accuracy: 0.9701\n","validation accuracy: 0.9765625\n","Epoch 62 Batch 0 Loss 0.4550\n","total_loss, num_steps: tf.Tensor(21.439949, shape=(), dtype=float32) 1\n","Epoch 62 Loss 21.439949\n","Time taken for 1 epoch 38.01770305633545 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0895 - accuracy: 0.9873\n","train accuracy: 0.9873046875\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4010 - accuracy: 0.9531\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3358 - accuracy: 0.9583\n","24/24 [==============================] - 1s 24ms/step - loss: 0.3425 - accuracy: 0.9557\n","validation accuracy: 0.9583333134651184\n","Epoch 63 Batch 0 Loss 0.4183\n","total_loss, num_steps: tf.Tensor(19.255003, shape=(), dtype=float32) 1\n","Epoch 63 Loss 19.255003\n","Time taken for 1 epoch 38.009756326675415 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0269 - accuracy: 0.9958\n","train accuracy: 0.9957682490348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2641 - accuracy: 0.9792\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2046 - accuracy: 0.9766\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3124 - accuracy: 0.9753\n","validation accuracy: 0.9791666865348816\n","Epoch 64 Batch 0 Loss 0.3880\n","total_loss, num_steps: tf.Tensor(18.441303, shape=(), dtype=float32) 1\n","Epoch 64 Loss 18.441303\n","Time taken for 1 epoch 37.96361184120178 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0198 - accuracy: 0.9977\n","train accuracy: 0.9977213740348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2710 - accuracy: 0.9766\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1861 - accuracy: 0.9818\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2219 - accuracy: 0.9766\n","validation accuracy: 0.9817708134651184\n","Epoch 65 Batch 0 Loss 0.3706\n","total_loss, num_steps: tf.Tensor(17.829468, shape=(), dtype=float32) 1\n","Epoch 65 Loss 17.829468\n","Time taken for 1 epoch 38.10318470001221 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0325 - accuracy: 0.9948\n","train accuracy: 0.9947916865348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2011 - accuracy: 0.9805\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2613 - accuracy: 0.9714\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1898 - accuracy: 0.9779\n","validation accuracy: 0.98046875\n","Epoch 66 Batch 0 Loss 0.3575\n","total_loss, num_steps: tf.Tensor(17.38725, shape=(), dtype=float32) 1\n","Epoch 66 Loss 17.387251\n","Time taken for 1 epoch 38.32258343696594 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0303 - accuracy: 0.9961\n","train accuracy: 0.99609375\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2266 - accuracy: 0.9805\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3085 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1683 - accuracy: 0.9818\n","validation accuracy: 0.9817708134651184\n","Epoch 67 Batch 0 Loss 0.3702\n","total_loss, num_steps: tf.Tensor(16.960283, shape=(), dtype=float32) 1\n","Epoch 67 Loss 16.960283\n","Time taken for 1 epoch 37.94539475440979 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0197 - accuracy: 0.9961\n","train accuracy: 0.99609375\n","24/24 [==============================] - 1s 24ms/step - loss: 0.2311 - accuracy: 0.9779\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1710 - accuracy: 0.9844\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2217 - accuracy: 0.9766\n","validation accuracy: 0.984375\n","Epoch 68 Batch 0 Loss 0.3320\n","total_loss, num_steps: tf.Tensor(16.523947, shape=(), dtype=float32) 1\n","Epoch 68 Loss 16.523947\n","Time taken for 1 epoch 38.023444414138794 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0244 - accuracy: 0.9961\n","train accuracy: 0.99609375\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1949 - accuracy: 0.9792\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2179 - accuracy: 0.9792\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2018 - accuracy: 0.9792\n","validation accuracy: 0.9791666865348816\n","Epoch 69 Batch 0 Loss 0.3394\n","total_loss, num_steps: tf.Tensor(16.14326, shape=(), dtype=float32) 1\n","Epoch 69 Loss 16.143259\n","Time taken for 1 epoch 38.107441663742065 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0559 - accuracy: 0.9902\n","train accuracy: 0.990234375\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3822 - accuracy: 0.9740\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1634 - accuracy: 0.9792\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2081 - accuracy: 0.9779\n","validation accuracy: 0.9791666865348816\n","Epoch 70 Batch 0 Loss 0.3450\n","total_loss, num_steps: tf.Tensor(15.817562, shape=(), dtype=float32) 1\n","Epoch 70 Loss 15.817562\n","Time taken for 1 epoch 37.97753715515137 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0309 - accuracy: 0.9967\n","train accuracy: 0.9967448115348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2571 - accuracy: 0.9727\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1446 - accuracy: 0.9805\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2479 - accuracy: 0.9766\n","validation accuracy: 0.98046875\n","Epoch 71 Batch 0 Loss 0.3145\n","total_loss, num_steps: tf.Tensor(15.454537, shape=(), dtype=float32) 1\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 3/3072 [00:00<02:00, 25.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["len(img_keys): 3840\n","start get feature of train data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3072/3072 [01:42<00:00, 30.07it/s]\n","  0%|          | 3/768 [00:00<00:30, 25.06it/s]"],"name":"stderr"},{"output_type":"stream","text":["start get feature of val data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 768/768 [00:25<00:00, 29.57it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["len(img_name_train), len(cap_train), len(img_name_val), len(cap_val):\n","3072 3072 768 768\n","all image feature have done \n","start change item2 to txt feature\n","Epoch 71 Loss 15.454537\n","Time taken for 1 epoch 166.85309886932373 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.1331 - accuracy: 0.9827\n","train accuracy: 0.9827473759651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3094 - accuracy: 0.9701\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2952 - accuracy: 0.9701\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2411 - accuracy: 0.9766\n","validation accuracy: 0.9765625\n","Epoch 72 Batch 0 Loss 0.4491\n","total_loss, num_steps: tf.Tensor(21.373531, shape=(), dtype=float32) 1\n","Epoch 72 Loss 21.373531\n","Time taken for 1 epoch 38.00836968421936 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.1538 - accuracy: 0.9808\n","train accuracy: 0.9807942509651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.4088 - accuracy: 0.9609\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3228 - accuracy: 0.9622\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3085 - accuracy: 0.9635\n","validation accuracy: 0.9635416865348816\n","Epoch 73 Batch 0 Loss 0.4035\n","total_loss, num_steps: tf.Tensor(19.3274, shape=(), dtype=float32) 1\n","Epoch 73 Loss 19.327400\n","Time taken for 1 epoch 37.964707374572754 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0324 - accuracy: 0.9954\n","train accuracy: 0.9954426884651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3358 - accuracy: 0.9674\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3064 - accuracy: 0.9766\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3047 - accuracy: 0.9779\n","validation accuracy: 0.9778645634651184\n","Epoch 74 Batch 0 Loss 0.3999\n","total_loss, num_steps: tf.Tensor(18.420576, shape=(), dtype=float32) 1\n","Epoch 74 Loss 18.420576\n","Time taken for 1 epoch 38.000876903533936 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0328 - accuracy: 0.9951\n","train accuracy: 0.9951171875\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2044 - accuracy: 0.9779\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1859 - accuracy: 0.9818\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2615 - accuracy: 0.9688\n","validation accuracy: 0.9817708134651184\n","Epoch 75 Batch 0 Loss 0.3931\n","total_loss, num_steps: tf.Tensor(17.84031, shape=(), dtype=float32) 1\n","Epoch 75 Loss 17.840309\n","Time taken for 1 epoch 37.96855592727661 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0431 - accuracy: 0.9941\n","train accuracy: 0.994140625\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2541 - accuracy: 0.9805\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3370 - accuracy: 0.9714\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2750 - accuracy: 0.9727\n","validation accuracy: 0.98046875\n","Epoch 76 Batch 0 Loss 0.3693\n","total_loss, num_steps: tf.Tensor(17.316423, shape=(), dtype=float32) 1\n","Epoch 76 Loss 17.316423\n","Time taken for 1 epoch 38.361698627471924 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0391 - accuracy: 0.9935\n","train accuracy: 0.9934895634651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2622 - accuracy: 0.9766\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2456 - accuracy: 0.9779\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2282 - accuracy: 0.9779\n","validation accuracy: 0.9778645634651184\n","Epoch 77 Batch 0 Loss 0.3647\n","total_loss, num_steps: tf.Tensor(16.78495, shape=(), dtype=float32) 1\n","Epoch 77 Loss 16.784950\n","Time taken for 1 epoch 37.86833643913269 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0334 - accuracy: 0.9941\n","train accuracy: 0.994140625\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2753 - accuracy: 0.9805\n","24/24 [==============================] - 1s 24ms/step - loss: 0.2567 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2358 - accuracy: 0.9792\n","validation accuracy: 0.98046875\n","Epoch 78 Batch 0 Loss 0.3272\n","total_loss, num_steps: tf.Tensor(16.2515, shape=(), dtype=float32) 1\n","Epoch 78 Loss 16.251499\n","Time taken for 1 epoch 37.95936822891235 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0173 - accuracy: 0.9971\n","train accuracy: 0.9970703125\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2717 - accuracy: 0.9779\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2526 - accuracy: 0.9688\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2439 - accuracy: 0.9727\n","validation accuracy: 0.9778645634651184\n","Epoch 79 Batch 0 Loss 0.3497\n","total_loss, num_steps: tf.Tensor(15.854424, shape=(), dtype=float32) 1\n","Epoch 79 Loss 15.854424\n","Time taken for 1 epoch 37.86200571060181 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0276 - accuracy: 0.9938\n","train accuracy: 0.9938151240348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2471 - accuracy: 0.9766\n","24/24 [==============================] - 1s 24ms/step - loss: 0.2152 - accuracy: 0.9792\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2241 - accuracy: 0.9740\n","validation accuracy: 0.9791666865348816\n","Epoch 80 Batch 0 Loss 0.3301\n","total_loss, num_steps: tf.Tensor(15.493478, shape=(), dtype=float32) 1\n","Epoch 80 Loss 15.493478\n","Time taken for 1 epoch 37.919849157333374 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0140 - accuracy: 0.9967\n","train accuracy: 0.9967448115348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2863 - accuracy: 0.9766\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2711 - accuracy: 0.9805\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2638 - accuracy: 0.9740\n","validation accuracy: 0.98046875\n","Epoch 81 Batch 0 Loss 0.3006\n","total_loss, num_steps: tf.Tensor(15.261242, shape=(), dtype=float32) 1\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 3/3072 [00:00<01:51, 27.47it/s]"],"name":"stderr"},{"output_type":"stream","text":["len(img_keys): 3840\n","start get feature of train data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3072/3072 [01:42<00:00, 30.03it/s]\n","  0%|          | 3/768 [00:00<00:27, 27.70it/s]"],"name":"stderr"},{"output_type":"stream","text":["start get feature of val data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 768/768 [00:24<00:00, 30.89it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["len(img_name_train), len(cap_train), len(img_name_val), len(cap_val):\n","3072 3072 768 768\n","all image feature have done \n","start change item2 to txt feature\n","Epoch 81 Loss 15.261242\n","Time taken for 1 epoch 165.80865001678467 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.1175 - accuracy: 0.9834\n","train accuracy: 0.9833984375\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2580 - accuracy: 0.9701\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1966 - accuracy: 0.9805\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2720 - accuracy: 0.9701\n","validation accuracy: 0.98046875\n","Epoch 82 Batch 0 Loss 0.4716\n","total_loss, num_steps: tf.Tensor(21.186499, shape=(), dtype=float32) 1\n","Epoch 82 Loss 21.186499\n","Time taken for 1 epoch 37.8151638507843 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0582 - accuracy: 0.9902\n","train accuracy: 0.990234375\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2403 - accuracy: 0.9688\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2126 - accuracy: 0.9727\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2166 - accuracy: 0.9753\n","validation accuracy: 0.9752604365348816\n","Epoch 83 Batch 0 Loss 0.3867\n","total_loss, num_steps: tf.Tensor(19.197525, shape=(), dtype=float32) 1\n","Epoch 83 Loss 19.197525\n","Time taken for 1 epoch 37.93209385871887 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0431 - accuracy: 0.9948\n","train accuracy: 0.9947916865348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3330 - accuracy: 0.9674\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2889 - accuracy: 0.9766\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3313 - accuracy: 0.9714\n","validation accuracy: 0.9765625\n","Epoch 84 Batch 0 Loss 0.3941\n","total_loss, num_steps: tf.Tensor(18.27424, shape=(), dtype=float32) 1\n","Epoch 84 Loss 18.274240\n","Time taken for 1 epoch 38.12285375595093 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0258 - accuracy: 0.9954\n","train accuracy: 0.9954426884651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1431 - accuracy: 0.9844\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1825 - accuracy: 0.9831\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2105 - accuracy: 0.9779\n","validation accuracy: 0.984375\n","Epoch 85 Batch 0 Loss 0.3701\n","total_loss, num_steps: tf.Tensor(17.648764, shape=(), dtype=float32) 1\n","Epoch 85 Loss 17.648764\n","Time taken for 1 epoch 38.05293798446655 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0359 - accuracy: 0.9945\n","train accuracy: 0.9944661259651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1213 - accuracy: 0.9831\n","24/24 [==============================] - 1s 24ms/step - loss: 0.1362 - accuracy: 0.9883\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1675 - accuracy: 0.9805\n","validation accuracy: 0.98828125\n","Epoch 86 Batch 0 Loss 0.3469\n","total_loss, num_steps: tf.Tensor(17.150026, shape=(), dtype=float32) 1\n","Epoch 86 Loss 17.150026\n","Time taken for 1 epoch 38.40885329246521 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0229 - accuracy: 0.9945\n","train accuracy: 0.9944661259651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1184 - accuracy: 0.9831\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2492 - accuracy: 0.9792\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1834 - accuracy: 0.9844\n","validation accuracy: 0.984375\n","Epoch 87 Batch 0 Loss 0.3567\n","total_loss, num_steps: tf.Tensor(16.627167, shape=(), dtype=float32) 1\n","Epoch 87 Loss 16.627167\n","Time taken for 1 epoch 38.023659229278564 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0443 - accuracy: 0.9935\n","train accuracy: 0.9934895634651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2425 - accuracy: 0.9779\n","24/24 [==============================] - 1s 24ms/step - loss: 0.2121 - accuracy: 0.9818\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1032 - accuracy: 0.9883\n","validation accuracy: 0.98828125\n","Epoch 88 Batch 0 Loss 0.3306\n","total_loss, num_steps: tf.Tensor(16.17083, shape=(), dtype=float32) 1\n","Epoch 88 Loss 16.170830\n","Time taken for 1 epoch 38.00965929031372 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0151 - accuracy: 0.9954\n","train accuracy: 0.9954426884651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1911 - accuracy: 0.9766\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2302 - accuracy: 0.9740\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1911 - accuracy: 0.9792\n","validation accuracy: 0.9791666865348816\n","Epoch 89 Batch 0 Loss 0.3276\n","total_loss, num_steps: tf.Tensor(15.794642, shape=(), dtype=float32) 1\n","Epoch 89 Loss 15.794642\n","Time taken for 1 epoch 38.02510213851929 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0443 - accuracy: 0.9896\n","train accuracy: 0.9895833134651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1225 - accuracy: 0.9831\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2108 - accuracy: 0.9792\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2160 - accuracy: 0.9701\n","validation accuracy: 0.9830729365348816\n","Epoch 90 Batch 0 Loss 0.3131\n","total_loss, num_steps: tf.Tensor(15.485625, shape=(), dtype=float32) 1\n","Epoch 90 Loss 15.485625\n","Time taken for 1 epoch 37.96758961677551 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0187 - accuracy: 0.9945\n","train accuracy: 0.9944661259651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1785 - accuracy: 0.9831\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2061 - accuracy: 0.9792\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2308 - accuracy: 0.9766\n","validation accuracy: 0.9830729365348816\n","Epoch 91 Batch 0 Loss 0.3135\n","total_loss, num_steps: tf.Tensor(15.222552, shape=(), dtype=float32) 1\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 3/3072 [00:00<01:44, 29.25it/s]"],"name":"stderr"},{"output_type":"stream","text":["len(img_keys): 3840\n","start get feature of train data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3072/3072 [01:40<00:00, 30.44it/s]\n","  0%|          | 3/768 [00:00<00:28, 26.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["start get feature of val data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 768/768 [00:26<00:00, 29.23it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["len(img_name_train), len(cap_train), len(img_name_val), len(cap_val):\n","3072 3072 768 768\n","all image feature have done \n","start change item2 to txt feature\n","Epoch 91 Loss 15.222552\n","Time taken for 1 epoch 165.92351508140564 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0927 - accuracy: 0.9893\n","train accuracy: 0.9892578125\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2560 - accuracy: 0.9727\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2587 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2272 - accuracy: 0.9766\n","validation accuracy: 0.9765625\n","Epoch 92 Batch 0 Loss 0.4560\n","total_loss, num_steps: tf.Tensor(21.350668, shape=(), dtype=float32) 1\n","Epoch 92 Loss 21.350668\n","Time taken for 1 epoch 38.09673595428467 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0969 - accuracy: 0.9857\n","train accuracy: 0.9856770634651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2865 - accuracy: 0.9727\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3107 - accuracy: 0.9583\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3790 - accuracy: 0.9635\n","validation accuracy: 0.97265625\n","Epoch 93 Batch 0 Loss 0.3987\n","total_loss, num_steps: tf.Tensor(19.28326, shape=(), dtype=float32) 1\n","Epoch 93 Loss 19.283260\n","Time taken for 1 epoch 37.96329474449158 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0280 - accuracy: 0.9958\n","train accuracy: 0.9957682490348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2365 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2867 - accuracy: 0.9701\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1793 - accuracy: 0.9766\n","validation accuracy: 0.9765625\n","Epoch 94 Batch 0 Loss 0.3750\n","total_loss, num_steps: tf.Tensor(18.335407, shape=(), dtype=float32) 1\n","Epoch 94 Loss 18.335407\n","Time taken for 1 epoch 38.06593370437622 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0230 - accuracy: 0.9961\n","train accuracy: 0.99609375\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2367 - accuracy: 0.9779\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2346 - accuracy: 0.9740\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2969 - accuracy: 0.9727\n","validation accuracy: 0.9778645634651184\n","Epoch 95 Batch 0 Loss 0.3784\n","total_loss, num_steps: tf.Tensor(17.725172, shape=(), dtype=float32) 1\n","Epoch 95 Loss 17.725172\n","Time taken for 1 epoch 37.9502432346344 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0243 - accuracy: 0.9945\n","train accuracy: 0.9944661259651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2490 - accuracy: 0.9766\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2496 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2567 - accuracy: 0.9766\n","validation accuracy: 0.9765625\n","Epoch 96 Batch 0 Loss 0.3472\n","total_loss, num_steps: tf.Tensor(17.231157, shape=(), dtype=float32) 1\n","Epoch 96 Loss 17.231157\n","Time taken for 1 epoch 38.38746953010559 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0480 - accuracy: 0.9948\n","train accuracy: 0.9947916865348816\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2616 - accuracy: 0.9740\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2345 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2928 - accuracy: 0.9766\n","validation accuracy: 0.9765625\n","Epoch 97 Batch 0 Loss 0.3379\n","total_loss, num_steps: tf.Tensor(16.68934, shape=(), dtype=float32) 1\n","Epoch 97 Loss 16.689341\n","Time taken for 1 epoch 38.0440719127655 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0197 - accuracy: 0.9961\n","train accuracy: 0.99609375\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2506 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2291 - accuracy: 0.9766\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3034 - accuracy: 0.9792\n","validation accuracy: 0.9791666865348816\n","Epoch 98 Batch 0 Loss 0.3333\n","total_loss, num_steps: tf.Tensor(16.192856, shape=(), dtype=float32) 1\n","Epoch 98 Loss 16.192856\n","Time taken for 1 epoch 37.84700083732605 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0272 - accuracy: 0.9945\n","train accuracy: 0.9944661259651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2147 - accuracy: 0.9766\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2644 - accuracy: 0.9792\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1850 - accuracy: 0.9766\n","validation accuracy: 0.9791666865348816\n","Epoch 99 Batch 0 Loss 0.3228\n","total_loss, num_steps: tf.Tensor(15.728955, shape=(), dtype=float32) 1\n","Epoch 99 Loss 15.728955\n","Time taken for 1 epoch 38.030439138412476 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0306 - accuracy: 0.9941\n","train accuracy: 0.994140625\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2400 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2651 - accuracy: 0.9727\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2401 - accuracy: 0.9753\n","validation accuracy: 0.9752604365348816\n","Epoch 100 Batch 0 Loss 0.3217\n","total_loss, num_steps: tf.Tensor(15.506569, shape=(), dtype=float32) 1\n","Epoch 100 Loss 15.506569\n","Time taken for 1 epoch 37.95845127105713 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.0549 - accuracy: 0.9902\n","train accuracy: 0.990234375\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3298 - accuracy: 0.9648\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2985 - accuracy: 0.9727\n","24/24 [==============================] - 1s 25ms/step - loss: 0.3336 - accuracy: 0.9688\n","validation accuracy: 0.97265625\n","Epoch 101 Batch 0 Loss 0.3249\n","total_loss, num_steps: tf.Tensor(15.364087, shape=(), dtype=float32) 1\n"],"name":"stdout"},{"output_type":"stream","text":["  0%|          | 3/3072 [00:00<01:55, 26.57it/s]"],"name":"stderr"},{"output_type":"stream","text":["len(img_keys): 3840\n","start get feature of train data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3072/3072 [01:41<00:00, 30.30it/s]\n","  0%|          | 3/768 [00:00<00:27, 28.18it/s]"],"name":"stderr"},{"output_type":"stream","text":["start get feature of val data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 768/768 [00:24<00:00, 31.12it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["len(img_name_train), len(cap_train), len(img_name_val), len(cap_val):\n","3072 3072 768 768\n","all image feature have done \n","start change item2 to txt feature\n","Epoch 101 Loss 15.364087\n","Time taken for 1 epoch 164.69212889671326 sec\n","\n","96/96 [==============================] - 2s 25ms/step - loss: 0.1371 - accuracy: 0.9857\n","train accuracy: 0.9856770634651184\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1701 - accuracy: 0.9779\n","24/24 [==============================] - 1s 25ms/step - loss: 0.2401 - accuracy: 0.9753\n","24/24 [==============================] - 1s 25ms/step - loss: 0.1781 - accuracy: 0.9779\n","validation accuracy: 0.9778645634651184\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"_9jViein6iju"},"source":["!cp /content/checkpoints /content/drive/MyDrive/datafinal/vgg19 -fr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"EUxXGBmx9GWS","executionInfo":{"elapsed":634,"status":"ok","timestamp":1627103310770,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"7f339dc3-8971-43bd-ad62-9870c14f531f"},"source":["ckpt_manager.save()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'./checkpoints/train/ckpt-25'"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"id":"G5ITEbc39GcT","executionInfo":{"elapsed":338,"status":"error","timestamp":1626606606810,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"747d2c0b-75b9-41a1-b58d-582c7afef94a"},"source":["    plt.plot(loss_plot, 'b', label='训练损失')\n","    # plt.plot(accu_plot, 'b', label='验证准确率')\n","    # b代表“蓝色实线”\n","    # plt.plot(epochs, val_loss, 'b', label='验证损失')\n","    plt.title('训练损失')\n","    plt.xlabel('迭代次数')\n","    plt.ylabel('损失')\n","    plt.legend()\n","    plt.show()\n","\n","    # “bo”代表 \"蓝点\"\n","    # plt.plot(loss_plot, 'b', label='训练损失')\n","    plt.plot(accu_plot, 'g.', label='验证准确率')\n","    # b代表“蓝色实线”\n","    # plt.plot(epochs, val_loss, 'b', label='验证损失')\n","    plt.title('验证准确率')\n","    plt.xlabel('迭代次数')\n","    plt.ylabel('准确率')\n","    plt.legend()\n","    plt.show()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c92238d43ae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'训练损失'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# plt.plot(accu_plot, 'b', label='验证准确率')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# b代表“蓝色实线”\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# plt.plot(epochs, val_loss, 'b', label='验证损失')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'训练损失'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"]}]},{"cell_type":"code","metadata":{"id":"rsnH5jUm9E9V"},"source":["!rm /content/checkpoints -fr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dSDint-E2AOX"},"source":["!rm /content/drive/MyDrive/data/model_last_inceptionV3_0718_4 -fr\n","!cp /content/checkpoints /content/drive/MyDrive/data/model_last_inceptionV3_0718_4 -fr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NENVZ3Yodkun","executionInfo":{"elapsed":2360,"status":"ok","timestamp":1626316027785,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"a86d1fc5-ee2d-4260-d0d3-e3f62353816d"},"source":["# 这个是为了统计样本的平衡性：\n","fname1_list = []\n","for (batch, (imgs, ls, fl)) in enumerate(val_dataset):\n","    fname1= [int(tf.strings.split(x,b\"/\")[0]) for x in fl]\n","    fname1 = tf.convert_to_tensor(fname1, dtype=tf.float32)\n","    fname1_list.append(fname1)\n","fname1_list = tf.convert_to_tensor(fname1_list, dtype=tf.int32)\n","print(fname1_list.shape)\n","fname1_list = tf.reshape(fname1_list, (-1,))\n","print(fname1_list.shape)\n","print(fname1_list.numpy())\n","fname1_list = list(fname1_list.numpy())\n","for x in list(set(fname1_list)):\n","    print (x, fname1_list.count(x))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(6, 64)\n","(384,)\n","[17  8 19 30  9  4  9 30 12  4 20 15  2 28 19 21 31  7 29 25 13 14 16 25\n","  2 18 21 29  0  8  3 18 15 14 31 20 16 12  3  6  2  4 14 10 17  3  3  2\n"," 20 14 28 21 16 20  2 31 23 16 29 24  2 19 29 18 26 25 23  5 14 17 18 19\n"," 16 12  0 30 21 20 23 22 10  2  2 10  0 25 24 26 30  2  0  7 12 23 23 27\n","  3 19  8 30 28 20 15 10 12 15 21  0  6 21 23  6 12 21 24 14  6  9 29 19\n"," 29 22 31 30 25 17 16 23 25 19 29 25  6  1 11 12 14 12 25 29  4 10  6 22\n","  1  0 23 15 30 17  1 28 18 19 25 18  8 14 31 25  3 11  0 16 27 15  5  0\n","  9 14 21 11 10 12  6 12 19 27 20  1 16 12 11  5 13 17  5 18 21  4 10 21\n"," 29 11 25 21 22 21  1  7  4 23  8 19 21 20  9 23  4 20 22 22  3  0 28 16\n"," 14  4 10 14 15 13  5 27 15 14 14 22  3 16 23 23 11 10 24  1 29 29 20 21\n"," 16 29 22 19  2 25  4  4 16 31  1 17  0 10 29 14  9 12 27 11 19 11  6 27\n"," 12 19 23  5 14  4  8  0 19 26 25 30  0 21 12 23 21  7 15  3  4 13  9  3\n"," 28 22  5  5 31  8  9 15 14 31 30 22  0 21  7 20 22 23  9  6  1 20 24 17\n"," 28 30  4 10  6  5  3 28  3 26  9  1  0  0 12 11 18 21 26 23  3 16  5 10\n"," 16 11 29 23  8  1  4 30 26 15 19  8 13 18  8  4  2 28 15  5 24 29 11 28\n"," 16 29 13 28 31  0 11 29  8 28 10  5 31 30 19 10 18 23 17 24 17 16  7 23]\n","0 16\n","1 10\n","2 11\n","3 13\n","4 15\n","5 12\n","6 10\n","7 6\n","8 11\n","9 10\n","10 14\n","11 12\n","12 15\n","13 6\n","14 16\n","15 12\n","16 16\n","17 10\n","18 10\n","19 16\n","20 12\n","21 18\n","22 11\n","23 19\n","24 7\n","25 13\n","26 6\n","27 6\n","28 12\n","29 17\n","30 12\n","31 10\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4DxcvrRqnL7_"},"source":["!rm /content/Leaves2/01   -fr\n","!rm /content/Leaves2/02   -fr\n","!rm /content/Leaves2/03   -fr\n","!rm /content/Leaves2/04   -fr\n","!rm /content/Leaves2/05   -fr\n","!rm /content/Leaves2/06   -fr\n","!rm /content/Leaves2/07   -fr\n","!rm /content/Leaves2/08   -fr\n","!rm /content/Leaves2/09   -fr\n","!rm /content/Leaves2/10   -fr\n","!rm /content/Leaves2/11   -fr\n","!rm /content/Leaves2/12   -fr\n","!rm /content/Leaves2/14   -fr\n","!rm /content/Leaves2/15   -fr\n","!rm /content/Leaves2/16   -fr\n","!rm /content/Leaves2/17   -fr\n","!rm /content/Leaves2/18   -fr\n","!rm /content/Leaves2/20   -fr\n","!rm /content/Leaves2/21   -fr\n","!rm /content/Leaves2/22   -fr\n","!rm /content/Leaves2/23   -fr\n","!rm /content/Leaves2/24   -fr\n","!rm /content/Leaves2/25   -fr\n","!rm /content/Leaves2/26   -fr\n","!rm /content/Leaves2/27   -fr\n","!rm /content/Leaves2/28   -fr\n","!rm /content/Leaves2/29   -fr\n","!rm /content/Leaves2/30   -fr\n","!rm /content/Leaves2/31   -fr\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aqA_hINVosGw"},"source":["train_dataset, val_dataset = prepareds(imgrootpath)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KVPS2EZjrhvQ","executionInfo":{"elapsed":383,"status":"ok","timestamp":1626244572251,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"2d1bf062-1c46-4248-95d6-428880c4a8e8"},"source":["for x in wb_d.keys():\n","    print (x, wb_d[x])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["14.txt ['大叶蜡梅（广西桂林）图版3', '140.1925.落叶灌木', '高达4米', '鳞芽通常着生于第二年生的枝条叶腋内', '芽鳞片近圆形', '覆瓦状排列', '外面被短柔毛', '叶纸质至近革质', '卵圆形、椭圆形、宽椭圆形至卵状椭圆形', '有时长圆状披针形', '长5-25厘米', '宽2-8厘米', '顶端急尖至渐尖', '有时具尾尖', '基部急尖至圆形', '除叶背脉上被疏微毛外无毛', '花着生于第二年生枝条叶腋内', '先花后叶', '芳香', '直径2-4厘米']\n","7.txt ['叶薄革质', '倒卵状阔披针形或长圆状倒披针形', '长8-18厘米', '宽(2)3.5-6(10)厘米', '先端渐尖或短尖', '基部楔形', '不下延', '上面无毛或沿中脉有毛', '老时完全无毛', '下面被黄褐色短柔毛', '中脉粗壮', '上面下陷', '侧脉每边6-8（10）条', '弧形', '在边缘网结并渐消失', '横脉及小脉在下面联结成明显的网状']\n","29.txt ['19．阔叶十大功劳图版47', '叶狭倒卵形至长圆形', '长27-51厘米', '宽10-20厘米', '具4-10对小叶', '最下一对小叶距叶柄基部0.5-2.5厘米', '上面暗灰绿色', '背面被白霜', '有时淡黄绿色或苍白色', '两面叶脉不显', '叶轴粗2-4毫米', '节间长3-10厘米', '小叶厚革质', '硬直', '自叶下部往上小叶渐次变长而狭', '最下一对小叶卵形', '长1.2-3.5厘米', '宽1-2厘米', '具1-2粗锯齿', '往上小叶近圆形至卵形或长圆形', '长2-10.5厘米', '宽2-6厘米', '基部阔楔形或圆形', '偏斜', '有时心形', '边缘每边具2-6粗锯齿', '先端具硬尖', '顶生小叶较大', '长7-13厘米', '宽3.5-10厘米', '具柄', '长1-6厘米']\n","5.txt ['小叶4-6对', '对生', '倒卵状长圆形或倒卵形', '长1.5-3厘米']\n","19.txt ['短枝密被叶痕', '黑灰色', '短枝上亦可长出长枝', '叶扇形', '有长柄', '淡绿色', '无毛', '有多数叉状并列细脉', '顶端宽5-8厘米', '在短枝上常具波状缺刻', '在长枝上常2裂', '幼树及萌生枝上的叶常较而深裂(叶片长达13厘米', '有时裂片再分裂(这与较原始的化石种类之叶相似)', '在短枝上3-8叶呈簇生状', '秋季落叶前变为黄色.球花雌雄异株', '生于短枝顶端的鳞片状叶的腋内']\n","6.txt ['158)1845.落叶小乔木', '叶纸质', '外貌圆形', '直径7-10厘米', '基部心脏形或近于心脏形稀截形', '5-9掌状分裂', '通常7裂', '裂片长圆卵形或披针形', '先端锐尖或长锐尖', '边缘具紧贴的尖锐锯齿', '裂片间的凹缺钝尖或锐尖', '深达叶片的直径的1/2或1/3', '下面淡绿色', '在叶脉的脉腋被有白色丛毛', '叶柄长4-6厘米', '细瘦', '无毛', '花紫色', '杂性', '雄花与两性花同株', '生于无毛的伞房花序', '总花梗长2-3厘米', '叶发出以后才开花']\n","18.txt ['叶在长枝上辐射伸展', '短枝之叶成簇生状（每年生出新叶约15-20枚）', '针形', '坚硬', '淡绿色或深绿色', '长2.5-5厘米', '宽1-1.5毫米', '上部较宽', '先端锐尖', '下部渐窄', '常成三棱形', '稀背脊明显', '叶之腹面两侧各有2-3条气孔线', '背面4-6条', '幼时气孔线有白粉']\n","25.txt ['小叶16-20', '对生或互生', '纸质', '卵状披针形或卵状长椭圆形', '长9-15厘米', '宽2.5-4厘米', '先端尾尖', '基部一侧圆形', '另一侧楔形', '不对称', '边全缘或有疏离的小锯齿', '两面均无毛', '无斑点', '背面常呈粉绿色', '侧脉每边18-24条', '平展', '与中脉几成直角开出', '背面略凸起', '圆锥花序与叶等长或更长', '被稀疏的锈色短柔毛或有时近无毛', '小聚伞花序生于短的小枝上', '多花']\n","17.txt ['叶片革质', '椭圆形、长椭圆形或椭圆状披针形', '长7-14.5厘米', '宽2.6-4.5厘米', '先端渐尖', '基部渐狭呈楔形或宽楔形', '全缘或通常上半部具细锯齿', '两面无毛', '腺点在两面连成小水泡状突起', '中脉在上面凹入', '下面凸起', '侧脉6-8对', '多达10对', '在上面凹入', '下面凸起', '叶柄长0.8-1.2厘米', '最长可达15厘米', '无毛', '聚伞花序簇生于叶腋', '或近于帚状', '每腋内有花多朵', '.其极大部分花序簇生于叶腋', '而在原始文献中', '', '的描述“花序簇生于叶腋”']\n","20.txt ['972.图3674.1972.落叶灌木或小乔木', '高可达7米', '叶互生或有时对生', '纸质', '椭圆形、阔矩圆形或倒卵形', '长2.5-7厘米', '宽1.5-4厘米', '顶端短尖或钝形', '有时微凹', '基部阔楔形或近圆形', '无毛或下面沿中脉有微柔毛', '侧脉3-7对', '小脉不明显', '树皮、叶及花为强泻剂']\n","32.txt ['叶马褂状', '长4-12（18）厘米', '近基部每边具1侧裂片', '先端具2浅裂', '下面苍白色', '叶柄长4-8（-16）厘米', '叶和树皮入药']\n","11.txt ['叶在长枝上互生', '在短枝上为1-4片簇生', '叶片纸质至坚纸质', '卵形、卵状椭圆形', '稀长圆状椭圆形', '长4-13(-15)厘米', '宽（3-）4-6厘米', '先端渐尖至短渐尖', '基部圆形或钝', '边缘具细锯齿', '叶面深绿色', '背面浅绿色', '两面无毛', '或叶面幼时疏被短的微柔毛', '主脉在叶面平或下陷', '疏被细小微柔毛或无毛', '背面隆起', '无毛或有时疏被细小微柔毛', '侧脉8-10对', '在叶面平坦或稍凸起', '在背面凸起', '于叶缘附近网结', '网状脉两面明显', '叶柄长1-1.2厘米', '上面具狭沟', '疏被细小微柔毛', '托叶胼胝质', '很小', '不明显', '单花或2-5花的聚伞花序', '单生或簇生于当年生或二年生枝的叶腋内', '或生于短枝的鳞片腋内或叶腋内', '总花梗长2-3毫米', '花梗长3-7毫米', '均无毛', '单生于叶腋或鳞片腋内', '花梗长6-18毫米', '无毛', '基部具2枚卵状小苞片', '本种的主要特点为小枝和花序无毛', '果柄与叶柄近等长或稍长于叶柄', '区别于它的各变种']\n","10.txt ['1.栾树（正字通）木栾（救荒本草）、栾华（植物名实图考）', '五乌拉叶（甘肃）', '乌拉（河北）', '乌拉胶', '黑色叶树（河北）', '石栾树（浙江）', '黑叶树、木栏牙（河南）图版', '小枝具疣点', '与叶轴、叶柄均被皱曲的短柔毛或无毛', '小叶(7-)11-18片(顶生小叶有时与最上部的一对小叶在中部以下合生)', '无柄或具极短的柄', '对生或互生', '纸质', '卵形、阔卵形至卵状披针形', '长(3-)5-10厘米', '宽3-6厘米', '顶端短尖或短渐尖', '基部钝至近截形', '边缘有不规则的钝锯齿', '齿端具小尖头', '有时近基部的齿疏离呈缺刻状', '或羽状深裂达中肋而形成二回羽状复叶', '上面仅中脉上散生皱曲的短柔毛', '下面在脉腋具髯毛', '有时小叶背面被茸毛']\n","24.txt ['15.女贞（神农本草经）青蜡树（江苏）', '大叶蜡树（江西）', '白蜡树（广西）', '蜡树（湖南）', '叶片常绿', '革质', '卵形、长卵形或椭圆形至宽椭圆形', '长6-17厘米', '宽3-8厘米', '先端锐尖至渐尖或钝', '基部圆形或近圆形', '有时宽楔形或渐狭', '叶缘平坦', '上面光亮', '两面无毛', '中脉在上面凹入', '下面凸起', '侧脉4-9对', '两面稍凸起或有时不明显', '叶柄长1-3厘米', '上面具沟', '无毛', '花序基部苞片常与叶同型', '小苞片披针形或线形', '长0.5-6厘米', '宽0.2-1.5厘米', '凋落']\n","15.txt ['枝、叶及木材均有樟脑气味', '叶互生', '卵状椭圆形', '长6-12厘米', '宽2.5-5.5厘米', '先端急尖', '基部宽楔形至近圆形', '边缘全缘', '软骨质', '有时呈微波状', '上面绿色或黄绿色', '有光泽', '下面黄绿色或灰绿色', '晦暗', '两面无毛或下面幼时略被微柔毛', '具离基三出脉', '有时过渡到基部具不显的5脉', '中脉两面明显', '上部每边有侧脉1-3-5(7)条.基生侧脉向叶缘一侧有少数支脉', '侧脉及支脉脉腋上面明显隆起下面有明显腺窝', '窝内常被柔毛', '叶柄纤细', '长2-3厘米', '腹凹背凸', '无毛', '木材及根、枝、叶可提取樟脑和樟油', '樟脑和樟油供医药及香料工业用', '从其樟油化学成分看', '可分三个类型', '即本樟（含樟脑为主）', '芳樟（含芳樟醇为主）和油樟（含松油醇为主）', '各个类型的经济价值不尽相同', '为结合生产应进行细分', '可依据樟树形态上的微细差异再结合枝、叶和木材的气味加以鉴别', '本樟树皮桃红', '裂片较大', '树身较矮.枝桠敞开而茂密', '占空间面积较大', '叶柄发红', '叶身较薄', '叶两面黄绿色', '出叶较迟', '枝、叶或木材嗅之有强烈的樟脑气味', '木髓带红', '将木片放入口中咀嚼后有苦涩味感觉', '这可证明有大量樟脑的存在', ')则树皮黄色', '质薄', '裂片少而浅', '树身较高', '枝桠直上', '分枝较疏', '叶柄绿色', '叶身厚', '叶背面灰白色', '出叶较早', '枝、叶或木材均有清香的芳樟醇气味', '.与樟相近', '但不同在于花及花序被微柔毛', '叶较狭而短', '芽鳞少数而小', '但根据我们对樟树的野外观察结果', '樟树常可根据花及花序无毛至近无毛或被灰白至黄褐色微柔毛分出两个类型', '此两个类型在同一生长地域内同时并存', '此外叶形、叶脉以及芽鳞的情况在同一植株中亦多有变异', '故我们认为', '归入黄樟是错误的', '后者的圆锥花序无毛', '叶下面侧脉脉腋腺窝通常不明显']\n","3.txt ['1986.落叶灌木', '高1-2米', '叶薄纸质', '近圆形或宽椭圆形', '叶片长2-6厘米', '宽1.5-3厘米', '先端圆钝', '基部楔形', '下延', '上面深绿色', '中脉和侧脉隆起', '背面淡绿色', '中脉和侧脉明显隆起', '两面网脉显著', '无毛', '叶缘平展', '每边具15-40刺齿']\n","23.txt ['叶片卵状椭圆形或倒卵椭圆形', '长5-9厘米', '宽2.5-5厘米', '先端渐尖', '基部圆形', '边有渐尖单锯齿及重锯齿', '齿尖有小腺体', '上面深绿色', '无毛', '下面淡绿色', '无毛', '有侧脉6-8对', '叶柄长1-1.5厘米', '无毛', '先端有1-3圆形腺体']\n","31.txt ['60.加杨（北方通称）加拿大杨（中国高等植物图鉴）、欧美杨、加拿大白杨、美国大叶白杨（中国树木分类学）图版22', '叶三角形或三角状卵形', '长7-10厘米', '长枝和萌枝叶较大', '长10-20厘米', '一般长大于宽', '先端渐尖', '基部截形或宽楔形', '无或有1-2腺体', '边缘半透明', '有圆锯齿', '近基部较疏', '具短缘毛', '上面暗绿色', '下面淡绿色', '叶柄侧扁而长', '带红色（苗期特明显）']\n","22.txt ['叶螺旋状着生', '条状披针形', '微弯', '长7-12厘米', '宽7-10毫米', '先端尖', '基部楔形', '上面深绿色', '有光泽', '中脉显著隆起', '下面带白色、灰绿色或淡绿色', '雌球花单生叶腋', '有梗', '基部有少数苞片']\n","30.txt ['小枝、芽、叶下面', '叶柄、均密被褐色或灰褐色短绒毛（幼树的叶下面无毛）', '叶厚革质', '椭圆形', '长圆状椭圆形或倒卵状椭圆形', '长10-20厘米', '宽4-7（10）厘米', '先端钝或短钝尖', '基部楔形', '叶面深绿色', '有光泽']\n","26.txt ['冬芽圆锥形', '顶端钝', '外被短柔毛', '常2-3个簇生', '中间为叶芽', '两侧为花芽', '叶片长圆披针形、椭圆披针形或倒卵状披针形', '长7-15厘米', '宽2-3.5厘米', '先端渐尖', '基部宽楔形', '上面无毛', '下面在脉腋间具少数短柔毛或无毛', '叶边具细锯齿或粗锯齿', '齿端具腺体或无腺体', '叶柄粗壮', '长1-2厘米', '常具1至数枚腺体', '有时无腺体', '花单生', '先于叶开放', '直径2.5-3.5厘米']\n","9.txt ['21.天竺桂（开宝本草）大叶天竺桂、竺香（浙江）', '山肉桂、土肉桂（台湾）、土桂、山玉桂（福建）图版52', '叶近对生或在枝条上部者互生', '卵圆状长圆形至长圆状披针形', '长7-10厘米', '宽3-3.5厘米', '先端锐尖至渐尖', '基部宽楔形或钝形', '革质', '上面绿色', '光亮', '下面灰绿色', '晦暗', '两面无毛', '离基三出脉', '中脉直贯叶端', '在叶片上部有少数支脉', '基生侧脉自叶基1-1.5厘米处斜向生出', '向叶缘一侧有少数支脉', '有时自叶基处生出一对稍为明显隆起的附加支脉', '中脉及侧脉两面隆起', '细脉在上面密集而呈明显的网结状但在下面呈细小的网孔', '叶柄粗壮', '腹凹背凸', '红褐色', '无毛', '枝叶及树皮可提取芳香油', '供制各种香精及香料的原料']\n","2.txt ['1.七叶树（河北习见树木图说）图版83', '33.1960.落叶乔木', '高达25米', '树皮深褐色或灰褐色', '小枝、圆柱形', '黄褐色或灰褐色', '无毛或嫩时有微柔毛', '有圆形或椭圆形淡黄色的皮孔', '掌状复叶', '由5-7小组成', '叶柄长10-12厘米', '有灰色微柔毛', '小叶纸质', '长圆披针形至长圆倒披针形', '稀长椭圆形钾先端短锐尖', '基部楔形或阔楔形', '边缘有钝尖形的细锯齿', '长8-16厘米', '宽3-5厘米', '上面深绿色', '无毛', '下面除中肋及侧脉的基部嫩时有疏柔毛外', '其余部分无毛', '中央小叶的小叶柄长1-1.8厘米', '两侧的小叶柄长5-10毫米', '有灰色微柔毛']\n","33.txt ['单身复叶', '翼叶通常狭窄', '或仅有痕迹', '叶片披针形', '椭圆形或阔卵形', '大小变异较大', '顶端常有凹口', '中脉由基部至凹口附近成叉状分枝', '叶缘至少上半段通常有钝或圆裂齿', '很少全缘', '种子或多或少数', '稀无籽', '通常卵形', '顶部狭尖', '基部浑圆', '子叶深绿、淡绿或间有近于乳白色', '合点紫色', '多胚', '少有单胚']\n","4.txt ['叶纸质', '近圆形或三角状圆形', '长5-10厘米', '宽与长相若或略短于长', '先端急尖', '基部浅至深心形', '两面通常无毛', '嫩叶绿色', '仅叶柄略带紫色', '叶缘膜质透明', '新鲜时明显可见']\n","12.txt ['叶聚生于枝顶', '二年生', '革质', '嫩时上下两面有柔毛', '以后变秃净', '倒卵形或倒卵状披针形', '长4-9厘米', '宽1.5-4厘米', '上面深绿色', '发亮、干后暗晦无光', '先端圆形或钝', '常微凹入或为微心形', '基部窄楔形', '侧脉6-8对', '在靠近边缘处相结合', '有时因侧脉间的支脉较明显而呈多脉状', '网脉稍明显', '网眼细小', '全缘', '干后反卷', '叶柄长达2厘米', '叶革质', '倒卵形', '先端圆', '簇生于枝顶呈假轮生状', '经长期栽培', '雄蕊常表现退化而不育', '结实率亦低']\n","27.txt ['叶革质、狭倒卵形、狭椭圆状倒卵形', '或倒披针形', '长8-17厘米', '宽2.5-5.5厘米', '先端短急尖', '通常尖头钝', '基部楔形', '沿叶柄稍下延', '边缘稍内卷', '下面疏生红褐色短毛', '叶柄长1-3厘米', '基部稍膨大', '托叶痕半椭圆形', '长3-4毫米', '本种与海南木莲极相似', '但叶质地比较厚', '干后两面叶脉不明显', '花柄有褐色毛', '心皮有长约1毫米的缘']\n","8.txt ['37.1935.落叶乔木', '高约10米', '最高可达30米', '胸径达70厘米以上', '树皮暗灰棕色', '叶片纸质', '在长枝上互生', '在短枝上簇生', '圆形或近圆形', '直径9-25厘米', '稀达35厘米', '掌状5-7浅裂', '裂片阔三角状卵形至长圆状卵形', '长不及全叶片的1/2', '茁壮枝上的叶片分裂较深', '裂片长超过全叶片的1/2', '先端渐尖', '基部心形', '上面深绿色', '无毛或几无毛', '下面淡绿色', '幼时疏生短柔毛', '边缘有细锯齿', '放射状主脉5-7条', '两面均明显', '叶柄细长', '长8-50厘米', '无毛', '树皮及叶含鞣酸', '可提制栲胶', '种子可榨油', '供工业用', '叶形多变化', '有时浅裂', '裂片阔三角状卵形', '有时分裂较深', '裂片长圆状卵形', '稀倒卵状长圆形', '长不及全叶片的1/2', '茁壮枝上的叶片', '分裂更深', '往往超过全叶片长的1/2']\n","28.txt ['叶纸质', '基部近于圆形或楔形', '外貌椭圆形或倒卵形', '长6-10厘米', '通常浅3裂', '裂片向前延伸', '稀全缘', '中央裂片三角卵形', '急尖、锐尖或短渐尖', '上面深绿色', '下面黄绿色或淡绿色', '被白粉', '略被毛', '在叶脉上较密', '初生脉3条', '稀基部叶脉也发育良好', '致成5条', '在上面不显著', '在下面显著', '叶柄长2.5-5厘米', '淡紫绿色', '细瘦', '无毛', '花多数常成顶生被短柔毛的伞房花序', '直径约3厘米', '总花梗长1.5-2厘米', '开花在叶长大以后']\n","16.txt ['图1156.1959.叶倒卵状矩圆形至矩圆形', '很少倒卵形', '长7-13(-16)厘米', '顶端钝或急狭而钝头', '基部宽楔形', '边缘常有较规则的波状浅钝锯齿', '侧脉6-8对', '圆锥花序通常生于具两对叶的幼技顶', '长9-15厘米', '直径8-13厘米']\n","21.txt ['叶3-4枚轮生', '下枝为对生', '窄披针形', '顶端急尖', '基部楔形', '叶缘反卷', '长11-15厘米', '宽2-2.5厘米', '叶面深绿', '无毛', '叶背浅绿色', '有多数洼点', '幼时被疏微毛', '老时毛渐脱落', '中脉在叶面陷入', '在叶背凸起', '侧脉两面扁平', '纤细', '密生而平行', '每边达120条', '直达叶缘', '叶柄扁平', '基部稍宽', '长5-8毫米', '幼时被微毛', '老时毛脱落', '叶柄内具腺体', '叶、树皮、根、花、种子均含有多种配醣体', '毒性极强', '人、畜误食能致死']\n","1.txt ['末级小枝具2-4叶', '叶耳不明显', '鞘口繸毛存在而为脱落性', '叶片较小较薄', '披针形', '长4-11厘米', '宽0.5-1.2厘米', '下表面在沿中脉基部具柔毛', '次脉3-6对', '再次脉9条', '花枝穗状', '长5-7厘米', '基部托以4-6片逐渐稍较大的微小鳞片状苞片', '有时花枝下方尚有1-3片近于正常发达的叶', '当此时则花枝呈顶生状', '佛焰苞通常在10片以上', '常偏于一侧', '呈整齐的复瓦状排列', '下部数片不孕而早落', '致使花枝下部露出而类似花枝之柄', '上部的边缘生纤毛及微毛', '无叶耳', '具易落的鞘口繸毛', '缩小叶小', '披针形至锥状', '每片孕性佛焰苞内具1-3枚假小穗', '颖1片', '长15-28毫米', '顶端常具锥状缩小叶有如佛焰苞', '下部、上部以及边缘常生毛茸']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fH06U4-tWmKm","executionInfo":{"elapsed":36289,"status":"ok","timestamp":1626338989034,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"2f6bfebc-2f3b-4352-bd82-823588840d6b"},"source":["train_dataset1, val_dataset1 = prepareds(imgrootpath)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 1/307 [00:00<00:50,  6.02it/s]"],"name":"stderr"},{"output_type":"stream","text":["start get feature of train data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 307/307 [00:29<00:00, 10.57it/s]\n","  3%|▎         | 2/77 [00:00<00:07, 10.42it/s]"],"name":"stderr"},{"output_type":"stream","text":["start get feature of val data\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 77/77 [00:07<00:00, 10.73it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["len(img_name_train), len(cap_train), len(img_name_val), len(cap_val):\n","307 307 77 77\n","all image feature have done \n","start change item2 to txt feature\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mt4oNncHCoUl"},"source":["!cp /content/Leaves3 /content/Leaves2 -fr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vfUj7zEaC67t"},"source":["!rm /content/model_text -fr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"id":"JPawJG-KibJG","executionInfo":{"elapsed":23592,"status":"ok","timestamp":1626332908568,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"1fa6bdc5-f0cf-4556-cd61-e6757131d185"},"source":["#@title 样本平衡性测试\n","from sklearn.metrics import classification_report\n","\n","# train_dataset1, val_dataset1 = prepareds(imgrootpath)\n","\n","# @tf.function\n","def testonvalimage1():\n","    fname1_list = []\n","    result_list = []\n","    # for imgs, ls, fl in tqdm(val_dataset,ncols=80):\n","    # for imgs, ls, fl in val_dataset:\n","    # for (batch, (imgs, ls, fl)) in enumerate(val_dataset):\n","    # for (batch, (imgs, ls, fl)) in enumerate(train_dataset):\n","    for (batch, (imgs, ls, fl)) in enumerate(train_dataset):\n","        # print (imgs, ls, fl)\n","        # raise (\"123\")\n","        if len(imgs) != 64:\n","            break\n","        \n","        # for x in fl:\n","        #     print(tf.strings.split(x,b\"/\"))\n","            # int(x.numpy().split(b\"/\")[0]\n","        # fname1= [int(x.split(b\"/\")[0]) for x in fl]\n","        fname1= [int(tf.strings.split(x,b\"/\")[0]) for x in fl]\n","        # fname1 = np.array(fname1, dtype=np.float32)\n","        fname1 = tf.convert_to_tensor(fname1, dtype=tf.float32)\n","        # print (\"fname1:\", fname1)\n","\n","        # batch_loss, t_loss = train_step(img_tensor, target, fname1)\n","        # print(imgs, ls, fl)\n","        # print(imgs.shape)\n","        # print(ls.shape)\n","\n","        # rid = np.random.randint(0, len(imgs))\n","        # print(rid)\n","        # image = imgs[rid]\n","        # print(imgs[rid])\n","        # print(ls[rid])\n","        # print(fl[rid])\n","        # print(\"image.shape\", image.shape)\n","        # print(image,ls[rid])\n","        # real_caption = ' '.join([tokenizer.index_word[i] for i in np.array(ls[rid]) if i not in [0]])\n","        # print(real_caption)\n","        result = []\n","        # for _ in range(10):\n","        #   result1, attention_plot = evaluate(image)\n","        #   result.extend(result1[:-1])\n","        # while len(result) <50:\n","        result, attention_plot = evaluate(imgs, ls, fname1)\n","        # print(len(result[0]))\n","        # print(fl[rid])\n","        # print(fl[rid].decode())\n","        # real_caption = codecs.open(\"./flaviatxt/\" + \"%0d\" % int(fl[rid].numpy().decode().split(\"/\")[0]) + \".txt\", encoding=\"gbk\").read()\n","        # print('Real Caption:', real_caption)\n","        # print(result)\n","        # tokenizer.text\n","        # print (tokenizer.text  result)\n","        # print(\"result:\",result)\n","        # print(\"tf.one_hot(fname1, 32):\", tf.one_hot(tf.cast(fname1,dtype=tf.int32), 32))\n","        # if fname1_list == []:\n","        #     fname1_list = fname1\n","        # else:\n","        #     fname1_list = tf.concat([fname1_list, fname1], axis = 1)\n","        fname1_list.append(fname1)\n","        result_list.append(result)\n","        # if result_list == []:\n","        #     result_list = result\n","        # else:\n","        #     result_list = tf.concat([result_list, result], axis = 1)\n","        \n","        # print(tokenizer.texts_to_sequences([result]))\n","        # result_seq = tokenizer.texts_to_sequences([result])\n","\n","\n","        # np.argmax(model_text.predict(result_seq))\n","        # print(np.argmax(model_text.predict(result_seq)))\n","\n","        # print('Prediction Caption:', ''.join(result))\n","        # result, attention_plot = evaluate(image)\n","        # PIL.open\n","        # print(fl[rid])\n","        # print(fl[rid].decode())\n","        # plt.title(\"实际图片\")\n","        # print(result)\n","        # print(result[0])\n","        # print(tokenizer.index_word.get(result[0], \" \"))\n","        # print(tokenizer.sequences_to_texts(result.numpy()))\n","        # result.append(tokenizer.index_word.get(result, \" \"))\n","\n","        # plt.imshow(Image.open(os.path.join(imgrootpath, fl[rid].numpy().decode())))\n","        # testimagepath = os.path.join(imgrootpath, fl[rid].numpy().decode())\n","        # plot_attention(testimagepath, result, attention_plot)\n","    fname1_list = tf.convert_to_tensor(fname1_list, dtype=tf.int32)\n","    result_list = tf.convert_to_tensor(result_list, dtype=tf.int32)\n","\n","    # print(\"fname1_list:\", fname1_list)\n","    # print(\"result_list:\", result_list)\n","    # print ((1,) + tuple(result_list.shape[-1:]))\n","    # print (result_list[1:])\n","    fname1_list = tf.reshape(fname1_list, (-1,))\n","    # fname1_list = tf.reshape(fname1_list, tuple(fname1_list.shape[-1]))\n","    result_list = tf.reshape(result_list, ((-1,) + tuple(result_list.shape[-1:])))\n","    # result_list = tf.reshape(tf.convert_to_tensor(result_list), 0)\n","    # print(\"fname1_list:\", fname1_list)\n","    # print(\"result_list:\", result_list)\n","    \n","    fname1_onehot = tf.one_hot(tf.cast(fname1_list,dtype=tf.int32), 32)\n","    evaresult = model_text.evaluate(tf.cast(result_list, dtype=tf.int32), fname1_onehot)\n","    print(evaresult)\n","    print(classification_report(tf.argmax(tf.convert_to_tensor(model_text.predict(tf.cast(result_list, dtype=tf.int32))),axis=1), fname1_list))\n","    fname1_list = list(fname1_list.numpy())\n","    # print(\"fname1_list:\", fname1_list)\n","    # print(\"result_list:\", result_list.numpy())\n","    # print(\"result_list:\", tokenizer.sequences_to_texts(result_list.numpy()))\n","    if 0:\n","        wr_d = {}\n","        for x,y,z in zip(fname1_list, tokenizer.sequences_to_texts(result_list.numpy()), result_list.numpy()):\n","            r = model_text.predict(tf.cast([z], dtype=tf.int32))\n","            print (x,y,r, tf.argsort(r, direction = \"DESCENDING\"))\n","            # wr_d[x] = \"12\"\n","            wr_d[x] = wr_d.get(x, \"\") + y\n","        # for x in wr_d.keys():\n","        #     print (x, wr_d[x])\n","        #     for x1 in list(set(wr_d[x].replace(\"<start>\",\"\").replace(\"<end>\",\",\").split(\",\"))):\n","        #         print (x1, wr_d[x].replace(\"<start>\",\"\").replace(\"<end>\",\",\").split(\",\").count(x1))\n","\n","        # for x in range(1,34):\n","        #     print(x, fname1_list.count(x))\n","testonvalimage1()\n","# testonvalimage1()\n","# testonvalimage1()\n","# testonvalimage1()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["8/8 [==============================] - 0s 18ms/step - loss: 14.0915 - accuracy: 0.0000e+00\n","[14.091514587402344, 0.0]\n","              precision    recall  f1-score   support\n","\n","           1       0.00      0.00      0.00       1.0\n","           3       0.00      0.00      0.00       0.0\n","           9       0.00      0.00      0.00       0.0\n","          10       0.00      0.00      0.00     181.0\n","          12       0.00      0.00      0.00       0.0\n","          17       0.00      0.00      0.00       0.0\n","          18       0.00      0.00      0.00       6.0\n","          19       0.00      0.00      0.00      68.0\n","          23       0.00      0.00      0.00       0.0\n","          24       0.00      0.00      0.00       0.0\n","\n","    accuracy                           0.00     256.0\n","   macro avg       0.00      0.00      0.00     256.0\n","weighted avg       0.00      0.00      0.00     256.0\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N4Zkj389uixK","executionInfo":{"elapsed":3669,"status":"ok","timestamp":1626329214163,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"567c3089-5ac3-4a84-d218-4a328b047063"},"source":["!rm /content/Leaves2/03/*\n","!rm /content/Leaves2/13/*\n","!rm /content/Leaves2/22/*\n","!rm /content/Leaves2/27/*\n","!rm /content/Leaves2/04/*\n","!rm /content/Leaves2/26/*"],"execution_count":null,"outputs":[{"output_type":"stream","text":["rm: cannot remove '/content/Leaves2/03/*': No such file or directory\n","rm: cannot remove '/content/Leaves2/13/*': No such file or directory\n","rm: cannot remove '/content/Leaves2/22/*': No such file or directory\n","rm: cannot remove '/content/Leaves2/27/*': No such file or directory\n","rm: cannot remove '/content/Leaves2/04/*': No such file or directory\n","rm: cannot remove '/content/Leaves2/26/*': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"id":"WJnhB-tPOW37","executionInfo":{"elapsed":107842,"status":"ok","timestamp":1626179781803,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"ef568f2a-d037-4719-8cd6-5d7994839544"},"source":["#@title part 6, 加入base model inceptionV3，以期更好地准确率\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n","from tensorflow.keras.models import Sequential,Model\n","\n","model_text_p = model_text.layers[:-1]\n","\n","model_text_p = tf.keras.Model(model_text.input, model_text.layers[-2].output)\n","model_text_p.summary()\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n","start_epoch = 0\n","# print(ckpt_manager.latest_checkpoint)\n","if ckpt_manager.latest_checkpoint:\n","    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n","    print(\"start_epoch:\", start_epoch)\n","    # restoring the latest checkpoint in checkpoint_path\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\n","\n","class Model_ed1(tf.keras.Model):\n","    # Since you have already extracted the features and dumped it using pickle\n","    # This encoder passes those features through a Fully connected layer\n","    def __init__(self,):\n","        super(Model_ed1, self).__init__()\n","        # shape after fc == (batch_size, 64, embedding_dim)\n","        # self.fc = tf.keras.layers.Dense(embedding_dim)\n","\n","    def call(self, images_tensor):\n","        # print(\"images_tensor.shape1:\", images_tensor.shape)\n","        images_tensor = tf.reshape(images_tensor, (-1, images_tensor.shape[3]))\n","        hidden = decoder.reset_state(batch_size=BATCH_SIZE)\n","        dec_input = tf.expand_dims([tokenizer.word_index['<start>']] *BATCH_SIZE , 1)\n","        # print(\"images_tensor.shape2:\", images_tensor.shape)\n","        features = encoder(images_tensor)    \n","        # print(\"images_tensor.shape3:\", images_tensor.shape)\n","        result = []\n","        result_list = []\n","        predictions_txtlist = dec_input\n","        predictions_txtlist = tf.cast(predictions_txtlist, dtype=tf.int64)\n","        for i in range(max_length):\n","            predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n","            predicted_id = tf.random.categorical(predictions, 1)\n","            predictions_txtlist = tf.concat([predictions_txtlist, predicted_id], axis = 1)\n","            dec_input = predicted_id\n","        result_list.append(predictions_txtlist)\n","        result_list = tf.convert_to_tensor(result_list, dtype=tf.int32)\n","        result_list = tf.reshape(result_list, ((-1,) + tuple(result_list.shape[-1:])))\n","        # presult = model_text_p.predict(tf.cast(result_list, dtype=tf.int32))\n","        presult = model_text_p(tf.cast(result_list, dtype=tf.int32))\n","        # print(presult)\n","        return presult\n","\n","model_ed1 = Model_ed1()\n","model_ed1.trainable = False\n","\n","IMAGE_SIZE_MobileNetV2 = (160,160)\n","IMAGE_SIZE_MobileNetV3Large = (224,224)\n","IMAGE_SIZE_VGG19 = (224,224)\n","IMAGE_SIZE_INCEPTIONV3 = (299,299)\n","IMG_SIZE = IMAGE_SIZE_VGG19\n","IMG_SIZE = IMAGE_SIZE_INCEPTIONV3\n","IMG_SHAPE = IMG_SIZE + (3,)\n","BATCH_SIZE = 64\n","train_dir = leaves_folder+\"2\"\n","train_db = image_dataset_from_directory(train_dir,\n","  shuffle=True,\n","  batch_size=BATCH_SIZE,\n","  subset='training',\n","    label_mode=\"categorical\",\n","  seed = 123,\n","  validation_split=0.2,\n","  image_size=IMG_SIZE)\n","validation_db = image_dataset_from_directory(train_dir,\n","  shuffle=True,\n","  batch_size=BATCH_SIZE,\n","  subset='validation',\n","    label_mode=\"categorical\",\n","  seed = 123,\n","  validation_split=0.2,\n","  image_size=IMG_SIZE)\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","train_db = train_db.prefetch(buffer_size=AUTOTUNE)\n","validation_db = validation_db.prefetch(buffer_size=AUTOTUNE)\n","data_augmentation = tf.keras.Sequential([\n","  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n","  tf.keras.layers.experimental.preprocessing.RandomRotation(0),\n","])\n","preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n","rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)\n","base_model_DenseNet201 = tf.keras.applications.DenseNet201(input_shape=IMG_SHAPE,\n","  include_top=False,\n","  weights='imagenet'\n","  )\n","base_model_InceptionV3 = tf.keras.applications.InceptionV3(input_shape=IMG_SHAPE,\n","  include_top=False,\n","  weights='imagenet'\n","  )\n","base_model = base_model_InceptionV3\n","image_batch, label_batch = next(iter(train_db))\n","feature_batch = base_model(image_batch)\n","print(\"after base_model, \",feature_batch.shape)\n","print(\"image_batch: \", image_batch.shape)\n","base_model.trainable = False\n","global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n","# prediction_layer = tf.keras.layers.Dense(32, activation='softmax')\n","prediction_layer = tf.keras.layers.Dense(32)\n","\n","inputs = Input(shape=IMG_SHAPE)\n","x = data_augmentation(inputs)\n","x = rescale(x)\n","x = base_model(x)\n","x1 = model_ed1(x)\n","x2 = global_average_layer(x)\n","# x2 = tf.keras.layers.Dense(512)(x2)\n","x = keras.layers.concatenate([x1, x2], axis=-1)\n","# x = tf.keras.layers.Dropout(0.3)(x)\n","# x = global_average_layer(x)\n","x = tf.keras.layers.Dense(1024)(x)\n","# x = tf.keras.layers.Dropout(0.3)(x)\n","x = tf.keras.layers.Dense(256)(x)\n","# x = tf.keras.layers.Dropout(0.3)(x)\n","x = prediction_layer(x)\n","\n","# model = Model(inputs=[inputs, auxiliary_input], outputs=[main_output, auxiliary_output])\n","model = Model(inputs, x)\n","\n","# model = Sequential()\n","# model.add(rescale)\n","# model.add(base_model)\n","# model.add(global_average_layer)\n","# model.add(prediction_layer)\n","\n","#opt = keras.optimizers.RMSprop(lr= 0.1, decay=1e-6)\n","#model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n","# model.compile(loss=\"mse\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n","# loss1 = tf.keras.losses.SparseCategoricalCrossentropy()\n","loss1 = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","# loss1 = tf.keras.losses.MeanSquaredError()\n","\n","# model.compile(optimizer='adam',\n","#   loss=loss,\n","#   metrics=['accuracy'])\n","# model.compile(optimizer='adam', loss=loss1, metrics=['accuracy'])\n","base_learning_rate = 0.0001\n","model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate), loss=loss1, metrics=['accuracy'])\n","\n","model.summary()\n","\n","initial_epochs = 100\n","loss0, accuracy0 = model.evaluate(validation_db)\n","print(\"initial loss: {:.2f}\".format(loss0))\n","print(\"initial accuracy: {:.2f}\".format(accuracy0))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"model_23\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_22_input (InputLay [(None, None)]            0         \n","_________________________________________________________________\n","embedding_22 (Embedding)     (None, None, 200)         2000200   \n","_________________________________________________________________\n","bidirectional_18 (Bidirectio (None, None, 400)         641600    \n","_________________________________________________________________\n","bidirectional_19 (Bidirectio (None, 200)               400800    \n","_________________________________________________________________\n","dense_96 (Dense)             (None, 512)               102912    \n","=================================================================\n","Total params: 3,145,512\n","Trainable params: 0\n","Non-trainable params: 3,145,512\n","_________________________________________________________________\n","start_epoch: 16\n","Found 1920 files belonging to 32 classes.\n","Using 1536 files for training.\n","Found 1920 files belonging to 32 classes.\n","Using 384 files for validation.\n","after base_model,  (64, 8, 8, 2048)\n","image_batch:  (64, 299, 299, 3)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"model_24\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_37 (InputLayer)           [(None, 299, 299, 3) 0                                            \n","__________________________________________________________________________________________________\n","sequential_13 (Sequential)      (None, 299, 299, 3)  0           input_37[0][0]                   \n","__________________________________________________________________________________________________\n","rescaling_11 (Rescaling)        (None, 299, 299, 3)  0           sequential_13[0][0]              \n","__________________________________________________________________________________________________\n","inception_v3 (Functional)       (None, 8, 8, 2048)   21802784    rescaling_11[0][0]               \n","__________________________________________________________________________________________________\n","model_ed1_11 (Model_ed1)        (64, 512)            0           inception_v3[0][0]               \n","__________________________________________________________________________________________________\n","global_average_pooling2d_11 (Gl (None, 2048)         0           inception_v3[0][0]               \n","__________________________________________________________________________________________________\n","concatenate_35 (Concatenate)    (64, 2560)           0           model_ed1_11[0][0]               \n","                                                                 global_average_pooling2d_11[0][0]\n","__________________________________________________________________________________________________\n","dense_45 (Dense)                (64, 1024)           2622464     concatenate_35[0][0]             \n","__________________________________________________________________________________________________\n","dense_46 (Dense)                (64, 256)            262400      dense_45[0][0]                   \n","__________________________________________________________________________________________________\n","dense_44 (Dense)                (64, 32)             8224        dense_46[0][0]                   \n","==================================================================================================\n","Total params: 24,695,872\n","Trainable params: 2,893,088\n","Non-trainable params: 21,802,784\n","__________________________________________________________________________________________________\n","6/6 [==============================] - 74s 988ms/step - loss: 17.1529 - accuracy: 0.0234\n","initial loss: 17.15\n","initial accuracy: 0.02\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"cellView":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"FV9GiGaItL8a","executionInfo":{"elapsed":681694,"status":"error","timestamp":1626183990358,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"33b6837d-8ad0-455a-ef2e-7dbda953b6ce"},"source":["#@title part6  训练一下\n","\n","def decay(epoch):\n","  if epoch < 5:\n","    return 1e-3\n","  elif epoch >= 5 and epoch < 30:\n","    return 1e-4\n","  elif epoch >= 30 and epoch < 100:\n","    return 4e-5\n","  else:\n","    return 1e-6\n","\n","def decay(epoch):\n","  return 2e-8\n","\n","class PrintLR(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs=None):\n","    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1, model.optimizer.lr.numpy()))\n","    \n","callbacks = [\n","    # tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n","    # tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True),\n","    tf.keras.callbacks.LearningRateScheduler(decay),\n","    PrintLR()\n","]\n","\n","# base_learning_rate = 0.00004\n","# model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate), loss=loss1, metrics=['accuracy'])\n","\n","history = model.fit(train_db,\n","  epochs=1000,\n","  validation_data=validation_db,\n","  callbacks=callbacks)\n","\n","loss0, accuracy0 = model.evaluate(validation_db)\n","print(\"after fit loss: {:.2f}\".format(loss0))\n","print(\"after fit accuracy: {:.2f}\".format(accuracy0))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/1000\n","24/24 [==============================] - 38s 1s/step - loss: 0.0252 - accuracy: 0.9941 - val_loss: 0.0768 - val_accuracy: 0.9714\n","\n","Learning rate for epoch 1 is 1.999999987845058e-08\n","Epoch 2/1000\n","24/24 [==============================] - 37s 1s/step - loss: 0.0290 - accuracy: 0.9948 - val_loss: 0.0796 - val_accuracy: 0.9740\n","\n","Learning rate for epoch 2 is 1.999999987845058e-08\n","Epoch 3/1000\n","24/24 [==============================] - 37s 1s/step - loss: 0.0269 - accuracy: 0.9941 - val_loss: 0.0827 - val_accuracy: 0.9714\n","\n","Learning rate for epoch 3 is 1.999999987845058e-08\n","Epoch 4/1000\n","24/24 [==============================] - 37s 1s/step - loss: 0.0264 - accuracy: 0.9941 - val_loss: 0.0811 - val_accuracy: 0.9740\n","\n","Learning rate for epoch 4 is 1.999999987845058e-08\n","Epoch 5/1000\n","24/24 [==============================] - 37s 1s/step - loss: 0.0297 - accuracy: 0.9915 - val_loss: 0.0881 - val_accuracy: 0.9688\n","\n","Learning rate for epoch 5 is 1.999999987845058e-08\n","Epoch 6/1000\n","24/24 [==============================] - 37s 1s/step - loss: 0.0280 - accuracy: 0.9922 - val_loss: 0.0767 - val_accuracy: 0.9766\n","\n","Learning rate for epoch 6 is 1.999999987845058e-08\n","Epoch 7/1000\n","24/24 [==============================] - 37s 1s/step - loss: 0.0289 - accuracy: 0.9915 - val_loss: 0.0794 - val_accuracy: 0.9714\n","\n","Learning rate for epoch 7 is 1.999999987845058e-08\n","Epoch 8/1000\n","24/24 [==============================] - 38s 1s/step - loss: 0.0305 - accuracy: 0.9928 - val_loss: 0.0809 - val_accuracy: 0.9740\n","\n","Learning rate for epoch 8 is 1.999999987845058e-08\n","Epoch 9/1000\n","24/24 [==============================] - 37s 1s/step - loss: 0.0304 - accuracy: 0.9909 - val_loss: 0.0839 - val_accuracy: 0.9714\n","\n","Learning rate for epoch 9 is 1.999999987845058e-08\n","Epoch 10/1000\n","24/24 [==============================] - 38s 1s/step - loss: 0.0245 - accuracy: 0.9948 - val_loss: 0.0872 - val_accuracy: 0.9766\n","\n","Learning rate for epoch 10 is 1.999999987845058e-08\n","Epoch 11/1000\n","24/24 [==============================] - 38s 1s/step - loss: 0.0298 - accuracy: 0.9902 - val_loss: 0.0788 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 11 is 1.999999987845058e-08\n","Epoch 12/1000\n","24/24 [==============================] - 37s 1s/step - loss: 0.0265 - accuracy: 0.9922 - val_loss: 0.0798 - val_accuracy: 0.9766\n","\n","Learning rate for epoch 12 is 1.999999987845058e-08\n","Epoch 13/1000\n","24/24 [==============================] - 37s 1s/step - loss: 0.0275 - accuracy: 0.9935 - val_loss: 0.0789 - val_accuracy: 0.9740\n","\n","Learning rate for epoch 13 is 1.999999987845058e-08\n","Epoch 14/1000\n","24/24 [==============================] - 37s 1s/step - loss: 0.0266 - accuracy: 0.9948 - val_loss: 0.0863 - val_accuracy: 0.9714\n","\n","Learning rate for epoch 14 is 1.999999987845058e-08\n","Epoch 15/1000\n","24/24 [==============================] - 37s 1s/step - loss: 0.0256 - accuracy: 0.9935 - val_loss: 0.0806 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 15 is 1.999999987845058e-08\n","Epoch 16/1000\n","24/24 [==============================] - 38s 1s/step - loss: 0.0273 - accuracy: 0.9941 - val_loss: 0.0806 - val_accuracy: 0.9688\n","\n","Learning rate for epoch 16 is 1.999999987845058e-08\n","Epoch 17/1000\n","24/24 [==============================] - 37s 1s/step - loss: 0.0259 - accuracy: 0.9941 - val_loss: 0.0853 - val_accuracy: 0.9688\n","\n","Learning rate for epoch 17 is 1.999999987845058e-08\n","Epoch 18/1000\n","24/24 [==============================] - 38s 1s/step - loss: 0.0262 - accuracy: 0.9922 - val_loss: 0.0848 - val_accuracy: 0.9766\n","\n","Learning rate for epoch 18 is 1.999999987845058e-08\n","Epoch 19/1000\n"," 3/24 [==>...........................] - ETA: 25s - loss: 0.0221 - accuracy: 0.9948"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-d5e76d026f7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_db\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   callbacks=callbacks)\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mloss0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_db\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PA7ObjCPTJPq","executionInfo":{"elapsed":30104,"status":"ok","timestamp":1626229547398,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"fa4adacb-b649-43eb-8e9c-df47f3cb0dbb"},"source":["#@title part 7, inceptionV3，准备将这个模型用于注意力机制中。\n","from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Input\n","from tensorflow.keras.models import Sequential,Model\n","\n","# model_text_p = model_text.layers[:-1]\n","\n","# model_text_p = tf.keras.Model(model_text.input, model_text.layers[-2].output)\n","# model_text_p.summary()\n","\n","# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n","# start_epoch = 0\n","# # print(ckpt_manager.latest_checkpoint)\n","# if ckpt_manager.latest_checkpoint:\n","#     start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n","#     print(\"start_epoch:\", start_epoch)\n","#     # restoring the latest checkpoint in checkpoint_path\n","#     ckpt.restore(ckpt_manager.latest_checkpoint)\n","\n","# class Model_ed1(tf.keras.Model):\n","#     # Since you have already extracted the features and dumped it using pickle\n","#     # This encoder passes those features through a Fully connected layer\n","#     def __init__(self,):\n","#         super(Model_ed1, self).__init__()\n","#         # shape after fc == (batch_size, 64, embedding_dim)\n","#         # self.fc = tf.keras.layers.Dense(embedding_dim)\n","\n","#     def call(self, images_tensor):\n","#         # print(\"images_tensor.shape1:\", images_tensor.shape)\n","#         images_tensor = tf.reshape(images_tensor, (-1, images_tensor.shape[3]))\n","#         hidden = decoder.reset_state(batch_size=BATCH_SIZE)\n","#         dec_input = tf.expand_dims([tokenizer.word_index['<start>']] *BATCH_SIZE , 1)\n","#         # print(\"images_tensor.shape2:\", images_tensor.shape)\n","#         features = encoder(images_tensor)    \n","#         # print(\"images_tensor.shape3:\", images_tensor.shape)\n","#         result = []\n","#         result_list = []\n","#         predictions_txtlist = dec_input\n","#         predictions_txtlist = tf.cast(predictions_txtlist, dtype=tf.int64)\n","#         for i in range(max_length):\n","#             predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n","#             predicted_id = tf.random.categorical(predictions, 1)\n","#             predictions_txtlist = tf.concat([predictions_txtlist, predicted_id], axis = 1)\n","#             dec_input = predicted_id\n","#         result_list.append(predictions_txtlist)\n","#         result_list = tf.convert_to_tensor(result_list, dtype=tf.int32)\n","#         result_list = tf.reshape(result_list, ((-1,) + tuple(result_list.shape[-1:])))\n","#         # presult = model_text_p.predict(tf.cast(result_list, dtype=tf.int32))\n","#         presult = model_text_p(tf.cast(result_list, dtype=tf.int32))\n","#         # print(presult)\n","#         return presult\n","\n","# model_ed1 = Model_ed1()\n","# model_ed1.trainable = False\n","\n","IMAGE_SIZE_MobileNetV2 = (160,160)\n","IMAGE_SIZE_MobileNetV3Large = (224,224)\n","IMAGE_SIZE_VGG19 = (224,224)\n","IMAGE_SIZE_INCEPTIONV3 = (299,299)\n","IMG_SIZE = IMAGE_SIZE_VGG19\n","IMG_SIZE = IMAGE_SIZE_INCEPTIONV3\n","IMG_SHAPE = IMG_SIZE + (3,)\n","BATCH_SIZE = 64\n","train_dir = leaves_folder+\"2\"\n","train_db = image_dataset_from_directory(train_dir,\n","  shuffle=True,\n","  batch_size=BATCH_SIZE,\n","  subset='training',\n","    label_mode=\"categorical\",\n","  seed = 123,\n","  validation_split=0.2,\n","  image_size=IMG_SIZE)\n","validation_db = image_dataset_from_directory(train_dir,\n","  shuffle=True,\n","  batch_size=BATCH_SIZE,\n","  subset='validation',\n","    label_mode=\"categorical\",\n","  seed = 123,\n","  validation_split=0.2,\n","  image_size=IMG_SIZE)\n","AUTOTUNE = tf.data.experimental.AUTOTUNE\n","train_db = train_db.prefetch(buffer_size=AUTOTUNE)\n","validation_db = validation_db.prefetch(buffer_size=AUTOTUNE)\n","data_augmentation = tf.keras.Sequential([\n","  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n","  tf.keras.layers.experimental.preprocessing.RandomRotation(0),\n","])\n","preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n","rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)\n","base_model_DenseNet201 = tf.keras.applications.DenseNet201(input_shape=IMG_SHAPE,\n","  include_top=False,\n","  weights='imagenet'\n","  )\n","base_model_InceptionV3 = tf.keras.applications.InceptionV3(input_shape=IMG_SHAPE,\n","  include_top=False,\n","  weights='imagenet'\n","  )\n","base_model = base_model_InceptionV3\n","image_batch, label_batch = next(iter(train_db))\n","feature_batch = base_model(image_batch)\n","print(\"after base_model, \",feature_batch.shape)\n","print(\"image_batch: \", image_batch.shape)\n","base_model.trainable = False\n","global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n","# prediction_layer = tf.keras.layers.Dense(32, activation='softmax')\n","prediction_layer = tf.keras.layers.Dense(32)\n","\n","inputs = Input(shape=IMG_SHAPE)\n","x = data_augmentation(inputs)\n","x = rescale(x)\n","x = base_model(x)\n","# x1 = model_ed1(x)\n","# x2 = global_average_layer(x)\n","# x2 = tf.keras.layers.Dense(512)(x2)\n","# x = keras.layers.concatenate([x1, x2], axis=-1)\n","# x = tf.keras.layers.Dropout(0.3)(x)\n","x = global_average_layer(x)\n","x = tf.keras.layers.Dense(1024)(x)\n","# x = tf.keras.layers.Dropout(0.3)(x)\n","x = tf.keras.layers.Dense(512)(x)\n","# x = tf.keras.layers.Dropout(0.3)(x)\n","x = prediction_layer(x)\n","\n","# model = Model(inputs=[inputs, auxiliary_input], outputs=[main_output, auxiliary_output])\n","model_image = Model(inputs, x)\n","\n","# model = Sequential()\n","# model.add(rescale)\n","# model.add(base_model)\n","# model.add(global_average_layer)\n","# model.add(prediction_layer)\n","\n","#opt = keras.optimizers.RMSprop(lr= 0.1, decay=1e-6)\n","#model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n","# model.compile(loss=\"mse\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n","# loss1 = tf.keras.losses.SparseCategoricalCrossentropy()\n","loss1 = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","# loss1 = tf.keras.losses.MeanSquaredError()\n","\n","# model.compile(optimizer='adam',\n","#   loss=loss,\n","#   metrics=['accuracy'])\n","# model.compile(optimizer='adam', loss=loss1, metrics=['accuracy'])\n","base_learning_rate = 0.0001\n","model_image.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate), loss=loss1, metrics=['accuracy'])\n","\n","model_image.summary()\n","\n","initial_epochs = 100\n","loss0, accuracy0 = model_image.evaluate(validation_db)\n","print(\"initial loss: {:.2f}\".format(loss0))\n","print(\"initial accuracy: {:.2f}\".format(accuracy0))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 1920 files belonging to 32 classes.\n","Using 1536 files for training.\n","Found 1920 files belonging to 32 classes.\n","Using 384 files for validation.\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n","74842112/74836368 [==============================] - 2s 0us/step\n","after base_model,  (64, 8, 8, 2048)\n","image_batch:  (64, 299, 299, 3)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_4 (InputLayer)         [(None, 299, 299, 3)]     0         \n","_________________________________________________________________\n","sequential_2 (Sequential)    (None, 299, 299, 3)       0         \n","_________________________________________________________________\n","rescaling (Rescaling)        (None, 299, 299, 3)       0         \n","_________________________________________________________________\n","inception_v3 (Functional)    (None, 8, 8, 2048)        21802784  \n","_________________________________________________________________\n","global_average_pooling2d (Gl (None, 2048)              0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 1024)              2098176   \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 512)               524800    \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 32)                16416     \n","=================================================================\n","Total params: 24,442,176\n","Trainable params: 2,639,392\n","Non-trainable params: 21,802,784\n","_________________________________________________________________\n","6/6 [==============================] - 5s 138ms/step - loss: 3.7168 - accuracy: 0.0469\n","initial loss: 3.72\n","initial accuracy: 0.05\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"cellView":"code","colab":{"base_uri":"https://localhost:8080/"},"id":"hswdYFRrTLMR","executionInfo":{"elapsed":487806,"status":"ok","timestamp":1626188165919,"user":{"displayName":"li jie","photoUrl":"","userId":"02092040955719806446"},"user_tz":-480},"outputId":"79c4122c-cbc5-4c10-e042-5f6d813e670f"},"source":["#@title part7  训练一下\n","\n","def decay(epoch):\n","  if epoch < 5:\n","    return 1e-3\n","  elif epoch >= 5 and epoch < 30:\n","    return 1e-4\n","  elif epoch >= 30 and epoch < 100:\n","    return 4e-5\n","  else:\n","    return 1e-6\n","\n","# def decay(epoch):\n","#   return 2e-8\n","\n","class PrintLR(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs=None):\n","    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1, model_image.optimizer.lr.numpy()))\n","    \n","callbacks = [\n","    # tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n","    # tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True),\n","    tf.keras.callbacks.LearningRateScheduler(decay),\n","    PrintLR()\n","]\n","\n","# base_learning_rate = 0.00004\n","# model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate), loss=loss1, metrics=['accuracy'])\n","\n","history = model_image.fit(train_db,\n","  epochs=30,\n","  validation_data=validation_db,\n","  callbacks=callbacks)\n","\n","model_image.save(\"model_image\")\n","\n","# model_image = tf.keras.models.load_model(\"model_image\")\n","\n","loss0, accuracy0 = model_image.evaluate(validation_db)\n","print(\"after fit loss: {:.2f}\".format(loss0))\n","print(\"after fit accuracy: {:.2f}\".format(accuracy0))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/30\n","24/24 [==============================] - 14s 463ms/step - loss: 0.1466 - accuracy: 0.9518 - val_loss: 0.1231 - val_accuracy: 0.9688\n","\n","Learning rate for epoch 1 is 0.0010000000474974513\n","Epoch 2/30\n","24/24 [==============================] - 14s 468ms/step - loss: 0.0628 - accuracy: 0.9779 - val_loss: 0.0920 - val_accuracy: 0.9609\n","\n","Learning rate for epoch 2 is 0.0010000000474974513\n","Epoch 3/30\n","24/24 [==============================] - 14s 467ms/step - loss: 0.0286 - accuracy: 0.9915 - val_loss: 0.0653 - val_accuracy: 0.9766\n","\n","Learning rate for epoch 3 is 0.0010000000474974513\n","Epoch 4/30\n","24/24 [==============================] - 14s 464ms/step - loss: 0.0116 - accuracy: 0.9987 - val_loss: 0.0510 - val_accuracy: 0.9766\n","\n","Learning rate for epoch 4 is 0.0010000000474974513\n","Epoch 5/30\n","24/24 [==============================] - 14s 460ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.0459 - val_accuracy: 0.9844\n","\n","Learning rate for epoch 5 is 0.0010000000474974513\n","Epoch 6/30\n","24/24 [==============================] - 14s 462ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.0387 - val_accuracy: 0.9870\n","\n","Learning rate for epoch 6 is 9.999999747378752e-05\n","Epoch 7/30\n","24/24 [==============================] - 14s 460ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0362 - val_accuracy: 0.9844\n","\n","Learning rate for epoch 7 is 9.999999747378752e-05\n","Epoch 8/30\n","24/24 [==============================] - 14s 460ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0367 - val_accuracy: 0.9844\n","\n","Learning rate for epoch 8 is 9.999999747378752e-05\n","Epoch 9/30\n","24/24 [==============================] - 14s 465ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0375 - val_accuracy: 0.9844\n","\n","Learning rate for epoch 9 is 9.999999747378752e-05\n","Epoch 10/30\n","24/24 [==============================] - 14s 462ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 0.9844\n","\n","Learning rate for epoch 10 is 9.999999747378752e-05\n","Epoch 11/30\n","24/24 [==============================] - 14s 462ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0386 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 11 is 9.999999747378752e-05\n","Epoch 12/30\n","24/24 [==============================] - 14s 459ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0385 - val_accuracy: 0.9844\n","\n","Learning rate for epoch 12 is 9.999999747378752e-05\n","Epoch 13/30\n","24/24 [==============================] - 14s 463ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 13 is 9.999999747378752e-05\n","Epoch 14/30\n","24/24 [==============================] - 14s 463ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0399 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 14 is 9.999999747378752e-05\n","Epoch 15/30\n","24/24 [==============================] - 15s 468ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0394 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 15 is 9.999999747378752e-05\n","Epoch 16/30\n","24/24 [==============================] - 14s 462ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0390 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 16 is 9.999999747378752e-05\n","Epoch 17/30\n","24/24 [==============================] - 14s 463ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0402 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 17 is 9.999999747378752e-05\n","Epoch 18/30\n","24/24 [==============================] - 14s 466ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0403 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 18 is 9.999999747378752e-05\n","Epoch 19/30\n","24/24 [==============================] - 14s 466ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0394 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 19 is 9.999999747378752e-05\n","Epoch 20/30\n","24/24 [==============================] - 14s 460ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0388 - val_accuracy: 0.9844\n","\n","Learning rate for epoch 20 is 9.999999747378752e-05\n","Epoch 21/30\n","24/24 [==============================] - 14s 464ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0387 - val_accuracy: 0.9844\n","\n","Learning rate for epoch 21 is 9.999999747378752e-05\n","Epoch 22/30\n","24/24 [==============================] - 14s 464ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0385 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 22 is 9.999999747378752e-05\n","Epoch 23/30\n","24/24 [==============================] - 14s 462ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0389 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 23 is 9.999999747378752e-05\n","Epoch 24/30\n","24/24 [==============================] - 14s 460ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0391 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 24 is 9.999999747378752e-05\n","Epoch 25/30\n","24/24 [==============================] - 14s 459ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0391 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 25 is 9.999999747378752e-05\n","Epoch 26/30\n","24/24 [==============================] - 14s 460ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0390 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 26 is 9.999999747378752e-05\n","Epoch 27/30\n","24/24 [==============================] - 14s 459ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0391 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 27 is 9.999999747378752e-05\n","Epoch 28/30\n","24/24 [==============================] - 14s 458ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0386 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 28 is 9.999999747378752e-05\n","Epoch 29/30\n","24/24 [==============================] - 14s 459ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0378 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 29 is 9.999999747378752e-05\n","Epoch 30/30\n","24/24 [==============================] - 14s 459ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0386 - val_accuracy: 0.9818\n","\n","Learning rate for epoch 30 is 9.999999747378752e-05\n","INFO:tensorflow:Assets written to: model_image/assets\n","6/6 [==============================] - 3s 133ms/step - loss: 0.0386 - accuracy: 0.9818\n","after fit loss: 0.04\n","after fit accuracy: 0.98\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4g1XCKxi5g1H"},"source":["#总结一下\n","1. 第一次训练的时候，使用的参数是 embedding_dim = 256， units = 512， 在train_dataset上面可以获得最多58%的准确率。由于model_text的准确率为93%。所以实际模型应该有至少62%的准确率，这样的情况下还是在没有调优的情况下获得的，还可以采用更多额办法来处理。\n","2. 第二次训练，使用的参数是embedding_dim = 512， units = 1024,准确率是55%~57%。和上面没有明显差异。\n","3. 上面两次训练都是把model_text的结果，计算一个mse的loss加到总的损失里面，第3次训练尝试不加这个loss来试一下。说明一下，模型的训练使用的40次的epochs。降到了54%，看起来还是有点效果的。\n","4. 将model_text的loss加进来，0.4，模型准确率为54~56%。\n","5. 使用model_text的交叉熵的损失函数，52%~54%\n","6. 第一次将image文本的生成长度变为150时，准确率达到了63%，说明这个文本确实影响了准确率而且影响很大。\n","7. 将结果去重以后再丢入文本分类模型，发现准确度变差很多，不知道原因时什么，猜测核数据集有很大的关系。\n","8. 将keyword变大，文本变长，可以比较容易得到一个高点的文本分类模型。但是不知道为什么此时的image文本的准确率下降的很夸张。\n","9. 目前的感觉时模型其实能力很强，但是训练的方式不同造成结果很差，从之前63%的结果来看，还有很大的提升空间。\n","10. 猜一下目前为什么去重后结果很差，因为某些图像的类别的文本的关键字实在太少了，所以在训练的时候模型需要用到重复文本的重复次数来作为判断的依据，造成去重后信息丢失，自然准确率变差了。同时也要相信image文本的准确率下降的原因也是信息变少造成的。\n","11. 为了找到是什么原因造成了image 文本的准确率的问题，做了以下事情：\n","\n","  A. 某次训练中，image文本的分数为40%左右，训练了至少30轮，将文本分类模型再训练一遍，结果变为了20%。这个结果是抖动的，抖动是正常的，偏差为4%。\n","  \n","  B. 再训练3轮 image文本模型，结果还是20%。\n","  C. 去重版本的image文本 验证结果6%. 结果更差，所以我们的这个条件不应该用去重。\n","  D. 再重新训练模型，得到30%。\n","  E. 再重新训练所有的模型，参数不变化，得到70%的准确率。"]},{"cell_type":"markdown","metadata":{"id":"lZEFf6gy6UpN"},"source":["# ICL 测试总结\n","1. ICL 最终的得分和flavia的得分差不多，51%~57%。这个说明分数低的原因不在于数据集的不同，而在于整套流程的问题，可能是model的问题，也可能是数据生成的问题。比较相关的是数据文本那里。但是检查了一遍并未检查出来原因是什么，需要对每个样品进行分析。\n","2. 还可以使用top5 来看下每个样本的结果。top 5的结果如下： \n","  loss: 0.7343 - top_k_categorical_accuracy: 0.9648\n","3. 折腾了很久发现这个结果有变低了好多，这个明显不是我想要的结果，看起来无计可施。下一步准备使用笔记本上的思路来做。所以继续回归lunwen3，来搞flavia\n"]}]}